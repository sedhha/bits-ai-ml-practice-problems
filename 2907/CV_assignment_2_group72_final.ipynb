{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9bda93ba",
      "metadata": {
        "id": "9bda93ba"
      },
      "source": [
        "## Group No\n",
        "\n",
        "## Group Member Names:\n",
        "1. SHIVAM SAHIL - 2023AA05663\n",
        "2. JAHNAVI GALI - 2023AA05684\n",
        "3. PRASHANT KUMAR - 2023AA05043\n",
        "4. SAHIL MEHRA - 2023AA05327"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46631608",
      "metadata": {
        "id": "46631608"
      },
      "outputs": [],
      "source": [
        "# Install and import necessary libraries\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.ops import box_iou\n",
        "import warnings\n",
        "from IPython.display import clear_output\n",
        "import datetime\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm  # Regular tqdm, not IProgress\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518ae04c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_train_directory():\n",
        "    return r'mot17/MOT17Det/train'\n",
        "def get_test_directory():\n",
        "    return r'mot17/MOT17Det/test'\n",
        "def get_root_directory():\n",
        "    return r'mot17/MOT17Det'\n",
        "def get_device():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
        "    print(\"Using device:\", device)\n",
        "    return device\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for the DataLoader.\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4fefc0f",
      "metadata": {},
      "source": [
        "#### Dataset Structure\n",
        "\n",
        "```text\n",
        "- MOT17Det\n",
        "    - test\n",
        "        - MOT17-01\n",
        "            - img1\n",
        "                - 000001.jpg\n",
        "                - 000002.jpg\n",
        "                ...\n",
        "            - seqinfo.ini\n",
        "        - MOT17-03\n",
        "        ...\n",
        "    - train\n",
        "        - MOT17-02\n",
        "            - gt\n",
        "                - gt.txt \n",
        "            - img1\n",
        "                - 000001.jpg\n",
        "                - 000002.jpg\n",
        "                ...\n",
        "            - seqinfo.ini\n",
        "        - MOT17-04\n",
        "        ...\n",
        "```\n",
        "\n",
        "- gt.txt content looks like this:\n",
        "```text\n",
        "1,1,912,484,97,109,0,7,1\n",
        "2,1,912,484,97,109,0,7,1\n",
        "3,1,912,484,97,109,0,7,1\n",
        "4,1,912,484,97,109,0,7,1\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fbdbfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_PATH = get_root_directory()\n",
        "TRAIN_DIR = get_train_directory()\n",
        "TEST_DIR = get_test_directory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fbe8f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify dataset paths\n",
        "def check_dataset_paths():\n",
        "    \"\"\"\n",
        "    Verify that the dataset paths exist and list available sequences.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(DATASET_PATH):\n",
        "        print(f\"Dataset path {DATASET_PATH} does not exist. Please update the path.\")\n",
        "        return False\n",
        "    \n",
        "    if not os.path.exists(TRAIN_DIR):\n",
        "        print(f\"Training directory {TRAIN_DIR} does not exist.\")\n",
        "        return False\n",
        "    \n",
        "    if not os.path.exists(TEST_DIR):\n",
        "        print(f\"Test directory {TEST_DIR} does not exist.\")\n",
        "        return False\n",
        "    \n",
        "    # List available training sequences\n",
        "    train_sequences = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\n",
        "    print(f\"Available training sequences ({len(train_sequences)}): {train_sequences}\")\n",
        "    \n",
        "    # List available test sequences\n",
        "    test_sequences = [d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))]\n",
        "    print(f\"Available test sequences ({len(test_sequences)}): {test_sequences}\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "check_dataset_paths()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b97b0447",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ground truth parsing function\n",
        "def parse_gt_file(gt_file_path):\n",
        "    \"\"\"\n",
        "    Parse the ground truth file and return a DataFrame with the annotations.\n",
        "    \n",
        "    The gt.txt format is:\n",
        "    <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <class>, <visibility>\n",
        "    \n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing the parsed annotations\n",
        "    \"\"\"\n",
        "    if not os.path.exists(gt_file_path):\n",
        "        print(f\"Ground truth file {gt_file_path} does not exist.\")\n",
        "        return None\n",
        "    \n",
        "    # Define column names based on MOT format\n",
        "    columns = [\n",
        "        'frame', 'id', 'bb_left', 'bb_top', 'bb_width', 'bb_height', 'conf', 'class', 'visibility'\n",
        "    ]\n",
        "    \n",
        "    # Read the ground truth file\n",
        "    try:\n",
        "        gt_df = pd.read_csv(gt_file_path, header=None, names=columns)\n",
        "        return gt_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing ground truth file: {e}\")\n",
        "        return None\n",
        "\n",
        "# Sequence info loading function\n",
        "def load_sequence_info(sequence_path):\n",
        "    \"\"\"\n",
        "    Load the sequence information from the seqinfo.ini file.\n",
        "    \n",
        "    Returns:\n",
        "        dict: A dictionary containing the sequence information\n",
        "    \"\"\"\n",
        "    seq_info_path = os.path.join(sequence_path, 'seqinfo.ini')\n",
        "    \n",
        "    if not os.path.exists(seq_info_path):\n",
        "        print(f\"Sequence info file {seq_info_path} does not exist.\")\n",
        "        return None\n",
        "    \n",
        "    # Parse the seqinfo.ini file\n",
        "    seq_info = {}\n",
        "    with open(seq_info_path, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.strip()\n",
        "            if line.startswith('[') or line == '':\n",
        "                continue\n",
        "            key, value = line.split('=')\n",
        "            seq_info[key.strip()] = value.strip()\n",
        "    \n",
        "    return seq_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa571aa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample frame loading and visualization function\n",
        "def load_frame(sequence_path, frame_number):\n",
        "    \"\"\"\n",
        "    Load a specific frame from a sequence.\n",
        "    \n",
        "    Args:\n",
        "        sequence_path (str): Path to the sequence directory\n",
        "        frame_number (int): Frame number to load\n",
        "        \n",
        "    Returns:\n",
        "        numpy.ndarray: The loaded frame as an BGR image\n",
        "    \"\"\"\n",
        "    # MOT format uses 1-based indexing for frames, with 6-digit zero-padded filenames\n",
        "    frame_filename = f\"{frame_number:06d}.jpg\"\n",
        "    frame_path = os.path.join(sequence_path, 'img1', frame_filename)\n",
        "    \n",
        "    if not os.path.exists(frame_path):\n",
        "        print(f\"Frame {frame_path} does not exist.\")\n",
        "        return None\n",
        "    \n",
        "    # Read the frame using OpenCV\n",
        "    frame = cv2.imread(frame_path)\n",
        "    \n",
        "    return frame\n",
        "\n",
        "def visualize_frame_with_annotations(sequence_path, gt_df, frame_number, figsize=(10, 8)):\n",
        "    \"\"\"\n",
        "    Visualize a frame with its ground truth annotations.\n",
        "    \n",
        "    Args:\n",
        "        sequence_path (str): Path to the sequence directory\n",
        "        gt_df (pandas.DataFrame): DataFrame containing ground truth annotations\n",
        "        frame_number (int): Frame number to visualize\n",
        "        figsize (tuple): Figure size for the plot\n",
        "    \"\"\"\n",
        "    # Load the frame\n",
        "    frame = load_frame(sequence_path, frame_number)\n",
        "    \n",
        "    if frame is None:\n",
        "        return\n",
        "    \n",
        "    # Convert from BGR to RGB for matplotlib\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Filter annotations for the current frame\n",
        "    frame_annotations = gt_df[gt_df['frame'] == frame_number]\n",
        "    \n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    \n",
        "    # Display the frame\n",
        "    ax.imshow(frame_rgb)\n",
        "    \n",
        "    # Generate random colors for each object ID\n",
        "    unique_ids = frame_annotations['id'].unique()\n",
        "    color_map = {obj_id: np.random.rand(3,) for obj_id in unique_ids}\n",
        "    \n",
        "    # Plot bounding boxes\n",
        "    for _, row in frame_annotations.iterrows():\n",
        "        x, y, w, h = row['bb_left'], row['bb_top'], row['bb_width'], row['bb_height']\n",
        "        obj_id = row['id']\n",
        "        obj_class = row['class']\n",
        "        \n",
        "        # Create rectangle patch\n",
        "        rect = Rectangle((x, y), w, h, linewidth=2, edgecolor=color_map[obj_id], facecolor='none')\n",
        "        \n",
        "        # Add the rectangle to the plot\n",
        "        ax.add_patch(rect)\n",
        "        \n",
        "        # Add label\n",
        "        if obj_class == 1:  # Person\n",
        "            class_name = 'Person'\n",
        "        else:\n",
        "            class_name = f'Class {obj_class}'\n",
        "        \n",
        "        ax.text(x, y-5, f\"ID: {obj_id}, {class_name}\", color=color_map[obj_id], fontsize=10, \n",
        "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
        "    \n",
        "    # Set title\n",
        "    ax.set_title(f\"Frame {frame_number} with {len(frame_annotations)} objects\")\n",
        "    \n",
        "    # Remove axes\n",
        "    ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27141b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_sample_sequence():\n",
        "    \"\"\"\n",
        "    Load a sample training sequence for exploration.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (sequence_path, gt_df, seq_info)\n",
        "    \"\"\"\n",
        "    train_sequences = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\n",
        "    \n",
        "    if not train_sequences:\n",
        "        print(\"No training sequences found.\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # Select the first training sequence\n",
        "    sample_sequence = train_sequences[0]\n",
        "    sequence_path = os.path.join(TRAIN_DIR, sample_sequence)\n",
        "    \n",
        "    # Load ground truth\n",
        "    gt_file_path = os.path.join(sequence_path, 'gt', 'gt.txt')\n",
        "    gt_df = parse_gt_file(gt_file_path)\n",
        "    \n",
        "    # Load sequence info\n",
        "    seq_info = load_sequence_info(sequence_path)\n",
        "    \n",
        "    print(f\"Loaded sample sequence: {sample_sequence}\")\n",
        "    return sequence_path, gt_df, seq_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80147f5a",
      "metadata": {},
      "source": [
        "## Load a sample sequence and visualize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c732efe",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_sequence_path, sample_gt_df, sample_seq_info = load_sample_sequence()\n",
        "if sample_gt_df is not None and sample_sequence_path is not None:\n",
        "    frame_numbers = sample_gt_df['frame'].unique()\n",
        "    if len(frame_numbers) > 0:\n",
        "        random_frame = random.choice(frame_numbers)\n",
        "        visualize_frame_with_annotations(sample_sequence_path, sample_gt_df, random_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b039de7",
      "metadata": {},
      "source": [
        "###  Custom Dataset Class and Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfabfadb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MOTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for MOT17Det dataset.\n",
        "    \n",
        "    This class handles:\n",
        "    1. Loading frames and annotations\n",
        "    2. Data normalization\n",
        "    3. Data augmentation\n",
        "    4. Formatting data for Faster R-CNN\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, sequences=None, transform=None, augment=False, test_mode=False):\n",
        "        \"\"\"\n",
        "        Initialize the MOT dataset.\n",
        "        \n",
        "        Args:\n",
        "            root_dir (str): Root directory of the MOT dataset\n",
        "            sequences (list): List of sequence names to use (if None, use all available sequences)\n",
        "            transform (callable): Optional transform to be applied to samples\n",
        "            augment (bool): Whether to apply data augmentation\n",
        "            test_mode (bool): Whether to use test mode (no ground truth)\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.test_mode = test_mode\n",
        "        \n",
        "        # Determine the data split (train or test)\n",
        "        self.split = 'test' if test_mode else 'train'\n",
        "        split_dir = os.path.join(root_dir, self.split)\n",
        "        \n",
        "        # Get available sequences\n",
        "        if sequences is None:\n",
        "            self.sequences = [d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))]\n",
        "        else:\n",
        "            self.sequences = sequences\n",
        "        \n",
        "        # Initialize data samples list\n",
        "        self.samples = []\n",
        "        \n",
        "        # Populate samples list\n",
        "        for sequence in self.sequences:\n",
        "            sequence_path = os.path.join(split_dir, sequence)\n",
        "            \n",
        "            # Load sequence info\n",
        "            seq_info = load_sequence_info(sequence_path)\n",
        "            if seq_info is None:\n",
        "                continue\n",
        "            \n",
        "            # Get frame range\n",
        "            frame_count = int(seq_info['seqLength'])\n",
        "            \n",
        "            # Load ground truth if in train mode\n",
        "            gt_df = None\n",
        "            if not test_mode:\n",
        "                gt_file_path = os.path.join(sequence_path, 'gt', 'gt.txt')\n",
        "                gt_df = parse_gt_file(gt_file_path)\n",
        "                if gt_df is None:\n",
        "                    continue\n",
        "            \n",
        "            # Add samples\n",
        "            for frame_number in range(1, frame_count + 1):\n",
        "                # Add frame path and annotations to samples list\n",
        "                frame_path = os.path.join(sequence_path, 'img1', f\"{frame_number:06d}.jpg\")\n",
        "                \n",
        "                # Skip if frame doesn't exist\n",
        "                if not os.path.exists(frame_path):\n",
        "                    continue\n",
        "                \n",
        "                # Get annotations for the current frame\n",
        "                frame_annotations = None\n",
        "                if gt_df is not None:\n",
        "                    frame_annotations = gt_df[gt_df['frame'] == frame_number]\n",
        "                \n",
        "                self.samples.append({\n",
        "                    'frame_path': frame_path,\n",
        "                    'frame_number': frame_number,\n",
        "                    'sequence': sequence,\n",
        "                    'annotations': frame_annotations,\n",
        "                    'seq_info': seq_info\n",
        "                })\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} samples from {len(self.sequences)} sequences\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a sample from the dataset.\n",
        "        \n",
        "        Args:\n",
        "            idx (int): Index of the sample to get\n",
        "            \n",
        "        Returns:\n",
        "            dict: A dictionary containing the sample data\n",
        "        \"\"\"\n",
        "        sample = self.samples[idx]\n",
        "        \n",
        "        # Load the frame\n",
        "        frame_path = sample['frame_path']\n",
        "        frame = cv2.imread(frame_path)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        \n",
        "        # Get annotations\n",
        "        annotations = sample['annotations']\n",
        "        \n",
        "        # Prepare targets for object detection\n",
        "        if annotations is not None and not self.test_mode:\n",
        "            # Extract bounding boxes, labels, and object IDs\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            obj_ids = []\n",
        "            \n",
        "            for _, row in annotations.iterrows():\n",
        "                # Extract bounding box coordinates (x, y, width, height)\n",
        "                x, y, w, h = row['bb_left'], row['bb_top'], row['bb_width'], row['bb_height']\n",
        "                \n",
        "                # Convert to (x1, y1, x2, y2) format required by Faster R-CNN\n",
        "                x1, y1, x2, y2 = x, y, x + w, y + h\n",
        "                \n",
        "                # Add bounding box to the list\n",
        "                boxes.append([x1, y1, x2, y2])\n",
        "                \n",
        "                # Add label (MOT17Det has only one class - person, identified by class=1)\n",
        "                # For Faster R-CNN, we use 1 for person (background is 0)\n",
        "                labels.append(int(row['class']))\n",
        "                \n",
        "                # Add object ID for tracking\n",
        "                obj_ids.append(int(row['id']))\n",
        "            \n",
        "            # Convert to tensors\n",
        "            if boxes:\n",
        "                boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "                labels = torch.tensor(labels, dtype=torch.int64)\n",
        "                obj_ids = torch.tensor(obj_ids, dtype=torch.int64)\n",
        "            else:\n",
        "                # Empty annotations\n",
        "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "                labels = torch.zeros(0, dtype=torch.int64)\n",
        "                obj_ids = torch.zeros(0, dtype=torch.int64)\n",
        "            \n",
        "            target = {\n",
        "                'boxes': boxes,\n",
        "                'labels': labels,\n",
        "                'image_id': torch.tensor([idx]),\n",
        "                'obj_ids': obj_ids\n",
        "            }\n",
        "        else:\n",
        "            # No annotations or test mode\n",
        "            target = {\n",
        "                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
        "                'labels': torch.zeros(0, dtype=torch.int64),\n",
        "                'image_id': torch.tensor([idx]),\n",
        "                'obj_ids': torch.zeros(0, dtype=torch.int64)\n",
        "            }\n",
        "        \n",
        "        # Convert frame to tensor\n",
        "        frame = F.to_tensor(frame)\n",
        "        \n",
        "        # Apply data augmentation if enabled\n",
        "        if self.augment and not self.test_mode:\n",
        "            frame, target = self.apply_augmentation(frame, target)\n",
        "        \n",
        "        # Apply normalization and other transformations\n",
        "        if self.transform is not None:\n",
        "            frame = self.transform(frame)\n",
        "        \n",
        "        return frame, target, sample\n",
        "    \n",
        "    def apply_augmentation(self, image, target):\n",
        "        \"\"\"\n",
        "        Apply data augmentation to the image and target.\n",
        "        \n",
        "        Args:\n",
        "            image (torch.Tensor): Image tensor [C, H, W]\n",
        "            target (dict): Target dictionary with boxes, labels, etc.\n",
        "            \n",
        "        Returns:\n",
        "            tuple: (augmented_image, augmented_target)\n",
        "        \"\"\"\n",
        "        # Skip augmentation if there are no boxes\n",
        "        if target['boxes'].shape[0] == 0:\n",
        "            return image, target\n",
        "        \n",
        "        # Convert image to PIL for some augmentations\n",
        "        image_pil = F.to_pil_image(image)\n",
        "        boxes = target['boxes']\n",
        "        \n",
        "        # Random horizontal flip with 50% probability\n",
        "        if random.random() > 0.5:\n",
        "            image_pil = F.hflip(image_pil)\n",
        "            width = image.shape[2]\n",
        "            # Flip bounding boxes\n",
        "            boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
        "        \n",
        "        # Random brightness and contrast adjustment\n",
        "        if random.random() > 0.5:\n",
        "            brightness_factor = random.uniform(0.8, 1.2)\n",
        "            contrast_factor = random.uniform(0.8, 1.2)\n",
        "            image_pil = F.adjust_brightness(image_pil, brightness_factor)\n",
        "            image_pil = F.adjust_contrast(image_pil, contrast_factor)\n",
        "        \n",
        "        # Random color jittering for robustness\n",
        "        if random.random() > 0.5:\n",
        "            saturation_factor = random.uniform(0.8, 1.2)\n",
        "            hue_factor = random.uniform(-0.1, 0.1)\n",
        "            image_pil = F.adjust_saturation(image_pil, saturation_factor)\n",
        "            image_pil = F.adjust_hue(image_pil, hue_factor)\n",
        "        \n",
        "        # Convert back to tensor\n",
        "        image = F.to_tensor(image_pil)\n",
        "        \n",
        "        # Update target with augmented boxes\n",
        "        target['boxes'] = boxes\n",
        "        \n",
        "        return image, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b675e04d",
      "metadata": {},
      "source": [
        "### Data normalization and transformation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20558f2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    \"\"\"\n",
        "    Normalize a tensor with mean and standard deviation.\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image to be normalized\n",
        "        Returns:\n",
        "            Tensor: Normalized image\n",
        "        \"\"\"\n",
        "        return F.normalize(tensor, mean=self.mean, std=self.std)\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    \"\"\"\n",
        "    Get transformation for the dataset.\n",
        "    \n",
        "    Args:\n",
        "        train (bool): Whether to use training transformations\n",
        "        \n",
        "    Returns:\n",
        "        callable: Transformation function\n",
        "    \"\"\"\n",
        "    transforms = []\n",
        "    \n",
        "    transforms.append(Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "    \n",
        "    return torchvision.transforms.Compose(transforms)\n",
        "\n",
        "# Function to create train and validation datasets\n",
        "def create_datasets(dataset_path, val_sequences=None, augment=True):\n",
        "    \"\"\"\n",
        "    Create training and validation datasets.\n",
        "    \n",
        "    Args:\n",
        "        dataset_path (str): Path to the MOT dataset\n",
        "        val_sequences (list): List of sequence names to use for validation\n",
        "        augment (bool): Whether to apply data augmentation\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (train_dataset, val_dataset)\n",
        "    \"\"\"\n",
        "    # Get all available training sequences\n",
        "    train_dir = os.path.join(dataset_path, 'train')\n",
        "    all_sequences = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
        "    \n",
        "    # If validation sequences are not specified, use 20% of sequences for validation\n",
        "    if val_sequences is None:\n",
        "        random.shuffle(all_sequences)\n",
        "        split_idx = max(1, int(len(all_sequences) * 0.8))  # Ensure at least one sequence for validation\n",
        "        train_sequences = all_sequences[:split_idx]\n",
        "        val_sequences = all_sequences[split_idx:]\n",
        "    else:\n",
        "        train_sequences = [seq for seq in all_sequences if seq not in val_sequences]\n",
        "    \n",
        "    print(f\"Using {len(train_sequences)} sequences for training and {len(val_sequences)} for validation\")\n",
        "    \n",
        "    # Create training dataset\n",
        "    train_dataset = MOTDataset(\n",
        "        root_dir=dataset_path,\n",
        "        sequences=train_sequences,\n",
        "        transform=get_transform(train=True),\n",
        "        augment=augment,\n",
        "        test_mode=False\n",
        "    )\n",
        "    \n",
        "    # Create validation dataset\n",
        "    val_dataset = MOTDataset(\n",
        "        root_dir=dataset_path,\n",
        "        sequences=val_sequences,\n",
        "        transform=get_transform(train=False),\n",
        "        augment=False,\n",
        "        test_mode=False\n",
        "    )\n",
        "    \n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Visualization function to verify the dataset and augmentation\n",
        "def visualize_dataset_sample(dataset, sample_idx=0, show_augmented=False):\n",
        "    \"\"\"\n",
        "    Visualize a sample from the dataset with or without augmentation.\n",
        "    \n",
        "    Args:\n",
        "        dataset (MOTDataset): Dataset to visualize\n",
        "        sample_idx (int): Index of the sample to visualize\n",
        "        show_augmented (bool): Whether to show augmentation effects\n",
        "    \"\"\"\n",
        "    # Get the sample\n",
        "    image, target, sample_info = dataset[sample_idx]\n",
        "    \n",
        "    # If showing augmentation, apply it manually\n",
        "    if show_augmented and dataset.augment:\n",
        "        # Get original image without augmentation\n",
        "        dataset_no_aug = MOTDataset(\n",
        "            root_dir=dataset.root_dir,\n",
        "            sequences=[sample_info['sequence']],\n",
        "            transform=None,\n",
        "            augment=False,\n",
        "            test_mode=dataset.test_mode\n",
        "        )\n",
        "        \n",
        "        # Find matching sample in non-augmented dataset\n",
        "        for i in range(len(dataset_no_aug)):\n",
        "            _, _, info = dataset_no_aug[i]\n",
        "            if (info['sequence'] == sample_info['sequence'] and\n",
        "                info['frame_number'] == sample_info['frame_number']):\n",
        "                orig_image, orig_target, _ = dataset_no_aug[i]\n",
        "                break\n",
        "        else:\n",
        "            print(\"Couldn't find matching sample without augmentation\")\n",
        "            return\n",
        "        \n",
        "        # Display both original and augmented images\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "        \n",
        "        # Convert tensors back to numpy for visualization\n",
        "        orig_image_np = orig_image.permute(1, 2, 0).numpy()\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # Denormalize if transform was applied\n",
        "        if dataset.transform is not None:\n",
        "            # Approximate denormalization\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            image_np = std * image_np + mean\n",
        "            image_np = np.clip(image_np, 0, 1)\n",
        "        \n",
        "        # Display original image with boxes\n",
        "        axes[0].imshow(orig_image_np)\n",
        "        axes[0].set_title(\"Original Image\")\n",
        "        \n",
        "        # Display boxes on original image\n",
        "        for box in orig_target['boxes']:\n",
        "            x1, y1, x2, y2 = box.numpy()\n",
        "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            axes[0].add_patch(rect)\n",
        "        \n",
        "        # Display augmented image with boxes\n",
        "        axes[1].imshow(image_np)\n",
        "        axes[1].set_title(\"Augmented Image\")\n",
        "        \n",
        "        # Display boxes on augmented image\n",
        "        for box in target['boxes']:\n",
        "            x1, y1, x2, y2 = box.numpy()\n",
        "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            axes[1].add_patch(rect)\n",
        "        \n",
        "        axes[0].axis('off')\n",
        "        axes[1].axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        # Simply display the single image with boxes\n",
        "        # Convert tensor back to numpy for visualization\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # Denormalize if transform was applied\n",
        "        if dataset.transform is not None:\n",
        "            # Approximate denormalization\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            image_np = std * image_np + mean\n",
        "            image_np = np.clip(image_np, 0, 1)\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(image_np)\n",
        "        \n",
        "        # Display boxes\n",
        "        for box in target['boxes']:\n",
        "            x1, y1, x2, y2 = box.numpy()\n",
        "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "        \n",
        "        plt.title(f\"Sample from {sample_info['sequence']}, Frame {sample_info['frame_number']}\")\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d22dd36d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "# Create sample datasets\n",
        "train_dataset, val_dataset = create_datasets(DATASET_PATH)\n",
        "\n",
        "# Visualize a sample from the training dataset\n",
        "if len(train_dataset) > 0:\n",
        "    # Show original sample\n",
        "    visualize_dataset_sample(train_dataset, sample_idx=0, show_augmented=False)\n",
        "    \n",
        "    # Show augmentation effects\n",
        "    visualize_dataset_sample(train_dataset, sample_idx=0, show_augmented=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70209e47",
      "metadata": {},
      "source": [
        "### Advanced Object Tracking and Detection in Video Streams\n",
        "\n",
        "#### Part 3: Model Development - Faster R-CNN and Tracking\n",
        "\n",
        "This integrates the Faster R-CNN model with tracking capabilities. It uses a pretrained Faster R-CNN model with ResNet50 backbone and FPN (Feature Pyramid Network) for object detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce00d2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FasterRCNNTracker(nn.Module):\n",
        "    \"\"\"\n",
        "    Faster R-CNN model with tracking capabilities.\n",
        "    \n",
        "    This model combines the Faster R-CNN object detector with \n",
        "    temporal consistency checks and adaptive tracking.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=2, pretrained=True):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "        \n",
        "        Args:\n",
        "            num_classes (int): Number of classes to detect (including background)\n",
        "            pretrained (bool): Whether to use pretrained weights\n",
        "        \"\"\"\n",
        "        super(FasterRCNNTracker, self).__init__()\n",
        "        \n",
        "        # Initialize the Faster R-CNN model\n",
        "        if pretrained:\n",
        "            # Use pretrained weights\n",
        "            weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "            self.model = fasterrcnn_resnet50_fpn(weights=weights)\n",
        "            \n",
        "            # Replace the classification head with a new one for our classes\n",
        "            in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "            self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "        else:\n",
        "            # Initialize with random weights\n",
        "            self.model = fasterrcnn_resnet50_fpn(num_classes=num_classes)\n",
        "        \n",
        "        # Tracking state\n",
        "        self.tracks = {}  # Dictionary to store active tracks: {track_id: track_info}\n",
        "        self.next_track_id = 0  # Next available track ID\n",
        "        \n",
        "        # Tracking parameters\n",
        "        self.detection_threshold = 0.7  # Minimum detection confidence\n",
        "        self.iou_threshold = 0.3  # Minimum IoU for track association\n",
        "        self.max_age = 30  # Maximum frames to keep a track alive without matches\n",
        "        \n",
        "        # Kalman filter parameters for adaptive tracking\n",
        "        self.use_kalman = True  # Whether to use Kalman filtering\n",
        "        self.kalman_states = {}  # Dictionary to store Kalman filter states\n",
        "    \n",
        "    def forward(self, images, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass for training and inference.\n",
        "        \n",
        "        Args:\n",
        "            images (list): List of input images\n",
        "            targets (list, optional): List of target dictionaries for training\n",
        "            \n",
        "        Returns:\n",
        "            dict or losses: Detection results or training losses\n",
        "        \"\"\"\n",
        "        return self.model(images, targets)\n",
        "    \n",
        "    def detect(self, image, threshold=None):\n",
        "        \"\"\"\n",
        "        Perform object detection on a single image.\n",
        "        \n",
        "        Args:\n",
        "            image (torch.Tensor): Input image tensor\n",
        "            threshold (float, optional): Detection confidence threshold\n",
        "            \n",
        "        Returns:\n",
        "            dict: Dictionary with detection results\n",
        "        \"\"\"\n",
        "        # Set threshold\n",
        "        if threshold is None:\n",
        "            threshold = self.detection_threshold\n",
        "        \n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Move image to the correct device\n",
        "        device = next(self.model.parameters()).device\n",
        "        image = image.to(device)\n",
        "        \n",
        "        # Perform inference\n",
        "        with torch.no_grad():\n",
        "            prediction = self.model([image])\n",
        "        \n",
        "        # Get predictions for the first (and only) image\n",
        "        boxes = prediction[0]['boxes'].cpu()\n",
        "        scores = prediction[0]['scores'].cpu()\n",
        "        labels = prediction[0]['labels'].cpu()\n",
        "        \n",
        "        # Filter by threshold\n",
        "        keep = scores > threshold\n",
        "        boxes = boxes[keep]\n",
        "        scores = scores[keep]\n",
        "        labels = labels[keep]\n",
        "        \n",
        "        return {\n",
        "            'boxes': boxes,\n",
        "            'scores': scores,\n",
        "            'labels': labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1eab45e",
      "metadata": {},
      "source": [
        "#### Kalman Filter implementation for object tracking\n",
        "\n",
        "A complete implementation of the Kalman Filter algorithm for adaptive tracking. The Kalman filter predicts object positions based on their previous movements, which is especially useful when objects are temporarily occluded or detection is unreliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993e6986",
      "metadata": {},
      "outputs": [],
      "source": [
        "class KalmanFilter:\n",
        "    \"\"\"\n",
        "    Kalman Filter implementation for bounding box tracking.\n",
        "    \n",
        "    This provides adaptive tracking by predicting object positions\n",
        "    based on previous observations and current measurements.\n",
        "    \"\"\"\n",
        "    def __init__(self, initial_state):\n",
        "        \"\"\"\n",
        "        Initialize Kalman Filter with initial state.\n",
        "        \n",
        "        Args:\n",
        "            initial_state (numpy.ndarray): Initial state [x, y, w, h, vx, vy, vw, vh]\n",
        "        \"\"\"\n",
        "        # State dimensions\n",
        "        self.dim_x = 8  # State vector dimension [x, y, w, h, vx, vy, vw, vh]\n",
        "        self.dim_z = 4  # Measurement vector dimension [x, y, w, h]\n",
        "        \n",
        "        # State vector\n",
        "        self.x = np.array(initial_state, dtype=float)\n",
        "        \n",
        "        # State transition matrix (motion model)\n",
        "        self.F = np.eye(self.dim_x)\n",
        "        # Position updates based on velocity\n",
        "        self.F[0, 4] = 1.0  # x += vx\n",
        "        self.F[1, 5] = 1.0  # y += vy\n",
        "        self.F[2, 6] = 1.0  # w += vw\n",
        "        self.F[3, 7] = 1.0  # h += vh\n",
        "        \n",
        "        # Measurement matrix (maps state to measurement)\n",
        "        self.H = np.zeros((self.dim_z, self.dim_x))\n",
        "        self.H[0, 0] = 1.0  # Measure x\n",
        "        self.H[1, 1] = 1.0  # Measure y\n",
        "        self.H[2, 2] = 1.0  # Measure w\n",
        "        self.H[3, 3] = 1.0  # Measure h\n",
        "        \n",
        "        # Covariance matrix\n",
        "        self.P = np.eye(self.dim_x) * 100.0\n",
        "        \n",
        "        # Process noise covariance\n",
        "        self.Q = np.eye(self.dim_x) * 0.01\n",
        "        self.Q[4:, 4:] *= 0.1  # Lower process noise for velocity components\n",
        "        \n",
        "        # Measurement noise covariance\n",
        "        self.R = np.eye(self.dim_z) * 1.0\n",
        "        \n",
        "        # Identity matrix\n",
        "        self.I = np.eye(self.dim_x)\n",
        "    \n",
        "    def predict(self):\n",
        "        \"\"\"\n",
        "        Predict the next state based on the current state.\n",
        "        \n",
        "        Returns:\n",
        "            numpy.ndarray: Predicted measurement [x, y, w, h]\n",
        "        \"\"\"\n",
        "        # State prediction\n",
        "        self.x = np.dot(self.F, self.x)\n",
        "        \n",
        "        # Covariance prediction\n",
        "        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q\n",
        "        \n",
        "        # Return predicted measurement\n",
        "        return np.dot(self.H, self.x)\n",
        "    \n",
        "    def update(self, z):\n",
        "        \"\"\"\n",
        "        Update the state based on the measurement.\n",
        "        \n",
        "        Args:\n",
        "            z (numpy.ndarray): Measurement [x, y, w, h]\n",
        "        \"\"\"\n",
        "        # Measurement residual\n",
        "        y = z - np.dot(self.H, self.x)\n",
        "        \n",
        "        # Residual covariance\n",
        "        S = np.dot(np.dot(self.H, self.P), self.H.T) + self.R\n",
        "        \n",
        "        # Kalman gain\n",
        "        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))\n",
        "        \n",
        "        # State update\n",
        "        self.x = self.x + np.dot(K, y)\n",
        "        \n",
        "        # Covariance update\n",
        "        self.P = np.dot((self.I - np.dot(K, self.H)), self.P)\n",
        "    \n",
        "    def get_state(self):\n",
        "        \"\"\"\n",
        "        Get the current state.\n",
        "        \n",
        "        Returns:\n",
        "            numpy.ndarray: Current state [x, y, w, h, vx, vy, vw, vh]\n",
        "        \"\"\"\n",
        "        return self.x.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c9db578",
      "metadata": {},
      "source": [
        "#### Temporal consistency tracker\n",
        "\n",
        "This implements the temporal consistency checks to ensure that detected objects maintain consistent identities across frames using IoU-based matching and the Kalman Filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87cddc27",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TemporalConsistencyTracker:\n",
        "    \"\"\"\n",
        "    Implements temporal consistency checks for object tracking.\n",
        "    \n",
        "    This tracker ensures that detected objects maintain consistent\n",
        "    identities across frames by using IoU-based matching and Kalman filtering.\n",
        "    \"\"\"\n",
        "    def __init__(self, iou_threshold=0.3, max_age=30, min_hits=3, detection_threshold=0.7):\n",
        "        \"\"\"\n",
        "        Initialize the tracker.\n",
        "        \n",
        "        Args:\n",
        "            iou_threshold (float): Minimum IoU for track association\n",
        "            max_age (int): Maximum frames to keep a track alive without matches\n",
        "            min_hits (int): Minimum hits before a track is considered valid\n",
        "            detection_threshold (float): Minimum detection confidence\n",
        "        \"\"\"\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.max_age = max_age\n",
        "        self.min_hits = min_hits\n",
        "        self.detection_threshold = detection_threshold\n",
        "        \n",
        "        self.tracks = {}  # Active tracks: {track_id: track_info}\n",
        "        self.next_track_id = 0  # Next available track ID\n",
        "        \n",
        "        # Flags\n",
        "        self.use_kalman = True  # Whether to use Kalman filtering\n",
        "    \n",
        "    def update(self, detections, frame_id):\n",
        "        \"\"\"\n",
        "        Update tracks with new detections.\n",
        "        \n",
        "        Args:\n",
        "            detections (dict): Detection results with 'boxes', 'scores', 'labels'\n",
        "            frame_id (int): Current frame ID\n",
        "            \n",
        "        Returns:\n",
        "            dict: Current tracks\n",
        "        \"\"\"\n",
        "        # Get detection boxes and scores\n",
        "        detected_boxes = detections['boxes']\n",
        "        detected_scores = detections['scores']\n",
        "        detected_labels = detections['labels']\n",
        "        \n",
        "        # Initialize arrays to keep track of matches\n",
        "        matched_track_indices = []\n",
        "        matched_detection_indices = []\n",
        "        \n",
        "        # Compute IoU between tracks and detections\n",
        "        if self.tracks and len(detected_boxes) > 0:\n",
        "            # Extract track boxes\n",
        "            track_ids = list(self.tracks.keys())\n",
        "            track_boxes = torch.stack([self.tracks[track_id]['box'] for track_id in track_ids])\n",
        "            \n",
        "            # Calculate IoU matrix\n",
        "            iou_matrix = box_iou(track_boxes, detected_boxes)\n",
        "            \n",
        "            # Match tracks to detections based on IoU\n",
        "            for track_idx, track_id in enumerate(track_ids):\n",
        "                # Get best matching detection for this track\n",
        "                if torch.any(iou_matrix[track_idx] >= self.iou_threshold):\n",
        "                    # Get detection with highest IoU\n",
        "                    det_idx = torch.argmax(iou_matrix[track_idx]).item()\n",
        "                    \n",
        "                    # If detection already matched to another track, keep the one with higher IoU\n",
        "                    if det_idx in matched_detection_indices:\n",
        "                        # Find the track that already matched to this detection\n",
        "                        existing_match_idx = matched_detection_indices.index(det_idx)\n",
        "                        existing_track_idx = matched_track_indices[existing_match_idx]\n",
        "                        existing_track_id = track_ids[existing_track_idx]\n",
        "                        \n",
        "                        # Compare IoUs\n",
        "                        if iou_matrix[track_idx, det_idx] > iou_matrix[existing_track_idx, det_idx]:\n",
        "                            # Current track has higher IoU, update the match\n",
        "                            matched_track_indices[existing_match_idx] = track_idx\n",
        "                            self._update_track(track_id, detected_boxes[det_idx], detected_scores[det_idx], \n",
        "                                              detected_labels[det_idx], frame_id)\n",
        "                            \n",
        "                            # Mark the existing track as unmatched\n",
        "                            self.tracks[existing_track_id]['age'] += 1\n",
        "                        else:\n",
        "                            # Existing track has higher IoU, keep it and mark current track as unmatched\n",
        "                            self.tracks[track_id]['age'] += 1\n",
        "                    else:\n",
        "                        # This is a new match\n",
        "                        matched_track_indices.append(track_idx)\n",
        "                        matched_detection_indices.append(det_idx)\n",
        "                        self._update_track(track_id, detected_boxes[det_idx], detected_scores[det_idx], \n",
        "                                          detected_labels[det_idx], frame_id)\n",
        "                else:\n",
        "                    # No match for this track, increase age\n",
        "                    self.tracks[track_id]['age'] += 1\n",
        "        \n",
        "        # Initialize new tracks for unmatched detections\n",
        "        for det_idx, (box, score, label) in enumerate(zip(detected_boxes, detected_scores, detected_labels)):\n",
        "            if det_idx not in matched_detection_indices and score >= self.detection_threshold:\n",
        "                # This is a new track\n",
        "                self._init_track(self.next_track_id, box, score, label, frame_id)\n",
        "                self.next_track_id += 1\n",
        "        \n",
        "        # Remove old tracks\n",
        "        track_ids_to_remove = []\n",
        "        for track_id, track_info in self.tracks.items():\n",
        "            if track_info['age'] > self.max_age:\n",
        "                track_ids_to_remove.append(track_id)\n",
        "        \n",
        "        for track_id in track_ids_to_remove:\n",
        "            del self.tracks[track_id]\n",
        "        \n",
        "        return self.tracks\n",
        "    \n",
        "    def _init_track(self, track_id, box, score, label, frame_id):\n",
        "        \"\"\"\n",
        "        Initialize a new track.\n",
        "        \n",
        "        Args:\n",
        "            track_id (int): Track ID\n",
        "            box (torch.Tensor): Bounding box\n",
        "            score (float): Detection score\n",
        "            label (int): Class label\n",
        "            frame_id (int): Current frame ID\n",
        "        \"\"\"\n",
        "        # Convert box to numpy format [x, y, w, h] for Kalman filter\n",
        "        x1, y1, x2, y2 = box.numpy()\n",
        "        width = x2 - x1\n",
        "        height = y2 - y1\n",
        "        \n",
        "        self.tracks[track_id] = {\n",
        "            'box': box,\n",
        "            'score': score,\n",
        "            'label': label,\n",
        "            'age': 0,\n",
        "            'hits': 1,\n",
        "            'history': [(frame_id, box.tolist())],\n",
        "            'velocity': torch.zeros(4)  # Initialize velocity for adaptive tracking\n",
        "        }\n",
        "        \n",
        "        # Initialize Kalman filter if enabled\n",
        "        if self.use_kalman:\n",
        "            # Initial state: [x, y, w, h, vx, vy, vw, vh]\n",
        "            initial_state = [x1, y1, width, height, 0, 0, 0, 0]\n",
        "            self.tracks[track_id]['kalman'] = KalmanFilter(initial_state)\n",
        "    \n",
        "    def _update_track(self, track_id, box, score, label, frame_id):\n",
        "        \"\"\"\n",
        "        Update an existing track with a new detection.\n",
        "        \n",
        "        Args:\n",
        "            track_id (int): Track ID\n",
        "            box (torch.Tensor): New bounding box\n",
        "            score (float): New detection score\n",
        "            label (int): New class label\n",
        "            frame_id (int): Current frame ID\n",
        "        \"\"\"\n",
        "        # Convert box to numpy for Kalman filter\n",
        "        x1, y1, x2, y2 = box.numpy()\n",
        "        width = x2 - x1\n",
        "        height = y2 - y1\n",
        "        \n",
        "        # Apply Kalman filter if enabled\n",
        "        if self.use_kalman and 'kalman' in self.tracks[track_id]:\n",
        "            # Predict new position\n",
        "            self.tracks[track_id]['kalman'].predict()\n",
        "            \n",
        "            # Update Kalman filter with measurement\n",
        "            self.tracks[track_id]['kalman'].update(np.array([x1, y1, width, height]))\n",
        "            \n",
        "            # Get updated state\n",
        "            state = self.tracks[track_id]['kalman'].get_state()\n",
        "            \n",
        "            # Convert back to [x1, y1, x2, y2] format for tracking\n",
        "            kalman_box = torch.tensor([\n",
        "                state[0], state[1], state[0] + state[2], state[1] + state[3]\n",
        "            ])\n",
        "            \n",
        "            # Use weighted average between detection and Kalman prediction\n",
        "            # Weight based on detection confidence and track age\n",
        "            alpha = min(0.9, score) / (1.0 + 0.1 * self.tracks[track_id]['age'])\n",
        "            box = alpha * box + (1.0 - alpha) * kalman_box\n",
        "        \n",
        "        # Calculate velocity for adaptive tracking\n",
        "        prev_box = self.tracks[track_id]['box']\n",
        "        velocity = box - prev_box\n",
        "        \n",
        "        # Update track information\n",
        "        self.tracks[track_id]['box'] = box\n",
        "        self.tracks[track_id]['score'] = score\n",
        "        self.tracks[track_id]['label'] = label\n",
        "        self.tracks[track_id]['age'] = 0\n",
        "        self.tracks[track_id]['hits'] += 1\n",
        "        self.tracks[track_id]['history'].append((frame_id, box.tolist()))\n",
        "        self.tracks[track_id]['velocity'] = velocity\n",
        "        \n",
        "        # Keep a reasonable history length\n",
        "        if len(self.tracks[track_id]['history']) > 30:\n",
        "            self.tracks[track_id]['history'] = self.tracks[track_id]['history'][-30:]\n",
        "    \n",
        "    def get_active_tracks(self):\n",
        "        \"\"\"\n",
        "        Get active tracks that have been confirmed with enough hits.\n",
        "        \n",
        "        Returns:\n",
        "            dict: Dictionary of active tracks\n",
        "        \"\"\"\n",
        "        return {\n",
        "            track_id: track_info \n",
        "            for track_id, track_info in self.tracks.items() \n",
        "            if track_info['hits'] >= self.min_hits\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15528f8d",
      "metadata": {},
      "source": [
        "#### Advanced Object Tracking and Detection in Video Streams\n",
        "##### Part 4 - Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa6ef61c",
      "metadata": {},
      "source": [
        "##### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0eb73d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_ap_for_class(predictions, targets, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate Average Precision for a single class.\n",
        "    \n",
        "    Args:\n",
        "        predictions (list): List of (boxes, scores) tuples for this class\n",
        "        targets (list): List of target boxes for this class\n",
        "        iou_threshold (float): IoU threshold for matching\n",
        "        \n",
        "    Returns:\n",
        "        float: AP value\n",
        "    \"\"\"\n",
        "    # Flatten all predictions and assign image indices\n",
        "    all_pred_boxes = []\n",
        "    all_pred_scores = []\n",
        "    all_pred_img_idx = []\n",
        "    \n",
        "    for img_idx, (pred_boxes, pred_scores) in enumerate(predictions):\n",
        "        all_pred_boxes.append(pred_boxes)\n",
        "        all_pred_scores.append(pred_scores)\n",
        "        all_pred_img_idx.extend([img_idx] * len(pred_boxes))\n",
        "    \n",
        "    if not all_pred_boxes:\n",
        "        return 0.0\n",
        "    \n",
        "    # Concatenate all predictions\n",
        "    all_pred_boxes = torch.cat(all_pred_boxes, dim=0) if all_pred_boxes else torch.zeros((0, 4))\n",
        "    all_pred_scores = torch.cat(all_pred_scores, dim=0) if all_pred_scores else torch.zeros(0)\n",
        "    all_pred_img_idx = torch.tensor(all_pred_img_idx)\n",
        "    \n",
        "    # Sort predictions by score (descending)\n",
        "    sorted_indices = torch.argsort(all_pred_scores, descending=True)\n",
        "    all_pred_boxes = all_pred_boxes[sorted_indices]\n",
        "    all_pred_scores = all_pred_scores[sorted_indices]\n",
        "    all_pred_img_idx = all_pred_img_idx[sorted_indices]\n",
        "    \n",
        "    # Assign ground truth to predictions\n",
        "    gt_matched = [torch.zeros(len(target_boxes), dtype=torch.bool) for target_boxes in targets]\n",
        "    \n",
        "    # True positives and false positives\n",
        "    tp = torch.zeros(len(all_pred_boxes))\n",
        "    fp = torch.zeros(len(all_pred_boxes))\n",
        "    \n",
        "    # For each prediction\n",
        "    for pred_idx, (pred_box, img_idx) in enumerate(zip(all_pred_boxes, all_pred_img_idx)):\n",
        "        # Get ground truth for this image\n",
        "        if img_idx >= len(targets) or len(targets[img_idx]) == 0:\n",
        "            # No ground truth for this image\n",
        "            fp[pred_idx] = 1\n",
        "            continue\n",
        "        \n",
        "        # Calculate IoU with all ground truth boxes\n",
        "        target_boxes = targets[img_idx]\n",
        "        ious = box_iou(pred_box.unsqueeze(0), target_boxes)[0]\n",
        "        \n",
        "        # Find best matching ground truth\n",
        "        if len(ious) > 0:\n",
        "            max_iou, max_idx = torch.max(ious, dim=0)\n",
        "            \n",
        "            # Check if match is valid\n",
        "            if max_iou >= iou_threshold and not gt_matched[img_idx][max_idx]:\n",
        "                tp[pred_idx] = 1\n",
        "                gt_matched[img_idx][max_idx] = True\n",
        "            else:\n",
        "                fp[pred_idx] = 1\n",
        "        else:\n",
        "            fp[pred_idx] = 1\n",
        "    \n",
        "    # Calculate precision and recall\n",
        "    tp_cumsum = torch.cumsum(tp, dim=0)\n",
        "    fp_cumsum = torch.cumsum(fp, dim=0)\n",
        "    \n",
        "    precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)\n",
        "    \n",
        "    # Count total number of ground truth\n",
        "    total_gt = sum(len(target_boxes) for target_boxes in targets)\n",
        "    recall = tp_cumsum / (total_gt + 1e-10)\n",
        "    \n",
        "    # Add start point for precision-recall curve\n",
        "    precision = torch.cat([torch.tensor([1]), precision])\n",
        "    recall = torch.cat([torch.tensor([0]), recall])\n",
        "    \n",
        "    # Calculate AP using all points\n",
        "    ap = torch.trapz(precision, recall).item()\n",
        "    \n",
        "    return ap\n",
        "def calculate_map(predictions, targets, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate mean Average Precision (mAP).\n",
        "    \n",
        "    Args:\n",
        "        predictions (list): List of prediction dictionaries\n",
        "        targets (list): List of target dictionaries\n",
        "        iou_threshold (float): IoU threshold for matching\n",
        "        \n",
        "    Returns:\n",
        "        float: mAP value\n",
        "    \"\"\"\n",
        "    if not predictions or not targets:\n",
        "        return 0.0\n",
        "    \n",
        "    # Group predictions and targets by class\n",
        "    classes = set()\n",
        "    for pred in predictions:\n",
        "        classes.update(pred['labels'].unique().tolist())\n",
        "    for target in targets:\n",
        "        classes.update(target['labels'].unique().tolist())\n",
        "    \n",
        "    # Calculate AP for each class\n",
        "    average_precisions = []\n",
        "    \n",
        "    for cls in classes:\n",
        "        # Skip background class (0)\n",
        "        if cls == 0:\n",
        "            continue\n",
        "            \n",
        "        # Collect all predictions and targets for this class\n",
        "        class_preds = []\n",
        "        class_targets = []\n",
        "        \n",
        "        for pred, target in zip(predictions, targets):\n",
        "            # Get predictions for this class\n",
        "            pred_idx = pred['labels'] == cls\n",
        "            pred_boxes = pred['boxes'][pred_idx]\n",
        "            pred_scores = pred['scores'][pred_idx]\n",
        "            \n",
        "            # Get targets for this class\n",
        "            target_idx = target['labels'] == cls\n",
        "            target_boxes = target['boxes'][target_idx]\n",
        "            \n",
        "            class_preds.append((pred_boxes, pred_scores))\n",
        "            class_targets.append(target_boxes)\n",
        "        \n",
        "        # Calculate AP for this class\n",
        "        ap = calculate_ap_for_class(class_preds, class_targets, iou_threshold)\n",
        "        average_precisions.append(ap)\n",
        "    \n",
        "    # Return mAP (mean of all class APs)\n",
        "    if not average_precisions:\n",
        "        return 0.0\n",
        "    \n",
        "    return sum(average_precisions) / len(average_precisions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf17b11",
      "metadata": {},
      "source": [
        "##### Utility class for tracking metrics during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efafb2ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmoothedValue:\n",
        "    \"\"\"\n",
        "    Track a series of values and provide access to smoothed values over a window.\n",
        "    \"\"\"\n",
        "    def __init__(self, window_size=20):\n",
        "        self.window_size = window_size\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.values = []\n",
        "        self.count = 0\n",
        "        self.sum = 0.0\n",
        "        self.avg = 0.0\n",
        "        \n",
        "    def update(self, value):\n",
        "        self.values.append(value)\n",
        "        self.count += 1\n",
        "        self.sum += value\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "        if len(self.values) > self.window_size:\n",
        "            self.sum -= self.values.pop(0)\n",
        "            self.count -= 1\n",
        "            self.avg = self.sum / self.count\n",
        "            \n",
        "    def get_median(self):\n",
        "        return np.median(np.array(self.values))\n",
        "    \n",
        "    def get_avg(self):\n",
        "        return self.avg\n",
        "    \n",
        "    def get_global_avg(self):\n",
        "        return self.sum / self.count\n",
        "\n",
        "class MetricLogger:\n",
        "    \"\"\"\n",
        "    Logger for tracking training and validation metrics.\n",
        "    \"\"\"\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "        \n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            self.meters[k].update(v)\n",
        "            \n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        return object.__getattr__(self, attr)\n",
        "        \n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(f\"{name}: {meter.get_avg():.4f}\")\n",
        "        return self.delimiter.join(loss_str)\n",
        "    \n",
        "    def add_meter(self, name, meter=None):\n",
        "        if meter is None:\n",
        "            meter = SmoothedValue()\n",
        "        self.meters[name] = meter\n",
        "        \n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        if header is not None:\n",
        "            print(header)\n",
        "            \n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue()\n",
        "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
        "        \n",
        "        for i, obj in enumerate(iterable):\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            \n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.get_avg() * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                \n",
        "                print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} \"\n",
        "                     f\"[{i:{space_fmt}}/{len(iterable)}] \"\n",
        "                     f\"eta: {eta_string} \"\n",
        "                     f\"time: {iter_time.get_avg():.4f} \"\n",
        "                     f\"{str(self)}\")\n",
        "                \n",
        "            end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc446a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training utility functions\n",
        "def train_one_epoch(model, data_loader, optimizer, device, epoch, print_freq=10):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "    \n",
        "    Args:\n",
        "        model (FasterRCNNTracker): Model to train\n",
        "        data_loader (DataLoader): Training data loader\n",
        "        optimizer (Optimizer): Optimizer\n",
        "        device (device): Device to train on\n",
        "        epoch (int): Current epoch number\n",
        "        print_freq (int): Frequency of printing logs\n",
        "        \n",
        "    Returns:\n",
        "        dict: Loss statistics\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    # Initialize metrics\n",
        "    metric_logger = MetricLogger()\n",
        "    metric_logger.add_meter('lr', SmoothedValue())\n",
        "    header = f'Epoch: [{epoch}]'\n",
        "    \n",
        "    # Update learning rate\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "        \n",
        "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "        )\n",
        "    \n",
        "    # Train loop\n",
        "    for i, (images, targets, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items() if k != 'obj_ids'} for t in targets]\n",
        "        \n",
        "        # Forward pass and compute loss\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        # Compute gradient and optimize\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "        \n",
        "        # Update metrics\n",
        "        metric_logger.update(loss=losses, **loss_dict)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "    \n",
        "    return metric_logger\n",
        "\n",
        "def validate(model, data_loader, device, print_freq=10):\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    \n",
        "    Args:\n",
        "        model (FasterRCNNTracker): Model to validate\n",
        "        data_loader (DataLoader): Validation data loader\n",
        "        device (device): Device to validate on\n",
        "        print_freq (int): Frequency of printing logs\n",
        "        \n",
        "    Returns:\n",
        "        dict: Validation metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metric_logger = MetricLogger()\n",
        "    header = 'Validation:'\n",
        "    \n",
        "    # Lists to store all predictions and targets for mAP calculation\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, targets, _ in metric_logger.log_every(data_loader, print_freq, header):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items() if k != 'obj_ids'} for t in targets]\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            \n",
        "            # Collect predictions and targets for mAP calculation\n",
        "            for i, (output, target) in enumerate(zip(outputs, targets)):\n",
        "                pred_boxes = output['boxes'].cpu()\n",
        "                pred_scores = output['scores'].cpu()\n",
        "                pred_labels = output['labels'].cpu()\n",
        "                \n",
        "                gt_boxes = target['boxes'].cpu()\n",
        "                gt_labels = target['labels'].cpu()\n",
        "                \n",
        "                # Skip empty predictions or targets\n",
        "                if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
        "                    continue\n",
        "                \n",
        "                all_predictions.append({\n",
        "                    'boxes': pred_boxes,\n",
        "                    'scores': pred_scores,\n",
        "                    'labels': pred_labels\n",
        "                })\n",
        "                \n",
        "                all_targets.append({\n",
        "                    'boxes': gt_boxes,\n",
        "                    'labels': gt_labels\n",
        "                })\n",
        "    \n",
        "    # Calculate mAP\n",
        "    mAP = calculate_map(all_predictions, all_targets)\n",
        "    metric_logger.update(mAP=mAP)\n",
        "    \n",
        "    return metric_logger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4707c569",
      "metadata": {},
      "source": [
        "##### Tracking Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab786fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_tracking(tracker, dataset, sequences=None, max_frames_per_seq=None):\n",
        "    \"\"\"\n",
        "    Evaluate tracking performance using MOT metrics.\n",
        "    \n",
        "    Args:\n",
        "        tracker (TemporalConsistencyTracker): Tracker to evaluate\n",
        "        dataset (MOTDataset): Dataset to evaluate on\n",
        "        sequences (list, optional): List of sequences to evaluate\n",
        "        max_frames_per_seq (int, optional): Maximum frames to process per sequence\n",
        "        \n",
        "    Returns:\n",
        "        dict: Evaluation metrics\n",
        "    \"\"\"\n",
        "    # Initialize metrics\n",
        "    metrics = {\n",
        "        'MOTA': 0,  # Multiple Object Tracking Accuracy\n",
        "        'MOTP': 0,  # Multiple Object Tracking Precision\n",
        "        'IDF1': 0,  # ID F1 Score\n",
        "        'IDs': 0,   # ID Switches\n",
        "        'MT': 0,    # Mostly Tracked\n",
        "        'ML': 0,    # Mostly Lost\n",
        "        'FP': 0,    # False Positives\n",
        "        'FN': 0,    # False Negatives\n",
        "        'Recall': 0,\n",
        "        'Precision': 0\n",
        "    }\n",
        "    \n",
        "    # Group samples by sequence\n",
        "    sequences_samples = defaultdict(list)\n",
        "    for i, (_, _, sample) in enumerate(dataset):\n",
        "        seq = sample['sequence']\n",
        "        sequences_samples[seq].append((i, sample['frame_number']))\n",
        "    \n",
        "    # Filter sequences if specified\n",
        "    if sequences:\n",
        "        sequences_samples = {seq: samples for seq, samples in sequences_samples.items() if seq in sequences}\n",
        "    \n",
        "    # Process each sequence\n",
        "    total_gt_tracks = 0\n",
        "    total_pred_tracks = 0\n",
        "    total_matches = 0\n",
        "    total_fp = 0\n",
        "    total_fn = 0\n",
        "    total_id_switches = 0\n",
        "    total_gt_objects = 0\n",
        "    \n",
        "    sequence_metrics = {}\n",
        "    \n",
        "    for seq, samples in sequences_samples.items():\n",
        "        print(f\"Evaluating sequence: {seq}\")\n",
        "        \n",
        "        # Sort samples by frame number\n",
        "        samples.sort(key=lambda x: x[1])\n",
        "        \n",
        "        # Limit frames if specified\n",
        "        if max_frames_per_seq:\n",
        "            samples = samples[:max_frames_per_seq]\n",
        "        \n",
        "        # Reset tracker for each sequence\n",
        "        tracker.tracks = {}\n",
        "        tracker.next_track_id = 0\n",
        "        \n",
        "        # Store mapping between ground truth and predicted tracks\n",
        "        gt_to_pred = {}\n",
        "        pred_to_gt = {}\n",
        "        \n",
        "        # Track history for this sequence\n",
        "        track_history = []\n",
        "        \n",
        "        # Process each frame\n",
        "        prev_matched_gt_ids = set()\n",
        "        \n",
        "        for i, (sample_idx, frame_number) in enumerate(tqdm(samples, desc=f\"Processing {seq}\")):\n",
        "            # Get sample\n",
        "            image, target, sample = dataset[sample_idx]\n",
        "            \n",
        "            # Get ground truth for this frame\n",
        "            gt_boxes = target['boxes']\n",
        "            gt_obj_ids = target['obj_ids']\n",
        "            \n",
        "            # Get predictions for this frame (use ground truth as detections for now)\n",
        "            # In a real evaluation, you would use your detector's predictions\n",
        "            detections = {\n",
        "                'boxes': gt_boxes.clone(),\n",
        "                'scores': torch.ones(len(gt_boxes)),\n",
        "                'labels': torch.ones(len(gt_boxes), dtype=torch.int64)\n",
        "            }\n",
        "            \n",
        "            # Update tracker\n",
        "            tracks = tracker.update(detections, frame_number)\n",
        "            \n",
        "            # Match predicted tracks to ground truth\n",
        "            # This is a simple IoU-based matching for demonstration\n",
        "            matched_gt_ids = set()\n",
        "            matched_pred_ids = set()\n",
        "            \n",
        "            # Create IoU matrix between ground truth and predictions\n",
        "            if len(gt_boxes) > 0 and tracks:\n",
        "                pred_boxes = torch.stack([track_info['box'] for track_id, track_info in tracks.items()])\n",
        "                pred_ids = list(tracks.keys())\n",
        "                \n",
        "                # Calculate IoU\n",
        "                iou_matrix = box_iou(gt_boxes, pred_boxes)\n",
        "                \n",
        "                # Match based on highest IoU\n",
        "                for gt_idx, gt_id in enumerate(gt_obj_ids):\n",
        "                    if len(pred_boxes) == 0:\n",
        "                        continue\n",
        "                        \n",
        "                    # Get best matching prediction\n",
        "                    best_iou, best_pred_idx = torch.max(iou_matrix[gt_idx], dim=0)\n",
        "                    \n",
        "                    if best_iou >= 0.5:  # IoU threshold\n",
        "                        pred_id = pred_ids[best_pred_idx]\n",
        "                        \n",
        "                        # Check for ID switch\n",
        "                        if gt_id.item() in prev_matched_gt_ids and gt_id.item() in gt_to_pred and gt_to_pred[gt_id.item()] != pred_id:\n",
        "                            total_id_switches += 1\n",
        "                        \n",
        "                        # Update mappings\n",
        "                        gt_to_pred[gt_id.item()] = pred_id\n",
        "                        pred_to_gt[pred_id] = gt_id.item()\n",
        "                        \n",
        "                        # Mark as matched\n",
        "                        matched_gt_ids.add(gt_id.item())\n",
        "                        matched_pred_ids.add(pred_id)\n",
        "            \n",
        "            # Update metrics\n",
        "            # False positives: predicted tracks that don't match any ground truth\n",
        "            fp = len(tracks) - len(matched_pred_ids)\n",
        "            total_fp += fp\n",
        "            \n",
        "            # False negatives: ground truth objects that aren't matched to any prediction\n",
        "            fn = len(gt_obj_ids) - len(matched_gt_ids)\n",
        "            total_fn += fn\n",
        "            \n",
        "            # Store track information for this frame\n",
        "            track_history.append({\n",
        "                'frame': frame_number,\n",
        "                'gt_boxes': gt_boxes.numpy(),\n",
        "                'gt_ids': gt_obj_ids.numpy(),\n",
        "                'pred_tracks': {k: v.copy() if isinstance(v, dict) else v for k, v in tracks.items()},\n",
        "                'matches': {gt_id: pred_id for gt_id, pred_id in gt_to_pred.items() if gt_id in matched_gt_ids},\n",
        "                'fp': fp,\n",
        "                'fn': fn\n",
        "            })\n",
        "            \n",
        "            # Update previous matched IDs\n",
        "            prev_matched_gt_ids = matched_gt_ids\n",
        "            \n",
        "            # Add to total counts\n",
        "            total_gt_objects += len(gt_obj_ids)\n",
        "        \n",
        "        # Calculate sequence-level metrics\n",
        "        \n",
        "        # Total number of ground truth tracks\n",
        "        gt_tracks = set()\n",
        "        for frame_data in track_history:\n",
        "            gt_tracks.update(frame_data['gt_ids'].tolist())\n",
        "        \n",
        "        # Total number of predicted tracks\n",
        "        pred_tracks = set()\n",
        "        for frame_data in track_history:\n",
        "            pred_tracks.update(frame_data['pred_tracks'].keys())\n",
        "        \n",
        "        # Total matches (unique GT-to-pred mappings)\n",
        "        matches = set()\n",
        "        for frame_data in track_history:\n",
        "            for gt_id, pred_id in frame_data['matches'].items():\n",
        "                matches.add((gt_id, pred_id))\n",
        "        \n",
        "        # Store sequence metrics\n",
        "        seq_fp = sum(frame_data['fp'] for frame_data in track_history)\n",
        "        seq_fn = sum(frame_data['fn'] for frame_data in track_history)\n",
        "        seq_id_switches = sum(1 for i in range(1, len(track_history))\n",
        "                              for gt_id in track_history[i-1]['matches']\n",
        "                              if gt_id in track_history[i]['matches'] and \n",
        "                              track_history[i-1]['matches'][gt_id] != track_history[i]['matches'][gt_id])\n",
        "        \n",
        "        # MOTA = 1 - (FP + FN + ID switches) / Total GT objects\n",
        "        total_gt_in_seq = sum(len(frame_data['gt_ids']) for frame_data in track_history)\n",
        "        seq_mota = 1.0 - (seq_fp + seq_fn + seq_id_switches) / max(1, total_gt_in_seq)\n",
        "        \n",
        "        # Precision and Recall\n",
        "        seq_precision = len(matches) / max(1, len(matches) + seq_fp)\n",
        "        seq_recall = len(matches) / max(1, total_gt_in_seq)\n",
        "        \n",
        "        # IDF1 score\n",
        "        seq_idf1 = 2 * len(matches) / max(1, len(gt_tracks) + len(pred_tracks))\n",
        "        \n",
        "        sequence_metrics[seq] = {\n",
        "            'MOTA': seq_mota,\n",
        "            'Precision': seq_precision,\n",
        "            'Recall': seq_recall,\n",
        "            'IDF1': seq_idf1,\n",
        "            'ID_Switches': seq_id_switches,\n",
        "            'FP': seq_fp,\n",
        "            'FN': seq_fn,\n",
        "            'GT_Tracks': len(gt_tracks),\n",
        "            'Pred_Tracks': len(pred_tracks),\n",
        "            'Matches': len(matches)\n",
        "        }\n",
        "        \n",
        "        # Accumulate for overall metrics\n",
        "        total_gt_tracks += len(gt_tracks)\n",
        "        total_pred_tracks += len(pred_tracks)\n",
        "        total_matches += len(matches)\n",
        "    \n",
        "    # Calculate overall metrics\n",
        "    metrics['MOTA'] = 1.0 - (total_fp + total_fn + total_id_switches) / max(1, total_gt_objects)\n",
        "    metrics['Precision'] = total_matches / max(1, total_matches + total_fp)\n",
        "    metrics['Recall'] = total_matches / max(1, total_gt_objects)\n",
        "    metrics['IDF1'] = 2 * total_matches / max(1, total_gt_tracks + total_pred_tracks)\n",
        "    metrics['IDs'] = total_id_switches\n",
        "    metrics['FP'] = total_fp\n",
        "    metrics['FN'] = total_fn\n",
        "    \n",
        "    # Create evaluation summary\n",
        "    print(\"\\n=== Tracking Evaluation Summary ===\")\n",
        "    print(f\"Total sequences evaluated: {len(sequence_metrics)}\")\n",
        "    print(f\"Total frames: {sum(len(samples) for samples in sequences_samples.values())}\")\n",
        "    print(f\"Overall MOTA: {metrics['MOTA']:.4f}\")\n",
        "    print(f\"Overall IDF1: {metrics['IDF1']:.4f}\")\n",
        "    print(f\"Overall Precision: {metrics['Precision']:.4f}\")\n",
        "    print(f\"Overall Recall: {metrics['Recall']:.4f}\")\n",
        "    print(f\"Total ID Switches: {metrics['IDs']}\")\n",
        "    print(f\"Total False Positives: {metrics['FP']}\")\n",
        "    print(f\"Total False Negatives: {metrics['FN']}\")\n",
        "    \n",
        "    # Print sequence-specific metrics\n",
        "    print(\"\\nSequence-specific metrics:\")\n",
        "    for seq, seq_metrics in sequence_metrics.items():\n",
        "        print(f\"\\n{seq}:\")\n",
        "        for metric, value in seq_metrics.items():\n",
        "            print(f\"  {metric}: {value:.4f}\" if isinstance(value, float) else f\"  {metric}: {value}\")\n",
        "    \n",
        "    return metrics, sequence_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a13da0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, train_dataset, val_dataset, num_epochs=10, batch_size=2, \n",
        "                      lr=0.005, momentum=0.9, weight_decay=0.0005, eval_tracking=True):\n",
        "    \"\"\"\n",
        "    Train and evaluate the model.\n",
        "    \n",
        "    Args:\n",
        "        model (FasterRCNNTracker): Model to train\n",
        "        train_dataset (MOTDataset): Training dataset\n",
        "        val_dataset (MOTDataset): Validation dataset\n",
        "        num_epochs (int): Number of epochs to train\n",
        "        batch_size (int): Batch size\n",
        "        lr (float): Learning rate\n",
        "        momentum (float): Momentum for SGD\n",
        "        weight_decay (float): Weight decay for regularization\n",
        "        eval_tracking (bool): Whether to evaluate tracking performance\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (trained_model, training_metrics, validation_metrics)\n",
        "    \"\"\"\n",
        "    # Create data loaders\n",
        "    # Use num_workers=0 to avoid pickling issues\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=0, collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=0, collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "    \n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    \n",
        "    # Setup optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.SGD(\n",
        "        params, lr=lr, momentum=momentum, weight_decay=weight_decay\n",
        "    )\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "    \n",
        "    # Training metrics\n",
        "    training_metrics = []\n",
        "    validation_metrics = []\n",
        "    \n",
        "    # Train for num_epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "        \n",
        "        # Train one epoch\n",
        "        train_metrics = train_one_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        training_metrics.append(train_metrics)\n",
        "        \n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # Validate\n",
        "        val_metrics = validate(model, val_loader, device)\n",
        "        validation_metrics.append(val_metrics)\n",
        "        \n",
        "        # Print metrics\n",
        "        print(f\"Training loss: {train_metrics.loss.get_global_avg():.4f}\")\n",
        "        print(f\"Validation mAP: {val_metrics.mAP.get_global_avg():.4f}\")\n",
        "        \n",
        "        # Evaluate tracking if requested\n",
        "        if eval_tracking and (epoch + 1) % 5 == 0:  # Evaluate every 5 epochs to save time\n",
        "            # Create tracker using the current model\n",
        "            tracker = TemporalConsistencyTracker(\n",
        "                iou_threshold=0.3,\n",
        "                max_age=30,\n",
        "                min_hits=3,\n",
        "                detection_threshold=0.5\n",
        "            )\n",
        "            \n",
        "            # Evaluate on a subset of validation sequences\n",
        "            tracking_metrics, _ = evaluate_tracking(\n",
        "                tracker, val_dataset, \n",
        "                sequences=None,  # Use all sequences\n",
        "                max_frames_per_seq=50  # Limit frames per sequence for faster evaluation\n",
        "            )\n",
        "            \n",
        "            print(f\"Tracking MOTA: {tracking_metrics['MOTA']:.4f}\")\n",
        "            print(f\"Tracking IDF1: {tracking_metrics['IDF1']:.4f}\")\n",
        "    \n",
        "    return model, training_metrics, validation_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9cb5f83",
      "metadata": {},
      "source": [
        "##### Training and Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b89710d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, train_dataset, val_dataset, num_epochs=10, batch_size=2, \n",
        "                      lr=0.005, momentum=0.9, weight_decay=0.0005, eval_tracking=True):\n",
        "    \"\"\"\n",
        "    Train and evaluate the model.\n",
        "    \n",
        "    Args:\n",
        "        model (FasterRCNNTracker): Model to train\n",
        "        train_dataset (MOTDataset): Training dataset\n",
        "        val_dataset (MOTDataset): Validation dataset\n",
        "        num_epochs (int): Number of epochs to train\n",
        "        batch_size (int): Batch size\n",
        "        lr (float): Learning rate\n",
        "        momentum (float): Momentum for SGD\n",
        "        weight_decay (float): Weight decay for regularization\n",
        "        eval_tracking (bool): Whether to evaluate tracking performance\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (trained_model, training_metrics, validation_metrics)\n",
        "    \"\"\"\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=4, collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=4, collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "    \n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    \n",
        "    # Setup optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.SGD(\n",
        "        params, lr=lr, momentum=momentum, weight_decay=weight_decay\n",
        "    )\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "    \n",
        "    # Training metrics\n",
        "    training_metrics = []\n",
        "    validation_metrics = []\n",
        "    \n",
        "    # Train for num_epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "        \n",
        "        # Train one epoch\n",
        "        train_metrics = train_one_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        training_metrics.append(train_metrics)\n",
        "        \n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # Validate\n",
        "        val_metrics = validate(model, val_loader, device)\n",
        "        validation_metrics.append(val_metrics)\n",
        "        \n",
        "        # Print metrics\n",
        "        print(f\"Training loss: {train_metrics.loss.get_global_avg():.4f}\")\n",
        "        print(f\"Validation mAP: {val_metrics.mAP.get_global_avg():.4f}\")\n",
        "        \n",
        "        # Evaluate tracking if requested\n",
        "        if eval_tracking and (epoch + 1) % 5 == 0:  # Evaluate every 5 epochs to save time\n",
        "            # Create tracker using the current model\n",
        "            tracker = TemporalConsistencyTracker(\n",
        "                iou_threshold=0.3,\n",
        "                max_age=30,\n",
        "                min_hits=3,\n",
        "                detection_threshold=0.5\n",
        "            )\n",
        "            \n",
        "            # Evaluate on a subset of validation sequences\n",
        "            tracking_metrics, _ = evaluate_tracking(\n",
        "                tracker, val_dataset, \n",
        "                sequences=None,  # Use all sequences\n",
        "                max_frames_per_seq=50  # Limit frames per sequence for faster evaluation\n",
        "            )\n",
        "            \n",
        "            print(f\"Tracking MOTA: {tracking_metrics['MOTA']:.4f}\")\n",
        "            print(f\"Tracking IDF1: {tracking_metrics['IDF1']:.4f}\")\n",
        "    \n",
        "    return model, training_metrics, validation_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f56e2a47",
      "metadata": {},
      "source": [
        "##### Visualization Functions for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53887e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_training_metrics(training_metrics, validation_metrics):\n",
        "    \"\"\"\n",
        "    Visualize training and validation metrics.\n",
        "    \n",
        "    Args:\n",
        "        training_metrics (list): List of training metrics\n",
        "        validation_metrics (list): List of validation metrics\n",
        "    \"\"\"\n",
        "    # Extract metrics\n",
        "    epochs = range(1, len(training_metrics) + 1)\n",
        "    train_loss = [m.loss.get_global_avg() for m in training_metrics]\n",
        "    val_map = [m.mAP.get_global_avg() for m in validation_metrics]\n",
        "    \n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Plot training loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Plot validation mAP\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, val_map, 'ro-', label='Validation mAP')\n",
        "    plt.title('Validation mAP')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mAP')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_detection_results(model, dataset, num_samples=5, detection_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Visualize detection results on random samples.\n",
        "    \n",
        "    Args:\n",
        "        model (FasterRCNNTracker): Trained model\n",
        "        dataset (MOTDataset): Dataset to visualize\n",
        "        num_samples (int): Number of samples to visualize\n",
        "        detection_threshold (float): Detection confidence threshold\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Get random samples\n",
        "    indices = torch.randperm(len(dataset))[:num_samples].tolist()\n",
        "    \n",
        "    for idx in indices:\n",
        "        # Get sample\n",
        "        image, target, sample = dataset[idx]\n",
        "        \n",
        "        # Move to device\n",
        "        image = image.to(device)\n",
        "        \n",
        "        # Perform detection\n",
        "        with torch.no_grad():\n",
        "            prediction = model([image])[0]\n",
        "        \n",
        "        # Filter by threshold\n",
        "        keep = prediction['scores'] > detection_threshold\n",
        "        boxes = prediction['boxes'][keep].cpu()\n",
        "        scores = prediction['scores'][keep].cpu()\n",
        "        labels = prediction['labels'][keep].cpu()\n",
        "        \n",
        "        # Get ground truth\n",
        "        gt_boxes = target['boxes']\n",
        "        gt_labels = target['labels']\n",
        "        \n",
        "        # Convert image for visualization\n",
        "        image_np = image.cpu().permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # Denormalize\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image_np = std * image_np + mean\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "        \n",
        "        # Create figure\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        \n",
        "        # Plot ground truth\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(image_np)\n",
        "        plt.title('Ground Truth')\n",
        "        \n",
        "        for box, label in zip(gt_boxes, gt_labels):\n",
        "            x1, y1, x2, y2 = box\n",
        "            rect = Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='g', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "            \n",
        "            class_name = 'Person' if label == 1 else f'Class {label}'\n",
        "            plt.text(x1, y1 - 5, class_name, bbox=dict(facecolor='g', alpha=0.5), fontsize=8, color='white')\n",
        "        \n",
        "        # Plot predictions\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(image_np)\n",
        "        plt.title('Predictions')\n",
        "        \n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            x1, y1, x2, y2 = box\n",
        "            rect = Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "            \n",
        "            class_name = 'Person' if label == 1 else f'Class {label}'\n",
        "            plt.text(x1, y1 - 5, f'{class_name}: {score:.2f}', bbox=dict(facecolor='r', alpha=0.5), fontsize=8, color='white')\n",
        "        \n",
        "        # Add sequence and frame info\n",
        "        plt.suptitle(f\"Sequence: {sample['sequence']}, Frame: {sample['frame_number']}\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def visualize_tracking_results(model, tracker, dataset, sequence, start_frame=1, num_frames=10, detection_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Visualize tracking results on a sequence.\n",
        "    \n",
        "    Args:\n",
        "        model (FasterRCNNTracker): Trained detection model\n",
        "        tracker (TemporalConsistencyTracker): Tracker \n",
        "        dataset (MOTDataset): Dataset to visualize\n",
        "        sequence (str): Sequence name\n",
        "        start_frame (int): Starting frame number\n",
        "        num_frames (int): Number of frames to visualize\n",
        "        detection_threshold (float): Detection confidence threshold\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Filter samples for the sequence\n",
        "    sequence_samples = []\n",
        "    for i, (_, _, sample) in enumerate(dataset):\n",
        "        if sample['sequence'] == sequence and sample['frame_number'] >= start_frame:\n",
        "            sequence_samples.append((i, sample['frame_number']))\n",
        "    \n",
        "    # Sort by frame number\n",
        "    sequence_samples.sort(key=lambda x: x[1])\n",
        "    \n",
        "    # Limit to num_frames\n",
        "    sequence_samples = sequence_samples[:num_frames]\n",
        "    \n",
        "    # Reset tracker\n",
        "    tracker.tracks = {}\n",
        "    tracker.next_track_id = 0\n",
        "    \n",
        "    # Process each frame\n",
        "    for i, (sample_idx, frame_number) in enumerate(sequence_samples):\n",
        "        # Get sample\n",
        "        image, target, sample = dataset[sample_idx]\n",
        "        \n",
        "        # Move to device\n",
        "        image = image.to(device)\n",
        "        \n",
        "        # Perform detection\n",
        "        with torch.no_grad():\n",
        "            prediction = model([image])[0]\n",
        "        \n",
        "        # Filter by threshold\n",
        "        keep = prediction['scores'] > detection_threshold\n",
        "        boxes = prediction['boxes'][keep].cpu()\n",
        "        scores = prediction['scores'][keep].cpu()\n",
        "        labels = prediction['labels'][keep].cpu()\n",
        "        \n",
        "        # Create detection dictionary\n",
        "        detections = {\n",
        "            'boxes': boxes,\n",
        "            'scores': scores,\n",
        "            'labels': labels\n",
        "        }\n",
        "        \n",
        "        # Update tracker\n",
        "        tracks = tracker.update(detections, frame_number)\n",
        "        \n",
        "        # Convert image for visualization\n",
        "        image_np = image.cpu().permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # Denormalize\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image_np = std * image_np + mean\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "        \n",
        "        # Create figure\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(image_np)\n",
        "        plt.title(f\"Sequence: {sequence}, Frame: {frame_number}\")\n",
        "        \n",
        "        # Generate colors for tracks\n",
        "        track_colors = {}\n",
        "        \n",
        "        # Plot tracks\n",
        "        for track_id, track_info in tracks.items():\n",
        "            if track_id not in track_colors:\n",
        "                track_colors[track_id] = np.random.rand(3,)\n",
        "            \n",
        "            color = track_colors[track_id]\n",
        "            box = track_info['box']\n",
        "            x1, y1, x2, y2 = box\n",
        "            \n",
        "            # Plot bounding box\n",
        "            rect = Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor=color, facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "            \n",
        "            # Plot track ID\n",
        "            plt.text(x1, y1 - 10, f\"ID: {track_id}\", fontsize=10, color='white',\n",
        "                    bbox=dict(facecolor=color, alpha=0.7))\n",
        "        \n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Clear output for next frame unless it's the last one\n",
        "        if i < len(sequence_samples) - 1:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "def calculate_tracking_speed(model, tracker, dataset, num_frames=100):\n",
        "    \"\"\"\n",
        "    Calculate tracking speed in frames per second.\n",
        "    \n",
        "    Args:\n",
        "        model (FasterRCNNTracker): Detection model\n",
        "        tracker (TemporalConsistencyTracker): Tracker\n",
        "        dataset (MOTDataset): Dataset to evaluate on\n",
        "        num_frames (int): Number of frames to process\n",
        "        \n",
        "    Returns:\n",
        "        float: Tracking speed in frames per second\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Select random frames\n",
        "    indices = torch.randperm(len(dataset))[:num_frames].tolist()\n",
        "    \n",
        "    # Reset tracker\n",
        "    tracker.tracks = {}\n",
        "    tracker.next_track_id = 0\n",
        "    \n",
        "    # Measure time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Process frames\n",
        "    for idx in indices:\n",
        "        # Get sample\n",
        "        image, _, sample = dataset[idx]\n",
        "        \n",
        "        # Move to device\n",
        "        image = image.to(device)\n",
        "        \n",
        "        # Perform detection\n",
        "        with torch.no_grad():\n",
        "            prediction = model([image])[0]\n",
        "        \n",
        "        # Filter predictions\n",
        "        keep = prediction['scores'] > 0.5\n",
        "        boxes = prediction['boxes'][keep].cpu()\n",
        "        scores = prediction['scores'][keep].cpu()\n",
        "        labels = prediction['labels'][keep].cpu()\n",
        "        \n",
        "        # Create detection dictionary\n",
        "        detections = {\n",
        "            'boxes': boxes,\n",
        "            'scores': scores,\n",
        "            'labels': labels\n",
        "        }\n",
        "        \n",
        "        # Update tracker\n",
        "        tracker.update(detections, sample['frame_number'])\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Calculate FPS\n",
        "    fps = num_frames / elapsed_time\n",
        "    \n",
        "    print(f\"Processed {num_frames} frames in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Tracking speed: {fps:.2f} FPS\")\n",
        "    \n",
        "    return fps\n",
        "\n",
        "def calculate_identity_switch_rate(tracker, dataset, sequence=None, max_frames=100):\n",
        "    \"\"\"\n",
        "    Calculate the identity switch rate.\n",
        "    \n",
        "    Args:\n",
        "        tracker (TemporalConsistencyTracker): Tracker\n",
        "        dataset (MOTDataset): Dataset\n",
        "        sequence (str, optional): Specific sequence to evaluate\n",
        "        max_frames (int): Maximum frames to process\n",
        "        \n",
        "    Returns:\n",
        "        float: Identity switch rate (switches per 100 frames)\n",
        "    \"\"\"\n",
        "    # Reset tracker\n",
        "    tracker.tracks = {}\n",
        "    tracker.next_track_id = 0\n",
        "    \n",
        "    # Filter samples\n",
        "    samples = []\n",
        "    if sequence:\n",
        "        for i, (_, _, sample) in enumerate(dataset):\n",
        "            if sample['sequence'] == sequence:\n",
        "                samples.append((i, sample['frame_number']))\n",
        "    else:\n",
        "        # Use samples from all sequences\n",
        "        for i, (_, _, sample) in enumerate(dataset):\n",
        "            samples.append((i, sample['frame_number']))\n",
        "    \n",
        "    # Sort by sequence and frame number\n",
        "    samples.sort(key=lambda x: (dataset[x[0]][2]['sequence'], x[1]))\n",
        "    \n",
        "    # Limit to max_frames\n",
        "    samples = samples[:max_frames]\n",
        "    \n",
        "    # Track ID matches between ground truth and predictions\n",
        "    gt_to_pred = {}\n",
        "    \n",
        "    # Count ID switches\n",
        "    id_switches = 0\n",
        "    total_frames = 0\n",
        "    \n",
        "    # Current sequence being processed\n",
        "    current_sequence = None\n",
        "    \n",
        "    for sample_idx, frame_number in tqdm(samples, desc=\"Calculating identity switch rate\"):\n",
        "        # Get sample\n",
        "        image, target, sample = dataset[sample_idx]\n",
        "        \n",
        "        # If sequence changed, reset tracker and mappings\n",
        "        if current_sequence is not None and current_sequence != sample['sequence']:\n",
        "            tracker.tracks = {}\n",
        "            tracker.next_track_id = 0\n",
        "            gt_to_pred = {}\n",
        "        \n",
        "        current_sequence = sample['sequence']\n",
        "        \n",
        "        # Get ground truth\n",
        "        gt_boxes = target['boxes']\n",
        "        gt_obj_ids = target['obj_ids']\n",
        "        \n",
        "        # Use ground truth as detections\n",
        "        detections = {\n",
        "            'boxes': gt_boxes,\n",
        "            'scores': torch.ones(len(gt_boxes)),\n",
        "            'labels': torch.ones(len(gt_boxes), dtype=torch.int64)\n",
        "        }\n",
        "        \n",
        "        # Update tracker\n",
        "        tracks = tracker.update(detections, frame_number)\n",
        "        \n",
        "        # Match ground truth to tracks based on IoU\n",
        "        matched_gt_ids = set()\n",
        "        \n",
        "        if len(gt_boxes) > 0 and tracks:\n",
        "            pred_boxes = torch.stack([track_info['box'] for track_id, track_info in tracks.items()])\n",
        "            pred_ids = list(tracks.keys())\n",
        "            \n",
        "            # Calculate IoU\n",
        "            iou_matrix = box_iou(gt_boxes, pred_boxes)\n",
        "            \n",
        "            # Match based on IoU\n",
        "            for gt_idx, gt_id in enumerate(gt_obj_ids):\n",
        "                gt_id = gt_id.item()\n",
        "                \n",
        "                if len(pred_boxes) == 0:\n",
        "                    continue\n",
        "                \n",
        "                # Find best match\n",
        "                best_iou, best_pred_idx = torch.max(iou_matrix[gt_idx], dim=0)\n",
        "                \n",
        "                if best_iou >= 0.5:  # IoU threshold\n",
        "                    pred_id = pred_ids[best_pred_idx]\n",
        "                    \n",
        "                    # Check for ID switch\n",
        "                    if gt_id in gt_to_pred and gt_to_pred[gt_id] != pred_id:\n",
        "                        id_switches += 1\n",
        "                    \n",
        "                    # Update mapping\n",
        "                    gt_to_pred[gt_id] = pred_id\n",
        "                    matched_gt_ids.add(gt_id)\n",
        "        \n",
        "        total_frames += 1\n",
        "    \n",
        "    # Calculate ID switch rate per 100 frames\n",
        "    id_switch_rate = (id_switches / total_frames) * 100\n",
        "    \n",
        "    print(f\"Total frames processed: {total_frames}\")\n",
        "    print(f\"Total ID switches: {id_switches}\")\n",
        "    print(f\"ID switch rate: {id_switch_rate:.2f} per 100 frames\")\n",
        "    \n",
        "    return id_switch_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d933d1b2",
      "metadata": {},
      "source": [
        "##### Main training and evaluation function\n",
        "This suite contains all the execution functions in the required order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a6f1fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def simplified_training(model, dataset_path, num_epochs=5, batch_size=2, lr=0.001):\n",
        "    \"\"\"\n",
        "    Simplified training function with tqdm progress bars for terminal usage.\n",
        "    \n",
        "    Args:\n",
        "        model: The model to train\n",
        "        dataset_path: Path to the dataset\n",
        "        num_epochs: Number of epochs to train\n",
        "        batch_size: Batch size\n",
        "        lr: Learning rate\n",
        "    \n",
        "    Returns:\n",
        "        Trained model\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = get_device()\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Get available training sequences for train/val split\n",
        "    train_dir = os.path.join(dataset_path, 'train')\n",
        "    all_sequences = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
        "    \n",
        "    if not all_sequences:\n",
        "        print(\"No sequences found. Please check the dataset path.\")\n",
        "        return model\n",
        "    \n",
        "    # Split sequences 80/20 for train/val\n",
        "    num_train = max(1, int(0.8 * len(all_sequences)))\n",
        "    train_sequences = all_sequences[:num_train]\n",
        "    val_sequences = all_sequences[num_train:]\n",
        "    \n",
        "    print(f\"Using {len(train_sequences)} sequences for training and {len(val_sequences)} for validation\")\n",
        "    \n",
        "    # Create training dataset\n",
        "    train_dataset = MOTDataset(\n",
        "        root_dir=dataset_path,\n",
        "        sequences=train_sequences,\n",
        "        transform=get_transform(train=True),\n",
        "        augment=True,\n",
        "        test_mode=False\n",
        "    )\n",
        "    \n",
        "    # Create validation dataset\n",
        "    val_dataset = MOTDataset(\n",
        "        root_dir=dataset_path,\n",
        "        sequences=val_sequences,\n",
        "        transform=get_transform(train=False),\n",
        "        augment=False,\n",
        "        test_mode=False\n",
        "    )\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Avoid multiprocessing issues in some environments\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False,\n",
        "        num_workers=0,  # Avoid multiprocessing issues in some environments\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    \n",
        "    # Set up optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"Starting training for {num_epochs} epochs\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n",
        "        for images, targets, _ in progress_bar:\n",
        "            # Move data to device\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items() if k != 'obj_ids'} for t in targets]\n",
        "            \n",
        "            # Forward pass and compute loss\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            \n",
        "            # Compute gradient and optimize\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Update statistics\n",
        "            train_loss += losses.item()\n",
        "            \n",
        "            # Update progress bar with current loss\n",
        "            progress_bar.set_postfix(loss=f\"{losses.item():.4f}\")\n",
        "        \n",
        "        # Print epoch training summary\n",
        "        avg_loss = train_loss / len(train_loader)\n",
        "        tqdm.write(f\"Epoch {epoch+1}: Training Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        \n",
        "        progress_bar = tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for images, targets, _ in progress_bar:\n",
        "                # Move data to device\n",
        "                images = [image.to(device) for image in images]\n",
        "                targets = [{k: v.to(device) for k, v in t.items() if k != 'obj_ids'} for t in targets]\n",
        "                \n",
        "                # Forward pass and compute loss\n",
        "                loss_dict = model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                \n",
        "                # Update statistics\n",
        "                val_loss += losses.item()\n",
        "                \n",
        "                # Update progress bar with current loss\n",
        "                progress_bar.set_postfix(loss=f\"{losses.item():.4f}\")\n",
        "        \n",
        "        # Print epoch validation summary\n",
        "        avg_val_loss = val_loss / max(1, len(val_loader))\n",
        "        tqdm.write(f\"Epoch {epoch+1}: Validation Loss: {avg_val_loss:.4f}\")\n",
        "        \n",
        "        # Save checkpoint\n",
        "        checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        tqdm.write(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "    \n",
        "    tqdm.write(\"Training complete!\")\n",
        "    return model\n",
        "\n",
        "# Optimized training for faster convergence and better accuracy\n",
        "\n",
        "def optimized_training(dataset_path, num_epochs=5, batch_size=2, eval_only=False):\n",
        "    \"\"\"\n",
        "    Optimized training with a focus on speed and accuracy.\n",
        "    \n",
        "    Args:\n",
        "        dataset_path: Path to the dataset\n",
        "        num_epochs: Number of epochs to train\n",
        "        batch_size: Batch size\n",
        "        eval_only: If True, only evaluate a pre-trained model\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Create a smaller dataset for faster training/testing\n",
        "    # Get available training sequences\n",
        "    train_dir = os.path.join(dataset_path, 'train')\n",
        "    all_sequences = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
        "    \n",
        "    if not all_sequences:\n",
        "        print(\"No sequences found. Please check the dataset path.\")\n",
        "        return\n",
        "    \n",
        "    # Limit to just 1-2 sequences for faster training\n",
        "    selected_sequences = all_sequences[:1]  # Start with just one sequence\n",
        "    print(f\"Using sequence(s): {selected_sequences}\")\n",
        "    \n",
        "    # Create dataset with minimal sample size\n",
        "    dataset = MOTDataset(\n",
        "        root_dir=dataset_path,\n",
        "        sequences=selected_sequences,\n",
        "        transform=get_transform(train=False),  # Use the fixed get_transform function\n",
        "        augment=False,\n",
        "        test_mode=False\n",
        "    )\n",
        "    \n",
        "    print(f\"Dataset size: {len(dataset)} frames\")\n",
        "    \n",
        "    # Further reduce dataset size for testing (use only every 10th frame)\n",
        "    # This significantly speeds up training while still providing useful data\n",
        "    indices = list(range(0, len(dataset), 10))\n",
        "    print(f\"Using {len(indices)} frames for training/testing\")\n",
        "    \n",
        "    # Initialize model with pre-trained weights\n",
        "    print(\"Initializing model with pre-trained weights...\")\n",
        "    model = FasterRCNNTracker(num_classes=2, pretrained=True)\n",
        "    \n",
        "    # Freeze backbone for faster training\n",
        "    # Only fine-tune the classification heads\n",
        "    for name, param in model.model.backbone.named_parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Print trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable_params:,} of {total_params:,} total parameters\")\n",
        "    \n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # If evaluation only, skip training\n",
        "    if eval_only:\n",
        "        # Test directly without training (assumes model is pre-trained)\n",
        "        print(\"Skipping training, evaluating model directly...\")\n",
        "        test_model(model, dataset, indices[:5])  # Test on 5 frames\n",
        "        return model\n",
        "    \n",
        "    # Split indices into train/val\n",
        "    train_size = int(0.8 * len(indices))\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:]\n",
        "    \n",
        "    # Create a subset of the dataset\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=1,  # Use batch size of 1 for validation\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    \n",
        "    # Use higher learning rate since we're only training classification heads\n",
        "    lr = 0.005\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0001)\n",
        "    \n",
        "    # Use a learning rate scheduler to improve convergence\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"Starting training for {num_epochs} epochs\")\n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for i, (images, targets, _) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items() if k != 'obj_ids'} for t in targets]\n",
        "            \n",
        "            # Skip empty targets (happens sometimes with subsets)\n",
        "            if all(len(t['boxes']) == 0 for t in targets):\n",
        "                continue\n",
        "            \n",
        "            # Forward pass and compute loss\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            \n",
        "            # Skip invalid losses\n",
        "            if not torch.isfinite(losses):\n",
        "                print(f\"Warning: non-finite loss, skipping batch {i}\")\n",
        "                continue\n",
        "            \n",
        "            # Compute gradient and optimize\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            \n",
        "            # Clip gradients for stability\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            # Update statistics\n",
        "            train_loss += losses.item()\n",
        "            \n",
        "            # Print progress\n",
        "            if (i + 1) % 5 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                imgs_per_sec = (i + 1) * batch_size / elapsed\n",
        "                print(f\"  Batch {i+1}/{len(train_loader)}, Loss: {losses.item():.4f}, \" \n",
        "                     f\"Speed: {imgs_per_sec:.2f} imgs/sec\")\n",
        "        \n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # Print epoch summary\n",
        "        avg_loss = train_loss / max(1, len(train_loader))\n",
        "        print(f\"  Training Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, (images, targets, _) in enumerate(val_loader):\n",
        "                # Move data to device\n",
        "                images = list(image.to(device) for image in images)\n",
        "                targets = [{k: v.to(device) for k, v in t.items() if k != 'obj_ids'} for t in targets]\n",
        "                \n",
        "                # Skip empty targets\n",
        "                if all(len(t['boxes']) == 0 for t in targets):\n",
        "                    continue\n",
        "                \n",
        "                # Forward pass and compute loss\n",
        "                loss_dict = model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                \n",
        "                # Skip invalid losses\n",
        "                if not torch.isfinite(losses):\n",
        "                    continue\n",
        "                \n",
        "                # Update statistics\n",
        "                val_loss += losses.item()\n",
        "        \n",
        "        # Print validation summary\n",
        "        avg_val_loss = val_loss / max(1, len(val_loader))\n",
        "        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
        "        \n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"  Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "    \n",
        "    # Test the model\n",
        "    test_model(model, dataset, val_indices[:5])  # Test on 5 frames\n",
        "    \n",
        "    return model\n",
        "\n",
        "def test_model(model, dataset, indices):\n",
        "    \"\"\"\n",
        "    Test model inference on selected frames.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        dataset: Dataset to test on\n",
        "        indices: List of indices to test\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    \n",
        "    # Initialize tracker\n",
        "    tracker = TemporalConsistencyTracker(\n",
        "        iou_threshold=0.3,\n",
        "        max_age=10,\n",
        "        min_hits=2,\n",
        "        detection_threshold=0.5\n",
        "    )\n",
        "    \n",
        "    print(f\"Testing model on {len(indices)} frames...\")\n",
        "    \n",
        "    for idx in indices:\n",
        "        image, target, sample = dataset[idx]\n",
        "        \n",
        "        # Forward pass\n",
        "        image = image.to(device)\n",
        "        with torch.no_grad():\n",
        "            prediction = model([image])[0]\n",
        "        \n",
        "        # Get predictions with confidence > 0.5\n",
        "        keep = prediction['scores'] > 0.5\n",
        "        boxes = prediction['boxes'][keep].cpu()\n",
        "        scores = prediction['scores'][keep].cpu()\n",
        "        labels = prediction['labels'][keep].cpu()\n",
        "        \n",
        "        # Create detection dictionary\n",
        "        detections = {\n",
        "            'boxes': boxes,\n",
        "            'scores': scores,\n",
        "            'labels': labels\n",
        "        }\n",
        "        \n",
        "        # Update tracker\n",
        "        tracks = tracker.update(detections, sample['frame_number'])\n",
        "        \n",
        "        # Convert image for visualization\n",
        "        img_np = image.cpu().permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # Denormalize\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img_np = std * img_np + mean\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "        \n",
        "        # Display the image with predictions\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(img_np)\n",
        "        \n",
        "        # Draw bounding boxes\n",
        "        for track_id, track_info in tracks.items():\n",
        "            box = track_info['box']\n",
        "            x1, y1, x2, y2 = box.tolist()\n",
        "            \n",
        "            # Generate a random but consistent color for this track\n",
        "            color = np.random.RandomState(track_id).rand(3,)\n",
        "            \n",
        "            rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                           linewidth=2, edgecolor=color, facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "            plt.text(x1, y1-5, f\"ID: {track_id}\", \n",
        "                    color='white', fontsize=10,\n",
        "                    bbox=dict(facecolor=color, alpha=0.5))\n",
        "        \n",
        "        plt.title(f\"Sequence: {sample['sequence']}, Frame: {sample['frame_number']}, Tracks: {len(tracks)}\")\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"Detected {len(boxes)} objects, Tracking {len(tracks)} objects\")\n",
        "    \n",
        "    # If the model needs training, remove eval_only=True\n",
        "    # optimized_training(DATASET_PATH, num_epochs=3, batch_size=2)\n",
        "\n",
        "# Simple test function to get started\n",
        "def test_model_inference(model, dataset, num_samples=3):\n",
        "    \"\"\"\n",
        "    Test model inference on a few samples.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        dataset: Dataset to test on\n",
        "        num_samples: Number of samples to test\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    \n",
        "    # Get random samples\n",
        "    indices = torch.randperm(len(dataset))[:num_samples].tolist()\n",
        "    \n",
        "    for idx in indices:\n",
        "        image, target, sample = dataset[idx]\n",
        "        \n",
        "        # Forward pass\n",
        "        image = image.to(device)\n",
        "        with torch.no_grad():\n",
        "            prediction = model([image])[0]\n",
        "        \n",
        "        # Get predictions with confidence > 0.5\n",
        "        keep = prediction['scores'] > 0.5\n",
        "        boxes = prediction['boxes'][keep].cpu()\n",
        "        scores = prediction['scores'][keep].cpu()\n",
        "        labels = prediction['labels'][keep].cpu()\n",
        "        \n",
        "        # Convert image for visualization\n",
        "        img_np = image.cpu().permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # Denormalize\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img_np = std * img_np + mean\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "        \n",
        "        # Display the image with predictions\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(img_np)\n",
        "        \n",
        "        # Draw bounding boxes\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            x1, y1, x2, y2 = box.tolist()\n",
        "            rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                           linewidth=2, edgecolor='r', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "            plt.text(x1, y1-5, f\"Score: {score:.2f}\", \n",
        "                    color='white', fontsize=10,\n",
        "                    bbox=dict(facecolor='red', alpha=0.5))\n",
        "        \n",
        "        plt.title(f\"Detection Results - {len(boxes)} objects detected\")\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def main(dataset_path, num_epochs=1, batch_size=1):\n",
        "    \"\"\"\n",
        "    Main function with simplified training to avoid pickling issues.\n",
        "    \n",
        "    Args:\n",
        "        dataset_path: Path to the dataset\n",
        "        num_epochs: Number of epochs to train\n",
        "        batch_size: Batch size\n",
        "    \"\"\"\n",
        "    # Initialize model\n",
        "    print(\"Initializing model...\")\n",
        "    model = FasterRCNNTracker(num_classes=2, pretrained=True)\n",
        "    \n",
        "    # Train the model\n",
        "    print(\"Training model...\")\n",
        "    model = simplified_training(model, dataset_path, num_epochs, batch_size)\n",
        "    \n",
        "    # Save the final model\n",
        "    torch.save(model.state_dict(), 'faster_rcnn_tracker_final.pth')\n",
        "    print(\"Saved final model to faster_rcnn_tracker_final.pth\")\n",
        "    \n",
        "    # Create a validation dataset for testing\n",
        "    print(\"Creating validation dataset for testing...\")\n",
        "    val_dataset = MOTDataset(\n",
        "        root_dir=dataset_path,\n",
        "        sequences=None,  # Use all available sequences\n",
        "        transform=get_transform(train=False),\n",
        "        augment=False,\n",
        "        test_mode=False\n",
        "    )\n",
        "    \n",
        "    # Test the model\n",
        "    print(\"Testing model inference...\")\n",
        "    test_model_inference(model, val_dataset)\n",
        "    \n",
        "    return model\n",
        "\n",
        "# main(dataset_path=DATASET_PATH, num_epochs=5)\n",
        "optimized_training(DATASET_PATH, num_epochs=3, batch_size=2, eval_only=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
