{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_seqinfo(seqinfo_path):\n",
    "    \"\"\"\n",
    "    Reads MOT17 seqinfo.ini to get info on frame rate, image size, and number of frames.\n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    with open(seqinfo_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if \"=\" in line:\n",
    "                key, value = line.split(\"=\")\n",
    "                info[key.strip()] = value.strip()\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gt_file(gt_txt_path, keep_classes=[1, 7]):\n",
    "    \"\"\"\n",
    "    Reads gt.txt and returns a dictionary of {frame_index: [list_of_boxes]}.\n",
    "    Each box is a dict of {xmin, ymin, width, height, class_id, etc.}\n",
    "    We'll keep only the chosen classes, e.g. pedestrians.\n",
    "    \"\"\"\n",
    "    frame_dict = {}\n",
    "    with open(gt_txt_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            vals = line.strip().split(\",\")\n",
    "            frame_id = int(vals[0])\n",
    "            obj_id = int(vals[1])\n",
    "            x = int(vals[2])\n",
    "            y = int(vals[3])\n",
    "            w = int(vals[4])\n",
    "            h = int(vals[5])\n",
    "            conf = float(vals[6])   # for ground truth, often 1 or 0\n",
    "            cls_id = int(vals[7])   # class label\n",
    "            # visibility = float(vals[8])  # optional\n",
    "\n",
    "            # Filter only pedestrians if that’s your target\n",
    "            if cls_id in keep_classes:\n",
    "                # Convert from float to int if you prefer\n",
    "                box_info = {\n",
    "                    \"obj_id\": obj_id,\n",
    "                    \"bbox\": [x, y, w, h],\n",
    "                    \"class_id\": 1  # unify pedestrian to class \"1\"\n",
    "                }\n",
    "                if frame_id not in frame_dict:\n",
    "                    frame_dict[frame_id] = []\n",
    "                frame_dict[frame_id].append(box_info)\n",
    "    return frame_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOT17PedestrianDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 images_dir, \n",
    "                 seqinfo, \n",
    "                 gt_dict, \n",
    "                 transforms=None, \n",
    "                 resize=(640, 360)):\n",
    "        \"\"\"\n",
    "        images_dir: path to 'img1' folder\n",
    "        seqinfo: dictionary returned by parse_seqinfo\n",
    "        gt_dict: dictionary {frame: list of boxes}\n",
    "        transforms: custom transforms or augmentations\n",
    "        resize: (width, height) for downscaling images\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.gt_dict = gt_dict\n",
    "        self.transforms = transforms\n",
    "        self.resize = resize\n",
    "        \n",
    "        # seqLength in seqinfo is total frames\n",
    "        self.num_frames = int(seqinfo.get(\"seqLength\", 0))\n",
    "        \n",
    "        # Frame names are 1-based in MOT. We'll store their paths in a list\n",
    "        self.image_paths = []\n",
    "        for frame_idx in range(1, self.num_frames+1):\n",
    "            # Format 6-digit file name, e.g. 000001.jpg\n",
    "            filename = f\"{frame_idx:06d}.jpg\"\n",
    "            full_path = os.path.join(images_dir, filename)\n",
    "            if os.path.exists(full_path):\n",
    "                self.image_paths.append(full_path)\n",
    "            else:\n",
    "                # Some sequences might skip frames, so just skip if not found\n",
    "                pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Determine frame index from file name\n",
    "        frame_number = int(os.path.splitext(os.path.basename(img_path))[0])\n",
    "        \n",
    "        # Retrieve bounding boxes from gt_dict\n",
    "        boxes_info = self.gt_dict.get(frame_number, [])\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for b in boxes_info:\n",
    "            x, y, w, h = b[\"bbox\"]\n",
    "            xmin, ymin = x, y\n",
    "            xmax, ymax = x + w, y + h\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(b[\"class_id\"])  # 1 for pedestrian\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        \n",
    "        # Resizing for faster training\n",
    "        if self.resize:\n",
    "            # Use a simple cv2 resize or something more advanced\n",
    "            image = cv2.resize(image, self.resize)\n",
    "            \n",
    "            # We need to rescale bounding boxes accordingly\n",
    "            original_h, original_w = cv2.imread(img_path).shape[:2]  # or store earlier\n",
    "            new_w, new_h = self.resize\n",
    "\n",
    "            scale_x = new_w / original_w\n",
    "            scale_y = new_h / original_h\n",
    "            \n",
    "            boxes[:, [0, 2]] *= scale_x\n",
    "            boxes[:, [1, 3]] *= scale_y\n",
    "        \n",
    "        # Convert from NumPy to PIL before torchvision transforms\n",
    "        image_pil = Image.fromarray(image)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image_pil, target = self.transforms(image_pil, target)\n",
    "        \n",
    "        return image_pil, target\n",
    "    \n",
    "class ToTensorTransform:\n",
    "    def __call__(self, image, target=None):\n",
    "        \"\"\"\n",
    "        Converts image to tensor and keeps target unchanged.\n",
    "        \"\"\"\n",
    "        image = F.to_tensor(image)  # Convert image to tensor\n",
    "        return (image, target) if target else image  # Preserve the target dict\n",
    "\n",
    "def get_transform(train=True):\n",
    "    transforms_list = [ToTensorTransform()]  # Only use transforms that accept (image, target)\n",
    "    return transforms_list[0]  # No need for Compose since we're handling multiple args manually\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faster_rcnn_model(num_classes=2):\n",
    "    # Load a model pre-trained on COCO\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the head with a new one (2 classes: background + pedestrian)\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Wrap the data_loader with tqdm for progress bar\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for images, targets in progress_bar:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "\n",
    "        # Update progress bar description with loss\n",
    "        progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivamsahil/Downloads/bits/assignments/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/shivamsahil/Downloads/bits/assignments/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Training:  10%|█         | 24/240 [54:27<10:07:11, 168.66s/it, loss=1.36] "
     ]
    }
   ],
   "source": [
    "# Suppose you have your MOT17 path\n",
    "data_dir = \"mot17/MOT17Det/train/MOT17-02\"\n",
    "seqinfo_path = os.path.join(data_dir, \"seqinfo.ini\")\n",
    "gt_txt_path = os.path.join(data_dir, \"gt\", \"gt.txt\")\n",
    "images_dir = os.path.join(data_dir, \"img1\")\n",
    "\n",
    "# Parse data\n",
    "seqinfo = parse_seqinfo(seqinfo_path)\n",
    "gt_dict = parse_gt_file(gt_txt_path, keep_classes=[1, 7])  # keep pedestrian classes\n",
    "\n",
    "# Create dataset\n",
    "dataset = MOT17PedestrianDataset(\n",
    "    images_dir=images_dir,\n",
    "    seqinfo=seqinfo,\n",
    "    gt_dict=gt_dict,\n",
    "    transforms=get_transform(train=True),\n",
    "    resize=(640, 360)  # example: scale down images for faster training\n",
    ")\n",
    "\n",
    "# Split into train/val if you want\n",
    "# For demonstration, let's do a quick small train set\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "train_indices = indices[:int(0.8 * len(indices))]\n",
    "val_indices   = indices[int(0.8 * len(indices)):]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset   = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: list(zip(*x)))\n",
    "val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: list(zip(*x)))\n",
    "\n",
    "# Get model\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "model = get_faster_rcnn_model(num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Train for a few epochs\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    loss_train = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss_train:.4f}\")\n",
    "    \n",
    "    # You could add a validation pass here...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
