{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9bda93ba",
      "metadata": {
        "id": "9bda93ba"
      },
      "source": [
        "## Group No\n",
        "\n",
        "## Group Member Names:\n",
        "1. SHIVAM SAHIL - 2023AA05663\n",
        "2. JAHNAVI GALI - 2023AA05684\n",
        "3. PRASHANT KUMAR - 2023AA05043\n",
        "4. SAHIL MEHRA - 2023AA05327"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46631608",
      "metadata": {
        "id": "46631608"
      },
      "outputs": [],
      "source": [
        "# Necessary Imports\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, ToPILImage, ToTensor, RandomHorizontalFlip, ColorJitter\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from tqdm import tqdm\n",
        "\n",
        "from filterpy.kalman import KalmanFilter\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "\n",
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_stdlib_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "df5ac3a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- A. Custom Dataset for MOT17 ----\n",
        "class MOT17DetectionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Reads images and bounding boxes from the MOT17 'det.txt' file.\n",
        "    Treats the detection bounding boxes as training labels (not usual practice, but for demonstration).\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_dir, transform=None, is_test_dir = False):\n",
        "        \"\"\"\n",
        "        seq_dir: path to a sequence directory, e.g. 'MOT17-02-DPM'.\n",
        "                 This directory should contain:\n",
        "                   - img1/ (folder with images)\n",
        "                   - det/det.txt (file with detection bboxes)\n",
        "        transform: augmentations and preprocessing (transforms.Compose)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.seq_dir = seq_dir\n",
        "        self.img_dir = os.path.join(seq_dir, 'img1')\n",
        "        self.is_test_dir = is_test_dir\n",
        "        if self.is_test_dir: self.det_file = os.path.join(seq_dir, 'det', 'det.txt')\n",
        "        else: self.det_file = os.path.join(seq_dir, 'gt', 'gt.txt')\n",
        "        \n",
        "        self.transform = transform\n",
        "        \n",
        "        # Parse det.txt into a dict { frame_id : [ list of bboxes ] }\n",
        "        # Each bbox is [x1, y1, x2, y2, confidence]\n",
        "        self.frame_to_boxes = {}\n",
        "        with open(self.det_file, 'r') as f:\n",
        "            for line in f:\n",
        "                # Format: frame_id, track_id, x, y, w, h, conf, ...\n",
        "                # Some files have more columns, we focus on the first 7\n",
        "                vals = line.strip().split(',')\n",
        "                frame_id = int(vals[0])\n",
        "                x = float(vals[2])\n",
        "                y = float(vals[3])\n",
        "                w = float(vals[4])\n",
        "                h = float(vals[5])\n",
        "                confidence = float(vals[6])\n",
        "\n",
        "                # Convert to x1,y1,x2,y2\n",
        "                x1, y1 = x, y\n",
        "                x2, y2 = x + w, y + h\n",
        "\n",
        "                if frame_id not in self.frame_to_boxes:\n",
        "                    self.frame_to_boxes[frame_id] = []\n",
        "                self.frame_to_boxes[frame_id].append([x1, y1, x2, y2, confidence])\n",
        "        \n",
        "        # Gather all possible frames from img1 folder\n",
        "        # Typically, images are named 000001.jpg, 000002.jpg, etc.\n",
        "        self.img_files = sorted([\n",
        "            os.path.join(self.img_dir, f) \n",
        "            for f in os.listdir(self.img_dir) \n",
        "            if f.endswith('.jpg')\n",
        "        ])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "    \n",
        "    def get_item_training(self, idx):\n",
        "        img_path = self.img_files[idx]\n",
        "        # Frame numbering in MOT17 typically starts at 1, so let's parse from filename\n",
        "        # e.g., 000069.jpg -> frame_id = 69\n",
        "        file_name = os.path.basename(img_path)\n",
        "        frame_id = int(file_name.split('.')[0])  # \"000069\" => 69\n",
        "\n",
        "        # Read the image\n",
        "        img_bgr = cv2.imread(img_path)\n",
        "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Gather bounding boxes for this frame\n",
        "        boxes_info = self.frame_to_boxes.get(frame_id, [])\n",
        "        \n",
        "        # For demonstration, let's label everything as class 1 = \"object\"\n",
        "        # (PyTorch detection models expect labels >= 1)\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for (x1, y1, x2, y2, conf) in boxes_info:\n",
        "            boxes.append([x1, y1, x2, y2])\n",
        "            labels.append(1)  # single-class approach\n",
        "        \n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        \n",
        "        # Create target dictionary\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = torch.tensor([idx])  # or frame_id\n",
        "        \n",
        "        if self.transform:\n",
        "            img_pil = ToPILImage()(img_rgb)\n",
        "            img_pil = self.transform(img_pil)\n",
        "            image = img_pil\n",
        "        else:\n",
        "            # Convert numpy -> torch\n",
        "            image = ToTensor()(img_rgb)\n",
        "        \n",
        "        return image, target\n",
        "    \n",
        "    def get_item_test(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            image (Tensor): the frame\n",
        "            detections (Tensor): bounding boxes [N, 5], each row is (x, y, w, h, conf)\n",
        "            frame_id (int)\n",
        "        \"\"\"\n",
        "        img_path = self.img_files[idx]\n",
        "        file_name = os.path.basename(img_path)\n",
        "        frame_id = int(file_name.split('.')[0])  # e.g. '000001.jpg' -> 1\n",
        "        \n",
        "        img_bgr = cv2.imread(img_path)\n",
        "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert to PyTorch tensor\n",
        "        if self.transform:\n",
        "            image = self.transform(img_rgb)\n",
        "        else:\n",
        "            image = ToTensor()(img_rgb)\n",
        "        \n",
        "        # Get detections for this frame\n",
        "        dets = self.frame_to_boxes.get(frame_id, [])\n",
        "        dets_tensor = torch.as_tensor(dets, dtype=torch.float32)  # shape [N, 5]\n",
        "        \n",
        "        return image, dets_tensor, frame_id\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test_dir:return self.get_item_test(idx = idx)\n",
        "        else:return self.get_item_training(idx = idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "289ecb64",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- B. Defining Transformations ----\n",
        "def get_train_transform():\n",
        "    return Compose([\n",
        "        # We'll do only these two to avoid complicated box coordinate transforms\n",
        "        RandomHorizontalFlip(p=0.5),\n",
        "        ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        ToTensor()  # Convert PIL image to torch tensor\n",
        "    ])\n",
        "\n",
        "def get_test_transform():\n",
        "    return Compose([\n",
        "        ToTensor()\n",
        "    ])\n",
        "    \n",
        "def get_faster_rcnn_model(num_classes):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained Faster R-CNN with ResNet50-FPN backbone\n",
        "    and modifies the box predictor for 'num_classes' classes.\n",
        "    \"\"\"\n",
        "    # 1. Load a pre-trained model\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "    \n",
        "    # 2. Get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    \n",
        "    # 3. Replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "0141fa96",
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_path = r'mot17/MOT17/train/MOT17-02-FRCNN'\n",
        "train_dataset = MOT17DetectionDataset(seq_dir=seq_path, transform=get_train_transform())\n",
        "def my_collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=my_collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2940c04d",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "7b29b0d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [17:31<00:00,  3.51s/batch, loss=0.9637]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] - Loss: 1.1765\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [17:32<00:00,  3.51s/batch, loss=0.8213]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10] - Loss: 0.8566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [2:07:38<00:00, 25.53s/batch, loss=0.6320]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10] - Loss: 0.7228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [2:21:19<00:00, 28.26s/batch, loss=0.8592]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10] - Loss: 0.6319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [2:25:53<00:00, 29.18s/batch, loss=0.6048]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10] - Loss: 0.5633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [27:46<00:00,  5.55s/batch, loss=0.5225]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10] - Loss: 0.5116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [17:45<00:00,  3.55s/batch, loss=0.4809]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10] - Loss: 0.4681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [17:45<00:00,  3.55s/batch, loss=0.4718]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10] - Loss: 0.4417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [18:04<00:00,  3.62s/batch, loss=0.3696]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10] - Loss: 0.4195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 300/300 [18:11<00:00,  3.64s/batch, loss=0.3546]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10] - Loss: 0.3917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = 2  # 1 class + background (assuming all objects share one label)\n",
        "model = get_faster_rcnn_model(num_classes).to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "num_epochs = 10\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    pbar = tqdm(data_loader, desc=\"Training\", unit=\"batch\")\n",
        "    \n",
        "    for images, targets in pbar:\n",
        "        # Move images and targets to the GPU (if available)\n",
        "        images = [img.to(device) for img in images]\n",
        "        new_targets = []\n",
        "        for t in targets:\n",
        "            new_t = {\n",
        "                \"boxes\": t[\"boxes\"].to(device),\n",
        "                \"labels\": t[\"labels\"].to(device),\n",
        "                \"image_id\": t[\"image_id\"].to(device)\n",
        "            }\n",
        "            new_targets.append(new_t)\n",
        "        \n",
        "        # Forward pass\n",
        "        loss_dict = model(images, new_targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += losses.item()\n",
        "        \n",
        "        # Update progress bar with the latest loss value\n",
        "        pbar.set_postfix(loss=f\"{losses.item():.4f}\")\n",
        "    \n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    pbar.close()\n",
        "    return avg_loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3847a749",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at fasterrcnn_checkpoint.pth\n"
          ]
        }
      ],
      "source": [
        "# Since model took too long to train, we don't want to lose it later\n",
        "def checkpoint_and_save_model(epoch, model, optimizer, checkpoint_path = \"fasterrcnn_checkpoint.pth\"):\n",
        "    torch.save({\n",
        "        'epoch': epoch,  # e.g. last epoch you finished\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': epoch_loss\n",
        "        }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "    \n",
        "def reload_model_into_memory(checkpoint_path = \"fasterrcnn_checkpoint.pth\"):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = get_faster_rcnn_model(num_classes).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "    resume_checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    # Restore model and optimizer\n",
        "    model.load_state_dict(resume_checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(resume_checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Start training from the next epoch\n",
        "    start_epoch = resume_checkpoint['epoch'] + 1\n",
        "    resume_loss = resume_checkpoint['loss']\n",
        "    print(f\"Resuming training from epoch {start_epoch} with previous loss: {resume_loss}\")\n",
        "    return model, optimizer, resume_checkpoint, start_epoch, resume_loss\n",
        "\n",
        "def train_model(checkpoint_path = \"fasterrcnn_checkpoint.pth\", additional_epochs = 5):\n",
        "    num_epochs_to_run = additional_epochs  # how many more epochs you want\n",
        "    model, optimizer, _, start_epoch, _ = reload_model_into_memory(checkpoint_path=checkpoint_path)\n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs_to_run):\n",
        "        epoch_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
        "        print(f\"Epoch [{epoch+1}/{start_epoch + num_epochs_to_run}] - Loss: {epoch_loss:.4f}\")\n",
        "        # Optional: save checkpoint after each epoch\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss\n",
        "        }, checkpoint_path)\n",
        "        \n",
        "\n",
        "# Saving long running model for re-usability purposes\n",
        "checkpoint_and_save_model(epoch,model,optimizer)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "2161055d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def iou(boxA, boxB):\n",
        "    \"\"\"\n",
        "    boxA, boxB: [x1, y1, x2, y2]\n",
        "    Return IoU of these two boxes.\n",
        "    \"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
        "\n",
        "class KalmanBoxTracker:\n",
        "    \"\"\"\n",
        "    This class represents the internal state of an object tracked as a bounding box.\n",
        "    State vector: [cx, cy, s, r, vx, vy, vs] where\n",
        "      - (cx, cy): center of the box\n",
        "      - s: scale (area)\n",
        "      - r: aspect ratio\n",
        "      - (vx, vy, vs): velocities\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, bbox):\n",
        "        \"\"\"\n",
        "        Initialize with bounding box [x1, y1, x2, y2].\n",
        "        \"\"\"\n",
        "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
        "        \n",
        "        # Define state transition matrix F and measurement matrix H\n",
        "        self.kf.F = np.array([[1, 0, 0, 0, 1, 0, 0],\n",
        "                              [0, 1, 0, 0, 0, 1, 0],\n",
        "                              [0, 0, 1, 0, 0, 0, 1],\n",
        "                              [0, 0, 0, 1, 0, 0, 0],\n",
        "                              [0, 0, 0, 0, 1, 0, 0],\n",
        "                              [0, 0, 0, 0, 0, 1, 0],\n",
        "                              [0, 0, 0, 0, 0, 0, 1]])\n",
        "        self.kf.H = np.array([[1, 0, 0, 0, 0, 0, 0],\n",
        "                              [0, 1, 0, 0, 0, 0, 0],\n",
        "                              [0, 0, 1, 0, 0, 0, 0],\n",
        "                              [0, 0, 0, 1, 0, 0, 0]])\n",
        "        \n",
        "        self.kf.P[4:, 4:] *= 1000.  # high uncertainty for the unobserved velocities\n",
        "        self.kf.P *= 10.\n",
        "\n",
        "        # Initialize state: convert bbox from [x1,y1,x2,y2] to [cx, cy, s, r]\n",
        "        cx, cy, s, r = self._convert_bbox_to_z(bbox)\n",
        "        self.kf.x[:4] = np.array([[cx], [cy], [s], [r]])\n",
        "        \n",
        "        self.time_since_update = 0\n",
        "        self.id = KalmanBoxTracker.count\n",
        "        KalmanBoxTracker.count += 1\n",
        "        self.history = []\n",
        "        self.hits = 1\n",
        "        self.hit_streak = 1\n",
        "\n",
        "    def update(self, bbox):\n",
        "        \"\"\"\n",
        "        Update state with observed bbox [x1, y1, x2, y2].\n",
        "        \"\"\"\n",
        "        self.time_since_update = 0\n",
        "        self.history = []\n",
        "        self.hits += 1\n",
        "        self.hit_streak += 1\n",
        "        z = np.array(self._convert_bbox_to_z(bbox)).reshape((4, 1))\n",
        "        self.kf.update(z)\n",
        "\n",
        "    def predict(self):\n",
        "        \"\"\"\n",
        "        Advance the state vector and return the predicted bounding box estimate.\n",
        "        \"\"\"\n",
        "        self.kf.predict()\n",
        "        self.time_since_update += 1\n",
        "        pred_bbox = self._convert_x_to_bbox(self.kf.x)\n",
        "        self.history.append(pred_bbox)\n",
        "        return pred_bbox\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"\n",
        "        Return the current bounding box estimate.\n",
        "        \"\"\"\n",
        "        return self._convert_x_to_bbox(self.kf.x)\n",
        "\n",
        "    @staticmethod\n",
        "    def _convert_bbox_to_z(bbox):\n",
        "        \"\"\"\n",
        "        Convert [x1, y1, x2, y2] to [cx, cy, s, r]:\n",
        "          - s: area\n",
        "          - r: aspect ratio (width/height)\n",
        "        \"\"\"\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        w = x2 - x1\n",
        "        h = y2 - y1\n",
        "        cx = x1 + w / 2.\n",
        "        cy = y1 + h / 2.\n",
        "        s = w * h\n",
        "        r = w / (h + 1e-6)\n",
        "        return [cx, cy, s, r]\n",
        "\n",
        "    @staticmethod\n",
        "    def _convert_x_to_bbox(x, score=None):\n",
        "        \"\"\"\n",
        "        Convert state vector (first 4 elements) [cx, cy, s, r] to [x1, y1, x2, y2].\n",
        "        Clamps s and r to avoid invalid values.\n",
        "        \"\"\"\n",
        "        # Extract first 4 state values\n",
        "        state = x.flatten()[:4]\n",
        "        cx, cy, s, r = state\n",
        "        \n",
        "        # Ensure s and r are positive to avoid invalid sqrt operations\n",
        "        s = max(s, 1e-6)\n",
        "        r = max(r, 1e-6)\n",
        "        \n",
        "        # Compute width and height\n",
        "        w = np.sqrt(s * r)\n",
        "        h = np.sqrt(s / (r + 1e-6))\n",
        "        \n",
        "        x1 = cx - w / 2.\n",
        "        y1 = cy - h / 2.\n",
        "        x2 = cx + w / 2.\n",
        "        y2 = cy + h / 2.\n",
        "        \n",
        "        if score is None:\n",
        "            return np.array([x1, y1, x2, y2])\n",
        "        else:\n",
        "            return np.array([x1, y1, x2, y2, score])\n",
        "        \"\"\"\n",
        "        [cx, cy, s, r] -> [x1, y1, x2, y2]\n",
        "        \"\"\"\n",
        "        w = np.sqrt(x[2]*x[3])\n",
        "        h = np.sqrt(x[2]/(x[3]+1e-6))\n",
        "        x1 = x[0] - w/2.\n",
        "        y1 = x[1] - h/2.\n",
        "        x2 = x[0] + w/2.\n",
        "        y2 = x[1] + h/2.\n",
        "        if score is None:\n",
        "            return np.array([x1, y1, x2, y2]).reshape((4,))\n",
        "        else:\n",
        "            return np.array([x1, y1, x2, y2, score]).reshape((5,))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "fec5bebf",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sort:\n",
        "    def __init__(self, max_age=5, min_hits=3, iou_threshold=0.3):\n",
        "        \"\"\"\n",
        "        max_age: frames to keep track alive without updates\n",
        "        min_hits: min detections before track is confirmed\n",
        "        iou_threshold: IoU matching threshold\n",
        "        \"\"\"\n",
        "        self.max_age = max_age\n",
        "        self.min_hits = min_hits\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.trackers = []\n",
        "\n",
        "    def update(self, dets=np.empty((0,5))):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "          dets: numpy array of detections in [x1, y1, x2, y2, score]\n",
        "        Returns:\n",
        "          an array of the final tracked objects: [x1,y1,x2,y2,track_id]\n",
        "        \"\"\"\n",
        "        # 1. Predict new locations for existing trackers.\n",
        "        for t in range(len(self.trackers)):\n",
        "            self.trackers[t].predict()\n",
        "\n",
        "        # 2. Build cost matrix for matching\n",
        "        trks = []\n",
        "        for t in range(len(self.trackers)):\n",
        "            pos = self.trackers[t].get_state()\n",
        "            trks.append(pos)\n",
        "        trks = np.array(trks)  # shape: [N, 4]\n",
        "\n",
        "        # If no trackers or no detections, handle edge cases:\n",
        "        if len(trks) == 0 or len(dets) == 0:\n",
        "            matches = []\n",
        "            unmatched_dets = range(len(dets))\n",
        "            unmatched_trks = range(len(trks))\n",
        "        else:\n",
        "            iou_matrix = np.zeros((len(trks), len(dets)), dtype=np.float32)\n",
        "            for t in range(len(trks)):\n",
        "                for d in range(len(dets)):\n",
        "                    iou_matrix[t,d] = iou(trks[t], dets[d,:4])\n",
        "            \n",
        "            row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
        "            matches = []\n",
        "            unmatched_dets = list(range(len(dets)))\n",
        "            unmatched_trks = list(range(len(trks)))\n",
        "            \n",
        "            for r, c in zip(row_ind, col_ind):\n",
        "                if iou_matrix[r,c] < self.iou_threshold:\n",
        "                    continue\n",
        "                matches.append([r, c])\n",
        "                unmatched_dets.remove(c)\n",
        "                unmatched_trks.remove(r)\n",
        "\n",
        "        # 3. Update matched trackers with assigned detections\n",
        "        for r, c in matches:\n",
        "            bbox = dets[c,:4]\n",
        "            self.trackers[r].update(bbox)\n",
        "\n",
        "        # 4. Create new trackers for unmatched detections\n",
        "        for i in unmatched_dets:\n",
        "            bbox = dets[i,:4]\n",
        "            new_tracker = KalmanBoxTracker(bbox)\n",
        "            self.trackers.append(new_tracker)\n",
        "\n",
        "        # 5. Get rid of dead tracks\n",
        "        ret = []\n",
        "        for t in reversed(range(len(self.trackers))):\n",
        "            trk = self.trackers[t]\n",
        "            d = trk.get_state()\n",
        "            # If the tracker hasn't been updated, remove if too old\n",
        "            if trk.time_since_update > self.max_age:\n",
        "                self.trackers.pop(t)\n",
        "                continue\n",
        "            # If confirmed (hits > min_hits) or recently updated, store\n",
        "            if (trk.hit_streak >= self.min_hits) or (trk.time_since_update < 1):\n",
        "                ret.append(np.concatenate((d, [trk.id])).reshape(1, -1))\n",
        "        if len(ret) > 0:\n",
        "            return np.concatenate(ret)\n",
        "        return np.empty((0,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "42b3a244",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking complete. Results: 6175\n"
          ]
        }
      ],
      "source": [
        "# Suppose your test sequence is \"MOT17-01-FRCNN\"\n",
        "test_seq_path = r'mot17/MOT17/test/MOT17-01-FRCNN'  # adjust accordingly\n",
        "\n",
        "test_dataset = MOT17DetectionDataset(seq_dir=test_seq_path, is_test_dir = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Switch model to eval mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize our SORT tracker\n",
        "sort_tracker = Sort(max_age=5, min_hits=2, iou_threshold=0.3)\n",
        "\n",
        "all_results = []  # to store final bounding boxes with IDs\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, dets_tensor, frame_id in test_loader:\n",
        "        # images shape: [1, 3, H, W]\n",
        "        images = [img.cuda() if torch.cuda.is_available() else img for img in images]\n",
        "        # 1. Inference with Faster R-CNN (optional if you want to combine both approaches)\n",
        "        outputs = model(images)  # list of dict\n",
        "        # The model outputs 'boxes' (N,4), 'labels', 'scores'\n",
        "        pred_boxes = outputs[0]['boxes'].cpu().numpy()  # shape [N,4]\n",
        "        pred_scores = outputs[0]['scores'].cpu().numpy()  # shape [N]\n",
        "        # 2. Or if you want to rely on the test set's detections:\n",
        "        # dets_tensor is shape [K,5] -> [x, y, w, h, conf]\n",
        "        # convert [x, y, w, h] => [x1, y1, x2, y2]\n",
        "        dets_np = dets_tensor[0].numpy()\n",
        "        boxes_xyxy = []\n",
        "        for i in range(dets_np.shape[0]):\n",
        "            x1 = dets_np[i,0]\n",
        "            y1 = dets_np[i,1]\n",
        "            w = dets_np[i,2]\n",
        "            h = dets_np[i,3]\n",
        "            conf = dets_np[i,4]\n",
        "            x2 = x1 + w\n",
        "            y2 = y1 + h\n",
        "            boxes_xyxy.append([x1, y1, x2, y2, conf])\n",
        "        boxes_xyxy = np.array(boxes_xyxy)  # shape: [K,5]\n",
        "\n",
        "        # (Pick whichever detection set you want to track:\n",
        "        #   your model's outputs or the .txt detections. \n",
        "        #   For demonstration, let's track the .txt detections directly.)\n",
        "        try:\n",
        "            tracked_objects = sort_tracker.update(boxes_xyxy)\n",
        "        except Exception as e:\n",
        "            print(f'Error for boxes-  {boxes_xyxy}')\n",
        "            raise e\n",
        "        \n",
        "        # tracked_objects is [N, 5] => [x1, y1, x2, y2, track_id]\n",
        "        if tracked_objects.shape[0] > 0:\n",
        "            for obj in tracked_objects:\n",
        "                x1, y1, x2, y2, track_id = obj\n",
        "                # Store or visualize\n",
        "                all_results.append({\n",
        "                    \"frame_id\": int(frame_id.item()),\n",
        "                    \"track_id\": int(track_id),\n",
        "                    \"bbox\": [float(x1), float(y1), float(x2), float(y2)]\n",
        "                })\n",
        "\n",
        "# all_results will have the final bounding boxes with consistent track IDs across frames\n",
        "print(\"Tracking complete. Results:\", len(all_results))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "f6afd99d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Detection Accuracy: 0.00%\n"
          ]
        }
      ],
      "source": [
        "def compute_iou(boxA, boxB):\n",
        "    \"\"\"\n",
        "    Computes the Intersection over Union (IoU) of two bounding boxes.\n",
        "    Each box is [x1, y1, x2, y2].\n",
        "    \"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    \n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    unionArea = areaA + areaB - interArea\n",
        "    \n",
        "    return interArea / unionArea if unionArea > 0 else 0\n",
        "def evaluate_detection_accuracy(model, dataloader, device, iou_threshold=0.5):\n",
        "    model.eval()\n",
        "    total_accuracy = 0.0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Unpack the batch. If three items are returned (image, target, frame_id), ignore the frame id.\n",
        "            if len(batch) == 3:\n",
        "                images, targets, _ = batch\n",
        "            else:\n",
        "                images, targets = batch\n",
        "\n",
        "            images = [img.to(device) for img in images]\n",
        "            outputs = model(images)\n",
        "            \n",
        "            # Loop over each image in the batch\n",
        "            for output, target in zip(outputs, targets):\n",
        "                pred_boxes = output['boxes'].cpu().numpy()  # shape [N, 4]\n",
        "\n",
        "                # Convert target to bounding box format if it's a tensor.\n",
        "                # Expected format in test dataset: [x, y, w, h, conf]\n",
        "                if isinstance(target, torch.Tensor):\n",
        "                    gt_tensor = target.cpu().numpy()  # shape [K,5]\n",
        "                    gt_boxes = []\n",
        "                    for row in gt_tensor:\n",
        "                        x, y, w, h, _ = row  # ignore confidence\n",
        "                        gt_boxes.append([x, y, x + w, y + h])\n",
        "                    gt_boxes = np.array(gt_boxes)\n",
        "                else:\n",
        "                    # If target is already a dictionary with key 'boxes'\n",
        "                    gt_boxes = target['boxes'].cpu().numpy()\n",
        "                \n",
        "                if len(gt_boxes) == 0:\n",
        "                    continue  # skip if no ground truth boxes\n",
        "                \n",
        "                true_positives = 0\n",
        "                # For each ground truth box, check if any predicted box has sufficient IoU\n",
        "                for gt in gt_boxes:\n",
        "                    for pred in pred_boxes:\n",
        "                        if compute_iou(gt, pred) >= iou_threshold:\n",
        "                            true_positives += 1\n",
        "                            break  # found a match, move to next ground truth box\n",
        "                frame_accuracy = true_positives / len(gt_boxes)\n",
        "                total_accuracy += frame_accuracy\n",
        "                count += 1\n",
        "\n",
        "    return total_accuracy / count if count > 0 else 0\n",
        "\n",
        "# Now evaluate and print the accuracy:\n",
        "model.eval()\n",
        "accuracy = evaluate_detection_accuracy(model, test_loader, device, iou_threshold=0.5)\n",
        "print(f\"Model Detection Accuracy: {accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "f0c8e7ed",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/PIL/ImageFile.py:547\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    548\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[77], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(frame_rgb)\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/matplotlib/pyplot.py:614\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/IPython/core/formatters.py:238\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    236\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m<decorator-gen-2>:2\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/IPython/core/formatters.py:282\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/IPython/core/formatters.py:402\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    404\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/IPython/core/pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 170\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/matplotlib/backend_bases.py:2184\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2184\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/matplotlib/backend_bases.py:2040\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2036\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2039\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2040\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:481\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:430\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    429\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 430\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/matplotlib/image.py:1634\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1633\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1634\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/PIL/Image.py:2568\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2565\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2568\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/PIL/PngImagePlugin.py:1431\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[1;32m   1428\u001b[0m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[1;32m   1429\u001b[0m     )\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im:\n\u001b[0;32m-> 1431\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/PIL/ImageFile.py:551\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    549\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 551\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    553\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.10/site-packages/PIL/ImageFile.py:570\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 570\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    571\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for item in all_results:\n",
        "    # open the corresponding frame\n",
        "    f_id = item[\"frame_id\"]\n",
        "    track_id = item[\"track_id\"]\n",
        "    (x1, y1, x2, y2) = item[\"bbox\"]\n",
        "\n",
        "    img_path = os.path.join(test_seq_path, 'img1', f\"{f_id:06d}.jpg\")\n",
        "    frame_bgr = cv2.imread(img_path)\n",
        "\n",
        "    # draw rectangle\n",
        "    cv2.rectangle(frame_bgr, (int(x1), int(y1)), (int(x2), int(y2)), (0,255,0), 2)\n",
        "    cv2.putText(frame_bgr, f\"ID: {track_id}\", (int(x1), int(y1)-5),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
        "    \n",
        "    # Save or show\n",
        "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(frame_rgb)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1152433",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your image\n",
        "img = cv2.imread('mot17/MOT17/train/MOT17-02-FRCNN/img1/000001.jpg')\n",
        "\n",
        "# Example: assuming first four numbers are x_min, y_min, x_max, y_max\n",
        "x_min, y_min, x_max, y_max = 1, 1, 912, 484\n",
        "\n",
        "# Draw the bounding box (red rectangle)\n",
        "cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2)\n",
        "\n",
        "# Optionally, add text for any additional info (e.g., class or confidence)\n",
        "cv2.putText(img, \"Detected\", (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
