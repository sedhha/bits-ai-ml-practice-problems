{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = r\"2907/checkpoints\"\n",
    "steps_per_checkpoint = 100  # Save a checkpoint every 100 training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for a (potentially small) subset of MOT17 or similar data.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root_dir, \n",
    "                 start_frame=1, \n",
    "                 end_frame=300, \n",
    "                 transforms=None, \n",
    "                 min_visibility=0.3, \n",
    "                 only_person=True):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir, \"img1\")\n",
    "        self.ann_file = os.path.join(root_dir, \"gt\", \"gt.txt\")\n",
    "        self.transforms = transforms\n",
    "        self.start_frame = start_frame\n",
    "        self.end_frame = end_frame\n",
    "        self.min_visibility = min_visibility\n",
    "        self.only_person = only_person\n",
    "        \n",
    "        # Read the annotation file\n",
    "        ann_cols = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\",\"class\",\"visibility\"]\n",
    "        df = pd.read_csv(self.ann_file, header=None, names=ann_cols)\n",
    "        \n",
    "        # Keep only frames in desired range\n",
    "        df = df[(df[\"frame\"] >= self.start_frame) & (df[\"frame\"] <= self.end_frame)]\n",
    "        \n",
    "        # Filter out low-visibility or low-confidence boxes\n",
    "        df = df[df[\"visibility\"] >= self.min_visibility]\n",
    "        df = df[df[\"conf\"] == 1]  # keep only confident = 1\n",
    "        \n",
    "        # (Optional) Keep only 'person' class if needed\n",
    "        if self.only_person:\n",
    "            # Typically 'class=1' in MOT indicates pedestrian\n",
    "            df = df[df[\"class\"] == 1]\n",
    "        \n",
    "        # Group by frame\n",
    "        self.frames_data = df.groupby(\"frame\")\n",
    "        \n",
    "        # Sorted list of valid frames\n",
    "        self.valid_frames = sorted(self.frames_data.groups.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_num = self.valid_frames[idx]\n",
    "        # Build image path\n",
    "        img_filename = f\"{frame_num:06d}.jpg\"\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Gather bounding boxes for this frame\n",
    "        df_frame = self.frames_data.get_group(frame_num)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for _, row in df_frame.iterrows():\n",
    "            xmin = row[\"x\"]\n",
    "            ymin = row[\"y\"]\n",
    "            xmax = xmin + row[\"w\"]\n",
    "            ymax = ymin + row[\"h\"]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            # For MOT, label = 1 for person\n",
    "            labels.append(1)\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Additional fields\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd,\n",
    "            \"frame_num\": torch.tensor([frame_num])  # helps with tracking logic\n",
    "        }\n",
    "        \n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transforms for data augmentation & normalization\n",
    "class CustomTransforms:\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        # Basic transforms for all:\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        # We'll define some random transforms for data augmentation (cropping, flipping, color jitter)\n",
    "        self.augmentations = T.Compose([\n",
    "            T.RandomHorizontalFlip(0.5),\n",
    "            T.RandomResizedCrop(size=(480, 640), scale=(0.8, 1.0)),  # random crop\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "        ])\n",
    "        # Normalization\n",
    "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL -> Tensor\n",
    "        image = self.to_tensor(image)\n",
    "        \n",
    "        if self.train:\n",
    "            # Augment only if training\n",
    "            image = self.augmentations(image)\n",
    "        \n",
    "        # Basic normalization\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 2. MODEL DEVELOPMENT (FASTER R-CNN)\n",
    "##############################################\n",
    "\n",
    "def get_faster_rcnn_model(num_classes=2):\n",
    "    \"\"\"\n",
    "    Pretrained Faster R-CNN with ResNet-50 FPN as backbone.\n",
    "    We replace the final predictor with (num_classes) outputs \n",
    "    (1 background + N classes).\n",
    "    \"\"\"\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 3. TEMPORAL CONSISTENCY + ADAPTIVE TRACKING\n",
    "##############################################\n",
    "\n",
    "class NaiveSORTTracker:\n",
    "    \"\"\"\n",
    "    Minimalistic tracker using a distance-based or IoU-based approach \n",
    "    (inspired by SORT) with optional 'KalmanFilter' stubs or bounding-box smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_distance=100.0):\n",
    "        self.next_id = 1\n",
    "        self.tracks = {}  # id -> (bbox, velocity, last_frame)\n",
    "        self.max_distance = max_distance\n",
    "\n",
    "    def iou(self, boxA, boxB):\n",
    "        \"\"\"Compute IoU between two boxes (x1,y1,x2,y2).\"\"\"\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "        interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "        boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "        boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "        iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "        return iou\n",
    "\n",
    "    def update(self, frame_num, detections):\n",
    "        \"\"\"\n",
    "        detections: list of (bbox, score)\n",
    "        Return: list of (track_id, bbox).\n",
    "        \"\"\"\n",
    "        assigned = {}\n",
    "        track_updates = {}\n",
    "        used_detections = set()\n",
    "\n",
    "        # For each active track, attempt to match with a detection\n",
    "        for t_id, (t_box, velocity, last_fr) in self.tracks.items():\n",
    "            best_iou = 0\n",
    "            best_det_idx = None\n",
    "            for i, (det_box, score) in enumerate(detections):\n",
    "                if i in used_detections:\n",
    "                    continue\n",
    "                iou_val = self.iou(t_box, det_box)\n",
    "                if iou_val > best_iou:\n",
    "                    best_iou = iou_val\n",
    "                    best_det_idx = i\n",
    "            # We set a threshold for iou to maintain track\n",
    "            if best_iou > 0.3 and best_det_idx is not None:\n",
    "                # update track\n",
    "                assigned[t_id] = (best_det_idx, best_iou)\n",
    "                used_detections.add(best_det_idx)\n",
    "\n",
    "        # Now update existing tracks with assigned detections\n",
    "        for t_id, (det_idx, iou_val) in assigned.items():\n",
    "            det_box, score = detections[det_idx]\n",
    "            # naive velocity update\n",
    "            old_box = self.tracks[t_id][0]\n",
    "            vx = (det_box[0] - old_box[0])  \n",
    "            vy = (det_box[1] - old_box[1])\n",
    "            new_velocity = (vx, vy)\n",
    "            track_updates[t_id] = (det_box, new_velocity, frame_num)\n",
    "\n",
    "        # Any detection not assigned => new track\n",
    "        for i, (det_box, score) in enumerate(detections):\n",
    "            if i not in used_detections:\n",
    "                track_updates[self.next_id] = (det_box, (0,0), frame_num)\n",
    "                self.next_id += 1\n",
    "\n",
    "        # Overwrite old tracks with new updated ones\n",
    "        self.tracks.update(track_updates)\n",
    "\n",
    "        # Build output (id, bbox)\n",
    "        results = []\n",
    "        for t_id, (t_box, vel, lf) in self.tracks.items():\n",
    "            if lf == frame_num:  # updated on this frame\n",
    "                results.append((t_id, t_box))\n",
    "        \n",
    "        return results\n",
    "\n",
    "##############################################\n",
    "# 7. ADDING PERFORMANCE COMPARISON\n",
    "##############################################\n",
    "\n",
    "def compare_with_baseline(model, tracker, dataset, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compare the current model and tracker with a baseline approach.\n",
    "    \n",
    "    For baseline, we'll use:\n",
    "    1. A simple tracker without temporal consistency\n",
    "    2. Raw Faster R-CNN detections without tracking\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Faster R-CNN model\n",
    "        tracker: Our tracker implementation\n",
    "        dataset: Validation dataset\n",
    "        device: Device to run computations on\n",
    "        threshold: Detection confidence threshold\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of comparison metrics\n",
    "    \"\"\"\n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'our_approach': {\n",
    "            'detections': 0,\n",
    "            'true_positives': 0,\n",
    "            'id_switches': 0\n",
    "        },\n",
    "        'no_tracking': {\n",
    "            'detections': 0,\n",
    "            'true_positives': 0,\n",
    "        },\n",
    "        'simple_iou_tracker': {\n",
    "            'detections': 0,\n",
    "            'true_positives': 0,\n",
    "            'id_switches': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create a simpler tracker for comparison (just using IoU without velocity)\n",
    "    class SimpleIOUTracker:\n",
    "        def __init__(self, iou_threshold=0.5):\n",
    "            self.next_id = 1\n",
    "            self.tracks = {}  # id -> bbox\n",
    "            self.iou_threshold = iou_threshold\n",
    "            \n",
    "        def update(self, frame_num, detections):\n",
    "            assigned = {}\n",
    "            track_updates = {}\n",
    "            used_detections = set()\n",
    "            \n",
    "            # Match existing tracks with new detections\n",
    "            for t_id, t_box in self.tracks.items():\n",
    "                best_iou = 0\n",
    "                best_det_idx = None\n",
    "                for i, (det_box, score) in enumerate(detections):\n",
    "                    if i in used_detections:\n",
    "                        continue\n",
    "                    iou_val = iou(t_box, det_box)\n",
    "                    if iou_val > best_iou:\n",
    "                        best_iou = iou_val\n",
    "                        best_det_idx = i\n",
    "                        \n",
    "                if best_iou > self.iou_threshold and best_det_idx is not None:\n",
    "                    det_box, _ = detections[best_det_idx]\n",
    "                    track_updates[t_id] = det_box\n",
    "                    used_detections.add(best_det_idx)\n",
    "            \n",
    "            # Any detection not assigned => new track\n",
    "            for i, (det_box, score) in enumerate(detections):\n",
    "                if i not in used_detections:\n",
    "                    track_updates[self.next_id] = det_box\n",
    "                    self.next_id += 1\n",
    "                    \n",
    "            # Update tracks\n",
    "            self.tracks = track_updates\n",
    "            \n",
    "            # Build output (id, bbox)\n",
    "            results = [(t_id, t_box) for t_id, t_box in self.tracks.items()]\n",
    "            return results\n",
    "    \n",
    "    # Create a simple IoU tracker\n",
    "    simple_tracker = SimpleIOUTracker()\n",
    "    \n",
    "    # Track identity switches for both trackers\n",
    "    last_matched_ids = {'our_approach': {}, 'simple_iou_tracker': {}}\n",
    "    \n",
    "    # Process each frame\n",
    "    for idx in tqdm(range(len(dataset)), desc=\"Comparing with baseline\"):\n",
    "        img, target = dataset[idx]\n",
    "        frame_num = target[\"frame_num\"].item()\n",
    "        gt_boxes = target[\"boxes\"].numpy()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Detect objects in the frame\n",
    "            detection = model([img.to(device)])[0]\n",
    "        \n",
    "        # Process raw detections\n",
    "        pred_boxes = detection[\"boxes\"].cpu().numpy()\n",
    "        scores = detection[\"scores\"].cpu().numpy()\n",
    "        \n",
    "        # Filter by threshold\n",
    "        high_conf_indices = np.where(scores > threshold)[0]\n",
    "        pred_boxes = pred_boxes[high_conf_indices]\n",
    "        scores = scores[high_conf_indices]\n",
    "        \n",
    "        # Create list of (bbox, score) for trackers\n",
    "        detections = [(box, score) for box, score in zip(pred_boxes, scores)]\n",
    "        \n",
    "        # 1. Our approach: Update our tracker\n",
    "        our_tracks = tracker.update(frame_num, detections)\n",
    "        our_track_boxes = [box for _, box in our_tracks]\n",
    "        our_track_ids = [tid for tid, _ in our_tracks]\n",
    "        \n",
    "        # 2. Simple IOU tracker: Update simple tracker\n",
    "        simple_tracks = simple_tracker.update(frame_num, detections)\n",
    "        simple_track_boxes = [box for _, box in simple_tracks]\n",
    "        simple_track_ids = [tid for tid, _ in simple_tracks]\n",
    "        \n",
    "        # 3. No tracking: Just use raw detections\n",
    "        no_track_boxes = pred_boxes\n",
    "        \n",
    "        # Count detections\n",
    "        metrics['our_approach']['detections'] += len(our_track_boxes)\n",
    "        metrics['simple_iou_tracker']['detections'] += len(simple_track_boxes)\n",
    "        metrics['no_tracking']['detections'] += len(no_track_boxes)\n",
    "        \n",
    "        # Evaluate against ground truth\n",
    "        for approach, boxes, ids in [\n",
    "            ('our_approach', our_track_boxes, our_track_ids),\n",
    "            ('simple_iou_tracker', simple_track_boxes, simple_track_ids)\n",
    "        ]:\n",
    "            # Match with ground truth\n",
    "            for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                best_iou = 0.5  # IoU threshold\n",
    "                best_match = -1\n",
    "                \n",
    "                for j, box in enumerate(boxes):\n",
    "                    iou_val = iou(gt_box, box)\n",
    "                    if iou_val > best_iou:\n",
    "                        best_iou = iou_val\n",
    "                        best_match = j\n",
    "                \n",
    "                if best_match >= 0:\n",
    "                    # We have a match (true positive)\n",
    "                    metrics[approach]['true_positives'] += 1\n",
    "                    \n",
    "                    # Check for ID switch\n",
    "                    gt_key = tuple(gt_box.tolist())\n",
    "                    if gt_key in last_matched_ids[approach]:\n",
    "                        if last_matched_ids[approach][gt_key] != ids[best_match]:\n",
    "                            metrics[approach]['id_switches'] += 1\n",
    "                    \n",
    "                    # Update last matched ID\n",
    "                    last_matched_ids[approach][gt_key] = ids[best_match]\n",
    "        \n",
    "        # For no tracking approach (just detections)\n",
    "        for gt_box in gt_boxes:\n",
    "            best_iou = 0.5\n",
    "            for box in no_track_boxes:\n",
    "                iou_val = iou(gt_box, box)\n",
    "                if iou_val > best_iou:\n",
    "                    best_iou = iou_val\n",
    "                    metrics['no_tracking']['true_positives'] += 1\n",
    "                    break\n",
    "    \n",
    "    # Calculate comparative metrics\n",
    "    for approach in metrics:\n",
    "        metrics[approach]['precision'] = metrics[approach]['true_positives'] / max(1, metrics[approach]['detections'])\n",
    "        metrics[approach]['recall'] = metrics[approach]['true_positives'] / max(1, len(dataset) * 5)  # Assuming ~5 objects per frame\n",
    "        \n",
    "        if approach != 'no_tracking':\n",
    "            metrics[approach]['id_switch_rate'] = metrics[approach]['id_switches'] / len(dataset)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compute_average_precision(detections, ground_truths, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute Average Precision for a specific class.\n",
    "    \n",
    "    Args:\n",
    "        detections: List of (boxes, scores) tuples for each image\n",
    "        ground_truths: List of ground truth boxes for each image\n",
    "        iou_threshold: IoU threshold for considering a detection as correct\n",
    "        \n",
    "    Returns:\n",
    "        Average Precision score\n",
    "    \"\"\"\n",
    "    # Total number of ground truth boxes across all images\n",
    "    total_gt = sum(len(gt) for gt in ground_truths)\n",
    "    \n",
    "    if total_gt == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Flatten all detections across images\n",
    "    all_scores = []\n",
    "    all_matched = []\n",
    "    \n",
    "    # Keep track of which ground truth boxes have been matched\n",
    "    gt_matched = []\n",
    "    for i in range(len(ground_truths)):\n",
    "        gt_matched.append(np.zeros(len(ground_truths[i]), dtype=bool))\n",
    "    \n",
    "    # Process each image\n",
    "    for i in range(len(detections)):\n",
    "        boxes, scores = detections[i]\n",
    "        gt_boxes = ground_truths[i]\n",
    "        \n",
    "        for j in range(len(boxes)):\n",
    "            all_scores.append(scores[j])\n",
    "            \n",
    "            # Check if this detection matches any ground truth\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            for k in range(len(gt_boxes)):\n",
    "                if gt_matched[i][k]:\n",
    "                    continue  # Skip already matched ground truths\n",
    "                    \n",
    "                iou_val = iou(boxes[j], gt_boxes[k])\n",
    "                if iou_val > best_iou:\n",
    "                    best_iou = iou_val\n",
    "                    best_gt_idx = k\n",
    "            \n",
    "            # Consider match if IoU > threshold\n",
    "            if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
    "                all_matched.append(1)\n",
    "                gt_matched[i][best_gt_idx] = True\n",
    "            else:\n",
    "                all_matched.append(0)\n",
    "    \n",
    "    # Sort by confidence score\n",
    "    indices = np.argsort(-np.array(all_scores))\n",
    "    all_matched = np.array(all_matched)[indices]\n",
    "    \n",
    "    # Compute cumulative TP and FP\n",
    "    tp = np.cumsum(all_matched)\n",
    "    fp = np.cumsum(1 - all_matched)\n",
    "    \n",
    "    # Compute precision and recall\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall = tp / total_gt\n",
    "    \n",
    "    # Compute AP using 11-point interpolation\n",
    "    ap = 0\n",
    "    for t in np.arange(0, 1.1, 0.1):\n",
    "        if np.sum(recall >= t) == 0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = np.max(precision[recall >= t])\n",
    "        ap += p / 11\n",
    "    \n",
    "    return ap\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Compute IoU between two boxes [x1, y1, x2, y2].\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    # Compute area of intersection\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    \n",
    "    # Compute area of both boxes\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    # Compute IoU\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-10)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def compute_tracking_accuracy_and_id_switches(tracker_results, ground_truth):\n",
    "    \"\"\"\n",
    "    Compute tracking accuracy and identity switch rate.\n",
    "    \n",
    "    Args:\n",
    "        tracker_results: Dictionary mapping frame_num to list of (track_id, bbox)\n",
    "        ground_truth: Dictionary mapping frame_num to list of gt_boxes\n",
    "        \n",
    "    Returns:\n",
    "        tracking_accuracy: Overall tracking accuracy (MOTA-inspired metric)\n",
    "        identity_switch_rate: Rate of ID switches per frame\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    total_gt = 0\n",
    "    total_matches = 0\n",
    "    total_misses = 0\n",
    "    total_false_positives = 0\n",
    "    total_id_switches = 0\n",
    "    \n",
    "    # Dictionary to keep track of last matched track_id for each gt box\n",
    "    last_matched_ids = {}\n",
    "    \n",
    "    # Process each frame\n",
    "    common_frames = sorted(set(tracker_results.keys()) & set(ground_truth.keys()))\n",
    "    \n",
    "    for frame_num in common_frames:\n",
    "        tracker_boxes = [box for _, box in tracker_results[frame_num]]\n",
    "        tracker_ids = [tid for tid, _ in tracker_results[frame_num]]\n",
    "        gt_boxes = ground_truth[frame_num]\n",
    "        \n",
    "        total_gt += len(gt_boxes)\n",
    "        \n",
    "        # Keep track of which ground truth and tracker boxes have been matched\n",
    "        gt_matched = [False] * len(gt_boxes)\n",
    "        tracker_matched = [False] * len(tracker_boxes)\n",
    "        \n",
    "        # Match ground truth with tracker boxes using IoU\n",
    "        for i, gt_box in enumerate(gt_boxes):\n",
    "            best_iou = 0.5  # Minimum IoU threshold\n",
    "            best_match = -1\n",
    "            \n",
    "            for j, tracker_box in enumerate(tracker_boxes):\n",
    "                if tracker_matched[j]:\n",
    "                    continue\n",
    "                    \n",
    "                iou_val = iou(gt_box, tracker_box)\n",
    "                if iou_val > best_iou:\n",
    "                    best_iou = iou_val\n",
    "                    best_match = j\n",
    "            \n",
    "            if best_match >= 0:\n",
    "                # We have a match\n",
    "                gt_matched[i] = True\n",
    "                tracker_matched[best_match] = True\n",
    "                \n",
    "                # Check if there's an ID switch\n",
    "                gt_idx = tuple(gt_box.tolist())  # Convert to hashable tuple\n",
    "                if gt_idx in last_matched_ids:\n",
    "                    if last_matched_ids[gt_idx] != tracker_ids[best_match]:\n",
    "                        total_id_switches += 1\n",
    "                \n",
    "                # Update last matched ID\n",
    "                last_matched_ids[gt_idx] = tracker_ids[best_match]\n",
    "                total_matches += 1\n",
    "            else:\n",
    "                total_misses += 1\n",
    "        \n",
    "        # Count false positives\n",
    "        total_false_positives += sum(not x for x in tracker_matched)\n",
    "    \n",
    "    # Compute MOTA-inspired tracking accuracy\n",
    "    tracking_accuracy = 1 - (total_misses + total_false_positives + total_id_switches) / max(1, total_gt)\n",
    "    \n",
    "    # Compute identity switch rate (ID switches per frame)\n",
    "    identity_switch_rate = total_id_switches / len(common_frames) if common_frames else 0\n",
    "    \n",
    "    return tracking_accuracy, identity_switch_rate\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_map(model, data_loader, device, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute mean Average Precision (mAP) at a given IoU threshold.\n",
    "    \n",
    "    Args:\n",
    "        model: The detection model\n",
    "        data_loader: DataLoader for the validation set\n",
    "        device: Device to run computations on\n",
    "        iou_threshold: IoU threshold for considering a detection as correct\n",
    "        \n",
    "    Returns:\n",
    "        mAP: Mean Average Precision score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store precision and recall values for each class\n",
    "    all_detections = []\n",
    "    all_ground_truths = []\n",
    "    \n",
    "    # Collect all predictions and ground truths\n",
    "    for images, targets in tqdm(data_loader, desc=\"Computing mAP\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i, output in enumerate(outputs):\n",
    "            pred_boxes = output[\"boxes\"].cpu().numpy()\n",
    "            pred_scores = output[\"scores\"].cpu().numpy()\n",
    "            pred_labels = output[\"labels\"].cpu().numpy()\n",
    "            \n",
    "            gt_boxes = targets[i][\"boxes\"].cpu().numpy()\n",
    "            gt_labels = targets[i][\"labels\"].cpu().numpy()\n",
    "            \n",
    "            # Store predictions and ground truths for this image\n",
    "            all_detections.append((pred_boxes, pred_scores, pred_labels))\n",
    "            all_ground_truths.append((gt_boxes, gt_labels))\n",
    "    \n",
    "    # Compute AP for each class (in this case, just \"person\" class=1)\n",
    "    class_ids = [1]  # Person class\n",
    "    aps = []\n",
    "    \n",
    "    for class_id in class_ids:\n",
    "        # Get detections and ground truths for this class\n",
    "        class_detections = []\n",
    "        class_ground_truths = []\n",
    "        \n",
    "        for i in range(len(all_detections)):\n",
    "            pred_boxes, pred_scores, pred_labels = all_detections[i]\n",
    "            gt_boxes, gt_labels = all_ground_truths[i]\n",
    "            \n",
    "            # Filter by class\n",
    "            class_pred_indices = np.where(pred_labels == class_id)[0]\n",
    "            class_gt_indices = np.where(gt_labels == class_id)[0]\n",
    "            \n",
    "            class_pred_boxes = pred_boxes[class_pred_indices]\n",
    "            class_pred_scores = pred_scores[class_pred_indices]\n",
    "            class_gt_boxes = gt_boxes[class_gt_indices]\n",
    "            \n",
    "            # Sort predictions by confidence score (descending)\n",
    "            sorted_indices = np.argsort(-class_pred_scores)\n",
    "            class_pred_boxes = class_pred_boxes[sorted_indices]\n",
    "            class_pred_scores = class_pred_scores[sorted_indices]\n",
    "            \n",
    "            class_detections.append((class_pred_boxes, class_pred_scores))\n",
    "            class_ground_truths.append(class_gt_boxes)\n",
    "        \n",
    "        # Compute precision and recall\n",
    "        ap = compute_average_precision(class_detections, class_ground_truths, iou_threshold)\n",
    "        aps.append(ap)\n",
    "    \n",
    "    # Return mean AP across all classes\n",
    "    return np.mean(aps)\n",
    "\n",
    "\n",
    "##############################################\n",
    "# 5. TRAINING LOOP WITH TQDM + CHECKPOINTING\n",
    "##############################################\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch_idx, global_step=0):\n",
    "    \"\"\"\n",
    "    Train for one epoch. \n",
    "    'global_step' is incremented each batch for frequent checkpointing.\n",
    "    Returns: (average_loss, updated_global_step)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch_idx+1} Training\", leave=False)\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{losses.item():.4f}\"})\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "        # ---------- Frequent checkpointing ----------\n",
    "        if global_step % steps_per_checkpoint == 0:\n",
    "            # We'll do a mid-epoch checkpoint\n",
    "            save_checkpoint(epoch_idx, global_step, model, optimizer,\n",
    "                            checkpoint_dir=checkpoints_path,\n",
    "                            filename_prefix=f\"checkpoint_step_{global_step}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss, global_step\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_model(model, data_loader, device, epoch_idx):\n",
    "    model.eval()\n",
    "    val_pbar = tqdm(data_loader, desc=f\"Epoch {epoch_idx+1} Validation\", leave=False)\n",
    "    total_detections = 0\n",
    "    total_gt_boxes = 0\n",
    "    for images, targets in val_pbar:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        outputs = model(images)\n",
    "        for i, output in enumerate(outputs):\n",
    "            total_detections += len(output[\"boxes\"])\n",
    "            total_gt_boxes += len(targets[i][\"boxes\"])\n",
    "    if len(data_loader) > 0:\n",
    "        val_pbar.set_postfix({\"avg_detected_per_image\": total_detections / len(data_loader)})\n",
    "    return total_detections, total_gt_boxes\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, global_step, model, optimizer, checkpoint_dir=checkpoints_path, filename_prefix=\"checkpoint\"):\n",
    "    \"\"\"\n",
    "    Saves a checkpoint with epoch, global_step, model state, and optimizer state.\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{filename_prefix}.pth\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    # Also save a 'last_checkpoint.pth' for easy resuming\n",
    "    last_ckpt_path = os.path.join(checkpoint_dir, \"last_checkpoint.pth\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }, last_ckpt_path)\n",
    "    print(f\"=> Checkpoint saved at {checkpoint_path} and {last_ckpt_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint_if_available(model, optimizer, checkpoint_dir=checkpoints_path):\n",
    "    \"\"\"\n",
    "    Checks if 'last_checkpoint.pth' exists and loads it.\n",
    "    Returns (start_epoch, global_step).\n",
    "    \"\"\"\n",
    "    last_ckpt_path = os.path.join(checkpoint_dir, \"last_checkpoint.pth\")\n",
    "    if os.path.exists(last_ckpt_path):\n",
    "        ckpt = torch.load(last_ckpt_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        global_step = ckpt.get(\"global_step\", 0)\n",
    "        print(f\"=> Loaded checkpoint at epoch={ckpt['epoch']}, step={global_step} from {last_ckpt_path}\")\n",
    "        return start_epoch, global_step\n",
    "    else:\n",
    "        print(\"=> No existing checkpoint found. Starting from scratch.\")\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 6. MAIN SCRIPT\n",
    "##############################################\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main driver function with frequent checkpointing and mid-epoch saves.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "    \n",
    "    dataset_root = r\"2907/mot17/MOT17Det/train/MOT17-02\"\n",
    "    train_start, train_end = 1, 300\n",
    "    val_start, val_end = 301, 350\n",
    "    \n",
    "    # Data sets\n",
    "    dataset_train = MOTDataset(\n",
    "        root_dir=dataset_root,\n",
    "        start_frame=train_start,\n",
    "        end_frame=train_end,\n",
    "        transforms=CustomTransforms(train=True)\n",
    "    )\n",
    "    dataset_val = MOTDataset(\n",
    "        root_dir=dataset_root,\n",
    "        start_frame=val_start,\n",
    "        end_frame=val_end,\n",
    "        transforms=CustomTransforms(train=False)\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = get_faster_rcnn_model(num_classes=2).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Load from checkpoint if available\n",
    "    start_epoch, global_step = load_checkpoint_if_available(model, optimizer, checkpoints_path)\n",
    "    \n",
    "    # Training\n",
    "    num_epochs = 5\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_loss, global_step = train_one_epoch(model, optimizer, train_loader, device, epoch, global_step)\n",
    "        print(f\"[Epoch {epoch+1}] Training loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Validation step\n",
    "        total_detections, total_gt_boxes = validate_model(model, val_loader, device, epoch)\n",
    "        print(f\"[Epoch {epoch+1}] Avg det/frame: {total_detections / len(val_loader):.2f}, GT/frame: {total_gt_boxes / len(val_loader):.2f}\")\n",
    "        \n",
    "        # End-of-epoch checkpoint\n",
    "        save_checkpoint(epoch, global_step, model, optimizer, checkpoint_dir=checkpoints_path,\n",
    "                        filename_prefix=f\"checkpoint_epoch_{epoch}\")\n",
    "    \n",
    "    # Evaluate with proper mAP calculation\n",
    "    print(\"\\nCalculating mAP on validation set...\")\n",
    "    map_val = compute_map(model, val_loader, device)\n",
    "    print(f\"mAP @ IoU=0.5: {map_val:.4f}\")\n",
    "    \n",
    "    # Tracking example\n",
    "    print(\"\\nPerforming tracking and evaluation...\")\n",
    "    tracker = NaiveSORTTracker()\n",
    "    \n",
    "    # We'll store results for each frame to measure tracking performance\n",
    "    track_results = {}\n",
    "    gt_dict = {}\n",
    "    \n",
    "    # Inference on the validation set frame by frame\n",
    "    for idx in tqdm(range(len(dataset_val)), desc=\"Tracking\"):\n",
    "        img, target = dataset_val[idx]\n",
    "        frame_num = target[\"frame_num\"].item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            detection = model([img.to(device)])[0]\n",
    "        \n",
    "        # Convert predictions to CPU numpy for the tracker\n",
    "        pred_boxes = detection[\"boxes\"].cpu().numpy()\n",
    "        scores = detection[\"scores\"].cpu().numpy()\n",
    "        \n",
    "        # Build list of (bbox, score)\n",
    "        detections = []\n",
    "        for b, s in zip(pred_boxes, scores):\n",
    "            if s > 0.5:\n",
    "                detections.append((b, s))\n",
    "        \n",
    "        # Update tracker\n",
    "        assigned_tracks = tracker.update(frame_num, detections)\n",
    "        track_results[frame_num] = assigned_tracks\n",
    "        \n",
    "        # Save ground truth\n",
    "        gt_boxes_np = target[\"boxes\"].numpy()\n",
    "        gt_dict[frame_num] = gt_boxes_np\n",
    "    \n",
    "    # Compute tracking metrics\n",
    "    tracking_acc, id_switch_rate = compute_tracking_accuracy_and_id_switches(track_results, gt_dict)\n",
    "    print(f\"Tracking Accuracy: {tracking_acc:.4f}\")\n",
    "    print(f\"Identity Switch Rate: {id_switch_rate:.4f}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    print(\"\\nComparing with baseline approaches...\")\n",
    "    comparison_metrics = compare_with_baseline(model, tracker, dataset_val, device)\n",
    "    \n",
    "    # Print comparison results\n",
    "    print(\"\\nComparison Results:\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(\"Method                | Precision | Recall | ID Switch Rate\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(f\"Our Approach          | {comparison_metrics['our_approach']['precision']:.4f} | {comparison_metrics['our_approach']['recall']:.4f} | {comparison_metrics['our_approach'].get('id_switch_rate', 0):.4f}\")\n",
    "    print(f\"Simple IoU Tracker    | {comparison_metrics['simple_iou_tracker']['precision']:.4f} | {comparison_metrics['simple_iou_tracker']['recall']:.4f} | {comparison_metrics['simple_iou_tracker'].get('id_switch_rate', 0):.4f}\")\n",
    "    print(f\"No Tracking           | {comparison_metrics['no_tracking']['precision']:.4f} | {comparison_metrics['no_tracking']['recall']:.4f} | N/A\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Finished training, validation, tracking, and evaluation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
