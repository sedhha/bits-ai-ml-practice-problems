{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkvJHiaGmIr5SyOj5psmt1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6S3cLza3h6zX","executionInfo":{"status":"ok","timestamp":1735463118313,"user_tz":-330,"elapsed":216428,"user":{"displayName":"Chandra Sekhar","userId":"00330720378866032186"}},"outputId":"23a0d765-1fc2-4845-e7bb-643b18828c8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 38ms/step - accuracy: 0.2758 - loss: 1.9607\n","Epoch 2/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 39ms/step - accuracy: 0.4278 - loss: 1.6136\n","Epoch 3/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 41ms/step - accuracy: 0.4677 - loss: 1.4942\n","Epoch 4/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38ms/step - accuracy: 0.4967 - loss: 1.4205\n","Epoch 5/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 40ms/step - accuracy: 0.5197 - loss: 1.3550\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.5279 - loss: 1.3480\n","Test Accuracy: 0.517799973487854\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","# Use MirroredStrategy for distributed training simulation\n","strategy = tf.distribute.MirroredStrategy()\n","\n","# Define the LeNet model\n","with strategy.scope():\n","    model = models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),\n","        layers.AveragePooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'),\n","        layers.AveragePooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dense(120, activation='relu'),\n","        layers.Dense(84, activation='relu'),\n","        layers.Dense(10, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","# Dataset preprocessing\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Prepare datasets\n","train_dataset, test_dataset = create_dataset()\n","\n","# Distribute the dataset\n","train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n","\n","# Train the model\n","model.fit(train_dist_dataset, epochs=5)\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(test_dist_dataset)\n","print(f\"Test Accuracy: {accuracy}\")\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","# Use MirroredStrategy for distributed training simulation\n","strategy = tf.distribute.MirroredStrategy()\n","\n","# Define the LeNet model\n","with strategy.scope():\n","    model = models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),  # Input layer\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Flatten(),                                        # Flatten\n","        layers.Dense(120, activation='relu'),                   # Dense layer\n","        layers.Dense(84, activation='relu'),                    # Dense layer\n","        layers.Dense(10, activation='softmax')                  # Output layer\n","    ])\n","    model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","# Dataset preparation\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Prepare datasets\n","train_dataset, test_dataset = create_dataset()\n","\n","# Distribute the dataset\n","train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n","\n","# Callback to observe gradients and weights\n","class GradientCallback(tf.keras.callbacks.Callback):\n","    def on_train_batch_end(self, batch, logs=None):\n","        print(f\"\\n--- Batch {batch} ---\")\n","        for i, weight in enumerate(self.model.trainable_weights):\n","            print(f\"Layer {i} weight shape: {weight.shape}\")\n","            print(f\"Sample weight values: {weight.numpy().flatten()[:5]}\")  # Display first 5 weights\n","        print(\"------------------\")\n","\n","# Function to print data distribution across replicas\n","@tf.function\n","def print_batch_distribution(features, labels):\n","    replica_context = tf.distribute.get_replica_context()\n","    tf.print(f\"Replica {replica_context.replica_id_in_sync_group}: \"\n","             f\"Feature shape: {tf.shape(features)}, Label shape: {tf.shape(labels)}\")\n","\n","# Observe how dataset is distributed\n","iterator = iter(train_dist_dataset)\n","features, labels = next(iterator)\n","strategy.run(print_batch_distribution, args=(features, labels))\n","\n","# Train the model and observe updates\n","model.fit(train_dist_dataset, epochs=1, callbacks=[GradientCallback()])\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(test_dist_dataset)\n","print(f\"Test Loss: {loss}\")\n","print(f\"Test Accuracy: {accuracy}\")\n"],"metadata":{"id":"IeUE3JWHlzBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import numpy as np\n","\n","# Define the LeNet model\n","def create_model():\n","    model = models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),  # Input layer\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Flatten(),                                        # Flatten\n","        layers.Dense(120, activation='relu'),                   # Dense layer\n","        layers.Dense(84, activation='relu'),                    # Dense layer\n","        layers.Dense(10, activation='softmax')                  # Output layer\n","    ])\n","    return model\n","\n","# Simulate multiple workers\n","class Worker:\n","    def __init__(self, worker_id, model):\n","        self.worker_id = worker_id\n","        self.model = model\n","        self.optimizer = tf.keras.optimizers.Adam()\n","        print(f\"Worker {worker_id} initialized.\")\n","\n","    def compute_gradients(self, features, labels):\n","        with tf.GradientTape() as tape:\n","            predictions = self.model(features, training=True)\n","            loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n","            loss = tf.reduce_mean(loss)\n","        gradients = tape.gradient(loss, self.model.trainable_variables)\n","        return gradients, loss\n","\n","    def set_parameters(self, parameters):\n","        self.model.set_weights(parameters)\n","\n","# Central Parameter Server\n","class ParameterServer:\n","    def __init__(self, model):\n","        self.model = model\n","        print(\"Parameter Server initialized.\")\n","\n","    def get_parameters(self):\n","        return self.model.get_weights()\n","\n","    def update_parameters(self, aggregated_gradients):\n","        optimizer = tf.keras.optimizers.Adam()  # Use a single optimizer for the parameter server\n","        optimizer.apply_gradients(zip(aggregated_gradients, self.model.trainable_variables))\n","        print(\"Updated parameters on the Parameter Server.\")\n","\n","# Dataset preparation\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Initialize components\n","train_dataset, test_dataset = create_dataset()\n","global_model = create_model()\n","parameter_server = ParameterServer(global_model)\n","\n","# Simulate 2 workers\n","workers = [Worker(worker_id=i, model=create_model()) for i in range(2)]\n","\n","# Training loop\n","epochs = 1\n","for epoch in range(epochs):\n","    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n","    for step, (features, labels) in enumerate(train_dataset):\n","        # Split data across workers (simple round-robin split for simulation)\n","        worker_data = np.array_split(features.numpy(), len(workers))\n","        worker_labels = np.array_split(labels.numpy(), len(workers))\n","\n","        # Each worker computes gradients\n","        worker_gradients = []\n","        losses = []\n","        for worker, data, lbl in zip(workers, worker_data, worker_labels):\n","            print(f\"Worker {worker.worker_id} processing data batch {step}\")\n","            gradients, loss = worker.compute_gradients(tf.convert_to_tensor(data), tf.convert_to_tensor(lbl))\n","            worker_gradients.append(gradients)\n","            losses.append(loss.numpy())\n","            print(f\"Worker {worker.worker_id} gradients (layer 0 sample): {gradients[0].numpy().flatten()[:5]}\")\n","\n","        # Aggregate gradients on the parameter server\n","        aggregated_gradients = [\n","            tf.reduce_mean([grad[i] for grad in worker_gradients], axis=0)\n","            for i in range(len(worker_gradients[0]))\n","        ]\n","        print(f\"Aggregated gradients (layer 0 sample): {aggregated_gradients[0].numpy().flatten()[:5]}\")\n","\n","        # Update parameters on the parameter server\n","        print(f\"Parameters before update (layer 0 sample): {parameter_server.get_parameters()[0].flatten()[:5]}\")\n","        parameter_server.update_parameters(aggregated_gradients)\n","        print(f\"Parameters after update (layer 0 sample): {parameter_server.get_parameters()[0].flatten()[:5]}\")\n","\n","        # Broadcast updated parameters to workers\n","        for worker in workers:\n","            worker.set_parameters(parameter_server.get_parameters())\n","            print(f\"Worker {worker.worker_id} synchronized with updated parameters.\")\n","\n","        if step % 10 == 0:\n","            print(f\"Step {step}, Losses: {losses}\")\n","\n","# Compile the global model for evaluation\n","global_model.compile(optimizer='adam',\n","                     loss='sparse_categorical_crossentropy',\n","                     metrics=['accuracy'])\n","\n","# Evaluate the global model\n","test_loss, test_acc = global_model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_acc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfWncVj6n-m3","executionInfo":{"status":"ok","timestamp":1735464302609,"user_tz":-330,"elapsed":151119,"user":{"displayName":"Chandra Sekhar","userId":"00330720378866032186"}},"outputId":"d5cf42ec-4b5a-40a0-9a0d-3bdba8736ae6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592758 0.00917801 0.15775949 0.04945175 0.03161769]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 289\n","Worker 0 gradients (layer 0 sample): [ 0.30667242  0.5697729  -0.44856307 -1.6287508  -0.15269202]\n","Worker 1 processing data batch 289\n","Worker 1 gradients (layer 0 sample): [ 0.2858757   0.5037129  -0.39404172 -1.5788977  -0.2402962 ]\n","Aggregated gradients (layer 0 sample): [ 0.29627407  0.5367429  -0.42130238 -1.6038243  -0.1964941 ]\n","Parameters before update (layer 0 sample): [0.14592758 0.00917801 0.15775949 0.04945175 0.03161769]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492759 0.00817803 0.15875947 0.05045174 0.03261766]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 290\n","Worker 0 gradients (layer 0 sample): [-0.6515429  -0.26452297  1.4414347   5.6289635   0.48781213]\n","Worker 1 processing data batch 290\n","Worker 1 gradients (layer 0 sample): [-0.44477567 -0.72779477  0.9609237   3.9466786   0.2487039 ]\n","Aggregated gradients (layer 0 sample): [-0.5481593  -0.49615887  1.2011793   4.787821    0.368258  ]\n","Parameters before update (layer 0 sample): [0.14492759 0.00817803 0.15875947 0.05045174 0.03261766]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592758 0.00917801 0.15775949 0.04945175 0.03161768]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 290, Losses: [2.3117607, 2.0585027]\n","Worker 0 processing data batch 291\n","Worker 0 gradients (layer 0 sample): [ 0.1547762   0.36717832 -0.25180614 -0.99023587 -0.1440762 ]\n","Worker 1 processing data batch 291\n","Worker 1 gradients (layer 0 sample): [ 0.00992802  0.24337459 -0.20509131 -0.2370496  -0.09413391]\n","Aggregated gradients (layer 0 sample): [ 0.08235211  0.30527645 -0.22844872 -0.61364275 -0.11910506]\n","Parameters before update (layer 0 sample): [0.14592758 0.00917801 0.15775949 0.04945175 0.03161768]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492762 0.00817803 0.15875946 0.05045174 0.03261765]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 292\n","Worker 0 gradients (layer 0 sample): [-0.6052247  -0.8000517   1.2127067   5.8171244   0.39587945]\n","Worker 1 processing data batch 292\n","Worker 1 gradients (layer 0 sample): [ 0.06102815 -1.2779529   0.19519103  0.6676974  -0.45399475]\n","Aggregated gradients (layer 0 sample): [-0.2720983  -1.0390023   0.70394886  3.242411   -0.02905765]\n","Parameters before update (layer 0 sample): [0.14492762 0.00817803 0.15875946 0.05045174 0.03261765]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592761 0.00917802 0.15775947 0.04945175 0.03361753]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 293\n","Worker 0 gradients (layer 0 sample): [-0.18788095  3.4909565  -0.48173067 -1.5876114   0.8848417 ]\n","Worker 1 processing data batch 293\n","Worker 1 gradients (layer 0 sample): [-0.02908196  0.454567    0.02017159  0.03788942  0.09574535]\n","Aggregated gradients (layer 0 sample): [-0.10848145  1.9727618  -0.23077954 -0.774861    0.4902935 ]\n","Parameters before update (layer 0 sample): [0.14592761 0.00917802 0.15775947 0.04945175 0.03361753]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692757 0.00817803 0.15875944 0.05045174 0.03261754]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 294\n","Worker 0 gradients (layer 0 sample): [ 0.09166332 -0.85191023  0.22574952  0.62249506 -0.20722756]\n","Worker 1 processing data batch 294\n","Worker 1 gradients (layer 0 sample): [-0.253511   -0.05509764  0.23579201  1.7576644   0.11702114]\n","Aggregated gradients (layer 0 sample): [-0.08092384 -0.45350394  0.23077077  1.1900797  -0.04510321]\n","Parameters before update (layer 0 sample): [0.14692757 0.00817803 0.15875944 0.05045174 0.03261754]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792752 0.00917801 0.15775947 0.04945175 0.03361747]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 295\n","Worker 0 gradients (layer 0 sample): [-0.01646503  0.37351316 -0.02758867  0.32197553  0.0778047 ]\n","Worker 1 processing data batch 295\n","Worker 1 gradients (layer 0 sample): [-0.04318362  2.0013387  -0.42255825 -1.1993642   0.4731308 ]\n","Aggregated gradients (layer 0 sample): [-0.02982432  1.187426   -0.22507346 -0.43869433  0.27546775]\n","Parameters before update (layer 0 sample): [0.14792752 0.00917801 0.15775947 0.04945175 0.03361747]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489274  0.00817802 0.15875944 0.05045174 0.03261748]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 296\n","Worker 0 gradients (layer 0 sample): [-0.5191809   0.02056209  0.48869273  3.3981366   0.40320897]\n","Worker 1 processing data batch 296\n","Worker 1 gradients (layer 0 sample): [-0.1744268  -0.6504701   0.31141835  1.7542474  -0.00761845]\n","Aggregated gradients (layer 0 sample): [-0.34680384 -0.31495398  0.40005553  2.576192    0.19779526]\n","Parameters before update (layer 0 sample): [0.1489274  0.00817802 0.15875944 0.05045174 0.03261748]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499274  0.00917801 0.15775946 0.04945175 0.03161751]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 297\n","Worker 0 gradients (layer 0 sample): [ 0.6474049   1.0732057  -0.44694948 -3.4885683  -0.23929077]\n","Worker 1 processing data batch 297\n","Worker 1 gradients (layer 0 sample): [ 0.61721176  2.3755667  -0.6525452  -4.144005   -0.08533949]\n","Aggregated gradients (layer 0 sample): [ 0.63230836  1.7243862  -0.54974735 -3.8162866  -0.16231513]\n","Parameters before update (layer 0 sample): [0.1499274  0.00917801 0.15775946 0.04945175 0.03161751]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489274  0.00817802 0.15875944 0.05045174 0.03261748]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 298\n","Worker 0 gradients (layer 0 sample): [-0.18470249 -0.8719735   0.7079396   2.8284278  -0.05604728]\n","Worker 1 processing data batch 298\n","Worker 1 gradients (layer 0 sample): [ 0.02368824 -0.7237607   0.22297794  0.95344836 -0.22577915]\n","Aggregated gradients (layer 0 sample): [-0.08050712 -0.7978671   0.46545878  1.890938   -0.14091322]\n","Parameters before update (layer 0 sample): [0.1489274  0.00817802 0.15875944 0.05045174 0.03261748]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992736 0.009178   0.15775946 0.04945175 0.03361745]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 299\n","Worker 0 gradients (layer 0 sample): [ 0.2311381   3.8137565  -0.75103205 -3.3654408   0.7783952 ]\n","Worker 1 processing data batch 299\n","Worker 1 gradients (layer 0 sample): [ 0.14305352  2.4883616  -0.5407941  -1.9047633   0.54653734]\n","Aggregated gradients (layer 0 sample): [ 0.1870958   3.1510592  -0.64591306 -2.635102    0.6624663 ]\n","Parameters before update (layer 0 sample): [0.14992736 0.009178   0.15775946 0.04945175 0.03361745]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892739 0.00817801 0.15875944 0.05045174 0.03261746]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 300\n","Worker 0 gradients (layer 0 sample): [-0.12046366 -0.162781    0.21533653  1.5483384   0.04086243]\n","Worker 1 processing data batch 300\n","Worker 1 gradients (layer 0 sample): [ 0.2670329  -1.4593453   0.35888103  0.7472792  -0.49047515]\n","Aggregated gradients (layer 0 sample): [ 0.07328461 -0.8110632   0.28710878  1.1478088  -0.22480635]\n","Parameters before update (layer 0 sample): [0.14892739 0.00817801 0.15875944 0.05045174 0.03261746]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792743 0.009178   0.15775946 0.04945175 0.03361744]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 300, Losses: [1.9806204, 2.185492]\n","Worker 0 processing data batch 301\n","Worker 0 gradients (layer 0 sample): [ 0.1369609   1.39081    -0.5700381  -2.039275    0.18994337]\n","Worker 1 processing data batch 301\n","Worker 1 gradients (layer 0 sample): [ 0.15760422  2.1344848  -0.47578937 -1.77195     0.34049016]\n","Aggregated gradients (layer 0 sample): [ 0.14728256  1.7626474  -0.5229137  -1.9056125   0.26521677]\n","Parameters before update (layer 0 sample): [0.14792743 0.009178   0.15775946 0.04945175 0.03361744]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692746 0.00817801 0.15875944 0.05045174 0.03261746]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 302\n","Worker 0 gradients (layer 0 sample): [-0.2191046  -1.039416    0.70113164  2.7363207  -0.15849821]\n","Worker 1 processing data batch 302\n","Worker 1 gradients (layer 0 sample): [-0.39869988 -1.3869579   0.8877496   4.31209    -0.10539432]\n","Aggregated gradients (layer 0 sample): [-0.30890223 -1.213187    0.7944406   3.5242052  -0.13194627]\n","Parameters before update (layer 0 sample): [0.14692746 0.00817801 0.15875944 0.05045174 0.03261746]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792745 0.009178   0.15775946 0.04945175 0.03361743]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 303\n","Worker 0 gradients (layer 0 sample): [ 0.37397343  1.3535159  -0.6227843  -2.7419095  -0.00587587]\n","Worker 1 processing data batch 303\n","Worker 1 gradients (layer 0 sample): [ 0.5289066   0.40250742 -0.48321933 -3.0088024  -0.31467044]\n","Aggregated gradients (layer 0 sample): [ 0.45144     0.87801164 -0.5530018  -2.875356   -0.16027316]\n","Parameters before update (layer 0 sample): [0.14792745 0.009178   0.15775946 0.04945175 0.03361743]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692746 0.00817801 0.15875944 0.05045174 0.03461741]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 304\n","Worker 0 gradients (layer 0 sample): [-0.21391283 -0.02473041  0.743751    2.6838474   0.25145522]\n","Worker 1 processing data batch 304\n","Worker 1 gradients (layer 0 sample): [-0.39021474 -0.32402515  0.44353992  2.833642    0.22255453]\n","Aggregated gradients (layer 0 sample): [-0.3020638  -0.17437778  0.59364545  2.7587447   0.23700488]\n","Parameters before update (layer 0 sample): [0.14692746 0.00817801 0.15875944 0.05045174 0.03461741]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792745 0.00917799 0.15775946 0.04945175 0.03361743]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 305\n","Worker 0 gradients (layer 0 sample): [ 0.29691076  1.9709839  -0.42533427 -2.4485078   0.21479844]\n","Worker 1 processing data batch 305\n","Worker 1 gradients (layer 0 sample): [ 0.2577522   2.1329546  -0.48542947 -2.4671082   0.37687525]\n","Aggregated gradients (layer 0 sample): [ 0.27733147  2.0519693  -0.45538187 -2.457808    0.29583684]\n","Parameters before update (layer 0 sample): [0.14792745 0.00917799 0.15775946 0.04945175 0.03361743]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692746 0.00817799 0.15875944 0.05045174 0.03261744]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 306\n","Worker 0 gradients (layer 0 sample): [-0.34676164 -1.2526155   0.98096514  4.1207247   0.01355308]\n","Worker 1 processing data batch 306\n","Worker 1 gradients (layer 0 sample): [ 0.00811592 -0.86621106  0.47498345  1.6268474  -0.24911231]\n","Aggregated gradients (layer 0 sample): [-0.16932286 -1.0594132   0.7279743   2.873786   -0.11777961]\n","Parameters before update (layer 0 sample): [0.14692746 0.00817799 0.15875944 0.05045174 0.03261744]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792743 0.00917798 0.15775946 0.04945175 0.03361741]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 307\n","Worker 0 gradients (layer 0 sample): [ 0.6747973   1.0949627  -0.9056948  -4.2237124  -0.34651366]\n","Worker 1 processing data batch 307\n","Worker 1 gradients (layer 0 sample): [ 0.19448128  1.1691562  -0.39319927 -1.4933589   0.09102812]\n","Aggregated gradients (layer 0 sample): [ 0.43463928  1.1320595  -0.649447   -2.8585358  -0.12774277]\n","Parameters before update (layer 0 sample): [0.14792743 0.00917798 0.15775946 0.04945175 0.03361741]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692745 0.00817799 0.15875944 0.05045174 0.03461738]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 308\n","Worker 0 gradients (layer 0 sample): [-0.4129544  -0.3211576   0.4085921   2.8107631   0.34288704]\n","Worker 1 processing data batch 308\n","Worker 1 gradients (layer 0 sample): [-0.13682923 -0.2584549   0.12623054  1.2292686  -0.03135791]\n","Aggregated gradients (layer 0 sample): [-0.2748918  -0.28980625  0.26741132  2.0200157   0.15576456]\n","Parameters before update (layer 0 sample): [0.14692745 0.00817799 0.15875944 0.05045174 0.03461738]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792743 0.00917798 0.15775946 0.04945175 0.0336174 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 309\n","Worker 0 gradients (layer 0 sample): [-0.08027386  1.3862324  -0.11796543 -0.14733085  0.31285325]\n","Worker 1 processing data batch 309\n","Worker 1 gradients (layer 0 sample): [ 0.43780428  0.01909542 -0.406775   -1.9014673  -0.32022238]\n","Aggregated gradients (layer 0 sample): [ 0.17876521  0.7026639  -0.26237023 -1.024399   -0.00368457]\n","Parameters before update (layer 0 sample): [0.14792743 0.00917798 0.15775946 0.04945175 0.0336174 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692746 0.00817799 0.15875944 0.05045174 0.03461654]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 310\n","Worker 0 gradients (layer 0 sample): [-0.2159156  -0.9700923   0.6060892   2.8954916  -0.02863632]\n","Worker 1 processing data batch 310\n","Worker 1 gradients (layer 0 sample): [-0.21085037 -0.3768319   0.46632016  2.2592707   0.18887058]\n","Aggregated gradients (layer 0 sample): [-0.21338299 -0.6734621   0.5362047   2.5773811   0.08011713]\n","Parameters before update (layer 0 sample): [0.14692746 0.00817799 0.15875944 0.05045174 0.03461654]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792743 0.00917798 0.15775946 0.04945175 0.03361658]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 310, Losses: [2.017293, 2.1562676]\n","Worker 0 processing data batch 311\n","Worker 0 gradients (layer 0 sample): [ 0.3822708   1.9566741  -0.50293094 -2.8145638   0.06286315]\n","Worker 1 processing data batch 311\n","Worker 1 gradients (layer 0 sample): [ 0.616963    0.98026204 -0.4909849  -3.2398012  -0.16105384]\n","Aggregated gradients (layer 0 sample): [ 0.49961692  1.4684681  -0.4969579  -3.0271826  -0.04909534]\n","Parameters before update (layer 0 sample): [0.14792743 0.00917798 0.15775946 0.04945175 0.03361658]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692745 0.00817798 0.15875944 0.05045174 0.03461651]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 312\n","Worker 0 gradients (layer 0 sample): [-0.60717094 -0.30970424  0.81755716  4.249165    0.4741832 ]\n","Worker 1 processing data batch 312\n","Worker 1 gradients (layer 0 sample): [-0.32646453 -0.07207081  0.20266822  1.9848692   0.20436683]\n","Aggregated gradients (layer 0 sample): [-0.46681774 -0.19088753  0.5101127   3.1170173   0.339275  ]\n","Parameters before update (layer 0 sample): [0.14692745 0.00817798 0.15875944 0.05045174 0.03461651]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792743 0.00917796 0.15775946 0.04945175 0.03361653]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 313\n","Worker 0 gradients (layer 0 sample): [ 0.47028732 -0.11633541 -0.59291375 -2.7850204  -0.5467243 ]\n","Worker 1 processing data batch 313\n","Worker 1 gradients (layer 0 sample): [ 0.3578244   1.2775937  -0.540727   -2.5292516  -0.06807312]\n","Aggregated gradients (layer 0 sample): [ 0.41405585  0.58062917 -0.5668204  -2.657136   -0.30739874]\n","Parameters before update (layer 0 sample): [0.14792743 0.00917796 0.15775946 0.04945175 0.03361653]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692745 0.00817797 0.15875944 0.05045174 0.03461651]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 314\n","Worker 0 gradients (layer 0 sample): [-0.5587928  -0.08438843  0.6099199   3.691938    0.32880375]\n","Worker 1 processing data batch 314\n","Worker 1 gradients (layer 0 sample): [-0.8212037   0.14404191  0.97305477  5.381179    0.7706835 ]\n","Aggregated gradients (layer 0 sample): [-0.68999827  0.02982674  0.79148734  4.536558    0.54974365]\n","Parameters before update (layer 0 sample): [0.14692745 0.00817797 0.15875944 0.05045174 0.03461651]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792743 0.00717809 0.15775946 0.04945175 0.03361652]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 315\n","Worker 0 gradients (layer 0 sample): [ 0.39397028  0.05140975 -0.38646036 -1.9753146  -0.16421364]\n","Worker 1 processing data batch 315\n","Worker 1 gradients (layer 0 sample): [ 0.49131253 -0.72558165 -0.44444135 -2.5791564  -0.3751008 ]\n","Aggregated gradients (layer 0 sample): [ 0.4426414  -0.33708596 -0.41545087 -2.2772355  -0.26965722]\n","Parameters before update (layer 0 sample): [0.14792743 0.00717809 0.15775946 0.04945175 0.03361652]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692745 0.00817807 0.15875944 0.05045174 0.03461651]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 316\n","Worker 0 gradients (layer 0 sample): [-0.24209636  0.142616    0.45371625  2.163575    0.38427445]\n","Worker 1 processing data batch 316\n","Worker 1 gradients (layer 0 sample): [-0.09766507 -0.71652275  0.6726203   2.2115688  -0.03744602]\n","Aggregated gradients (layer 0 sample): [-0.16988072 -0.2869534   0.5631683   2.187572    0.17341422]\n","Parameters before update (layer 0 sample): [0.14692745 0.00817807 0.15875944 0.05045174 0.03461651]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792742 0.00917805 0.15775946 0.04945175 0.03361653]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 317\n","Worker 0 gradients (layer 0 sample): [ 0.5232025   1.4219623  -0.572018   -2.7971187  -0.04225464]\n","Worker 1 processing data batch 317\n","Worker 1 gradients (layer 0 sample): [ 0.44503686  0.05293526 -0.25579894 -2.0420604  -0.3453533 ]\n","Aggregated gradients (layer 0 sample): [ 0.48411965  0.73744875 -0.41390848 -2.4195895  -0.19380397]\n","Parameters before update (layer 0 sample): [0.14792742 0.00917805 0.15775946 0.04945175 0.03361653]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692743 0.00817806 0.15875944 0.05045174 0.03461651]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 318\n","Worker 0 gradients (layer 0 sample): [-0.20879218 -0.4138371   0.45805514  2.4028242  -0.14450674]\n","Worker 1 processing data batch 318\n","Worker 1 gradients (layer 0 sample): [-0.3213437   0.31156003  0.3358873   2.2230904   0.231538  ]\n","Aggregated gradients (layer 0 sample): [-0.26506793 -0.05113854  0.39697123  2.3129573   0.04351563]\n","Parameters before update (layer 0 sample): [0.14692743 0.00817806 0.15875944 0.05045174 0.03461651]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792742 0.00917799 0.15775946 0.04945175 0.03361659]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 319\n","Worker 0 gradients (layer 0 sample): [-0.02472724  1.2601615  -0.39209968 -1.2525696   0.13327166]\n","Worker 1 processing data batch 319\n","Worker 1 gradients (layer 0 sample): [ 0.21624076  0.34531295 -0.285708   -1.8993232  -0.19509915]\n","Aggregated gradients (layer 0 sample): [ 0.09575676  0.80273724 -0.33890384 -1.5759463  -0.03091374]\n","Parameters before update (layer 0 sample): [0.14792742 0.00917799 0.15775946 0.04945175 0.03361659]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692746 0.008178   0.15875944 0.05045174 0.03461648]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 320\n","Worker 0 gradients (layer 0 sample): [-0.10062865 -0.83815163  0.46124345  2.4276357  -0.13519631]\n","Worker 1 processing data batch 320\n","Worker 1 gradients (layer 0 sample): [-0.01553045 -0.13942209  0.17733511  0.94685876 -0.17364234]\n","Aggregated gradients (layer 0 sample): [-0.05807955 -0.48878688  0.31928927  1.6872473  -0.15441933]\n","Parameters before update (layer 0 sample): [0.14692746 0.008178   0.15875944 0.05045174 0.03461648]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1479274  0.00917799 0.15775946 0.04945175 0.03561645]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 320, Losses: [2.0379953, 2.0013719]\n","Worker 0 processing data batch 321\n","Worker 0 gradients (layer 0 sample): [-0.03635371  2.37975    -0.29676902 -0.9480024   0.681292  ]\n","Worker 1 processing data batch 321\n","Worker 1 gradients (layer 0 sample): [ 0.10043764  0.6440224  -0.27633452 -1.1286291   0.20742555]\n","Aggregated gradients (layer 0 sample): [ 0.03204197  1.5118862  -0.28655177 -1.0383158   0.44435877]\n","Parameters before update (layer 0 sample): [0.1479274  0.00917799 0.15775946 0.04945175 0.03561645]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469275  0.008178   0.15875944 0.05045174 0.03461646]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 322\n","Worker 0 gradients (layer 0 sample): [-0.32161215 -0.48601723  0.8159779   3.4255571   0.0995652 ]\n","Worker 1 processing data batch 322\n","Worker 1 gradients (layer 0 sample): [-0.6030935   0.06670547  0.79078114  3.98917     0.49312598]\n","Aggregated gradients (layer 0 sample): [-0.4623528  -0.20965588  0.80337954  3.7073636   0.2963456 ]\n","Parameters before update (layer 0 sample): [0.1469275  0.008178   0.15875944 0.05045174 0.03461646]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1479275  0.00917798 0.15775946 0.04945175 0.03361648]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 323\n","Worker 0 gradients (layer 0 sample): [ 0.3267734   1.4595425  -0.45398957 -2.2099571   0.16039881]\n","Worker 1 processing data batch 323\n","Worker 1 gradients (layer 0 sample): [ 0.4968984   2.7841697  -1.0552342  -4.4330115   0.17437026]\n","Aggregated gradients (layer 0 sample): [ 0.4118359   2.1218562  -0.75461185 -3.3214843   0.16738454]\n","Parameters before update (layer 0 sample): [0.1479275  0.00917798 0.15775946 0.04945175 0.03361648]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469275  0.00817799 0.15875944 0.05045174 0.0326165 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 324\n","Worker 0 gradients (layer 0 sample): [ 0.15487811 -0.9246581   0.05347247  0.43213403 -0.25960714]\n","Worker 1 processing data batch 324\n","Worker 1 gradients (layer 0 sample): [ 0.41646698 -1.6479901   0.3460843  -0.01120698 -0.39422664]\n","Aggregated gradients (layer 0 sample): [ 0.28567255 -1.2863241   0.19977838  0.21046352 -0.32691687]\n","Parameters before update (layer 0 sample): [0.1469275  0.00817799 0.15875944 0.05045174 0.0326165 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592752 0.00917798 0.15775947 0.04945176 0.03361649]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 325\n","Worker 0 gradients (layer 0 sample): [-0.30146292  1.1913843  -0.03643654  0.5225626   0.29464358]\n","Worker 1 processing data batch 325\n","Worker 1 gradients (layer 0 sample): [-0.38456655  1.6657329   0.18643221  1.5965052   0.503371  ]\n","Aggregated gradients (layer 0 sample): [-0.34301472  1.4285586   0.07499784  1.0595338   0.3990073 ]\n","Parameters before update (layer 0 sample): [0.14592752 0.00917798 0.15775947 0.04945176 0.03361649]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469275  0.00817799 0.15675952 0.04845177 0.0326165 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 326\n","Worker 0 gradients (layer 0 sample): [ 0.40344682 -0.73862463 -0.28750467 -1.5979364  -0.44299656]\n","Worker 1 processing data batch 326\n","Worker 1 gradients (layer 0 sample): [ 0.59300625 -0.913178   -0.25646415 -2.0924366  -0.48297727]\n","Aggregated gradients (layer 0 sample): [ 0.49822652 -0.8259013  -0.2719844  -1.8451865  -0.46298692]\n","Parameters before update (layer 0 sample): [0.1469275  0.00817799 0.15675952 0.04845177 0.0326165 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592752 0.00917798 0.1577595  0.04945176 0.03361649]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 327\n","Worker 0 gradients (layer 0 sample): [-0.33746472 -0.6352043   0.91014     3.5263858   0.19006155]\n","Worker 1 processing data batch 327\n","Worker 1 gradients (layer 0 sample): [-0.29207295  0.8842461   0.3563754   1.7708155   0.47557363]\n","Aggregated gradients (layer 0 sample): [-0.31476885  0.1245209   0.6332577   2.6486006   0.33281758]\n","Parameters before update (layer 0 sample): [0.14592752 0.00917798 0.1577595  0.04945176 0.03361649]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469275  0.00817801 0.15675952 0.04845177 0.0326165 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 328\n","Worker 0 gradients (layer 0 sample): [ 0.43524253  0.5412134  -0.37909323 -2.1478412  -0.21637313]\n","Worker 1 processing data batch 328\n","Worker 1 gradients (layer 0 sample): [ 0.35389423  0.13695654 -0.39575553 -2.055923   -0.28689086]\n","Aggregated gradients (layer 0 sample): [ 0.39456838  0.33908498 -0.38742438 -2.101882   -0.251632  ]\n","Parameters before update (layer 0 sample): [0.1469275  0.00817801 0.15675952 0.04845177 0.0326165 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592752 0.00717803 0.1577595  0.04945176 0.03361648]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 329\n","Worker 0 gradients (layer 0 sample): [-0.24375136 -0.52485514  0.39215118  1.9764102   0.04204915]\n","Worker 1 processing data batch 329\n","Worker 1 gradients (layer 0 sample): [-0.04063768 -0.94399315  0.39974776  1.4515455  -0.12566712]\n","Aggregated gradients (layer 0 sample): [-0.14219452 -0.7344241   0.39594948  1.7139778  -0.04180899]\n","Parameters before update (layer 0 sample): [0.14592752 0.00717803 0.1577595  0.04945176 0.03361648]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692749 0.00817801 0.15675952 0.04845177 0.0346164 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 330\n","Worker 0 gradients (layer 0 sample): [ 0.25043494  1.7206848  -0.41564    -2.0680647   0.28247666]\n","Worker 1 processing data batch 330\n","Worker 1 gradients (layer 0 sample): [ 0.08099693  0.48389518 -0.07786685 -0.49391943  0.18250912]\n","Aggregated gradients (layer 0 sample): [ 0.16571593  1.1022899  -0.24675342 -1.280992    0.2324929 ]\n","Parameters before update (layer 0 sample): [0.14692749 0.00817801 0.15675952 0.04845177 0.0346164 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592752 0.00717802 0.1577595  0.04945176 0.03361642]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 330, Losses: [2.2452989, 2.0919342]\n","Worker 0 processing data batch 331\n","Worker 0 gradients (layer 0 sample): [-0.2813219  -0.29155785  0.26509857  1.8170667   0.00965431]\n","Worker 1 processing data batch 331\n","Worker 1 gradients (layer 0 sample): [ 0.10341118 -0.01095934 -0.15230528 -0.3848019  -0.09383143]\n","Aggregated gradients (layer 0 sample): [-0.08895537 -0.15125859  0.05639665  0.7161324  -0.04208856]\n","Parameters before update (layer 0 sample): [0.14592752 0.00717802 0.1577595  0.04945176 0.03361642]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692748 0.008178   0.15675956 0.04845177 0.03461634]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 332\n","Worker 0 gradients (layer 0 sample): [ 0.35837966  0.53319526 -0.421876   -2.1899927   0.14209527]\n","Worker 1 processing data batch 332\n","Worker 1 gradients (layer 0 sample): [ 0.43581867  0.05359246 -0.2851849  -1.7180411  -0.22502378]\n","Aggregated gradients (layer 0 sample): [ 0.39709917  0.29339385 -0.35353047 -1.9540169  -0.04146425]\n","Parameters before update (layer 0 sample): [0.14692748 0.008178   0.15675956 0.04845177 0.03461634]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592749 0.00717801 0.15775955 0.04945176 0.03561626]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 333\n","Worker 0 gradients (layer 0 sample): [-0.40084517 -0.6478143   0.64211726  3.2343342   0.14831188]\n","Worker 1 processing data batch 333\n","Worker 1 gradients (layer 0 sample): [-0.12843657 -1.0858872   0.79611415  2.9402955  -0.10786077]\n","Aggregated gradients (layer 0 sample): [-0.26464087 -0.86685073  0.71911573  3.0873148   0.02022555]\n","Parameters before update (layer 0 sample): [0.14592749 0.00717801 0.15775955 0.04945176 0.03561626]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692748 0.008178   0.15675956 0.04845177 0.03461642]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 334\n","Worker 0 gradients (layer 0 sample): [ 0.24954939  0.23720689 -0.48933697 -1.5795965  -0.102498  ]\n","Worker 1 processing data batch 334\n","Worker 1 gradients (layer 0 sample): [ 0.48643452  1.3601274  -0.6509898  -2.806968   -0.01070995]\n","Aggregated gradients (layer 0 sample): [ 0.36799195  0.7986672  -0.57016337 -2.1932821  -0.05660398]\n","Parameters before update (layer 0 sample): [0.14692748 0.008178   0.15675956 0.04845177 0.03461642]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592749 0.00717801 0.15775955 0.04945176 0.03561636]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 335\n","Worker 0 gradients (layer 0 sample): [-0.7575234   0.01366864  1.1347824   5.2002416   0.8114339 ]\n","Worker 1 processing data batch 335\n","Worker 1 gradients (layer 0 sample): [-0.24480155  0.05775956  0.36999822  1.8072281   0.32851076]\n","Aggregated gradients (layer 0 sample): [-0.50116247  0.0357141   0.7523903   3.5037348   0.56997234]\n","Parameters before update (layer 0 sample): [0.14592749 0.00717801 0.15775955 0.04945176 0.03561636]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692748 0.00617811 0.15675956 0.04845177 0.03461637]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 336\n","Worker 0 gradients (layer 0 sample): [ 0.22687916 -0.2912438  -0.07442437 -0.7305055  -0.26821756]\n","Worker 1 processing data batch 336\n","Worker 1 gradients (layer 0 sample): [ 0.3547353   0.24965699 -0.7181566  -2.941699   -0.15314656]\n","Aggregated gradients (layer 0 sample): [ 0.29080725 -0.0207934  -0.39629048 -1.8361022  -0.21068206]\n","Parameters before update (layer 0 sample): [0.14692748 0.00617811 0.15675956 0.04845177 0.03461637]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592749 0.00717795 0.15775955 0.04945176 0.03561635]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 337\n","Worker 0 gradients (layer 0 sample): [-0.467673    0.16801828  0.51501155  2.7313488   0.71485794]\n","Worker 1 processing data batch 337\n","Worker 1 gradients (layer 0 sample): [-0.20172358  0.0196386   0.16435559  0.9754518   0.11398573]\n","Aggregated gradients (layer 0 sample): [-0.3346983   0.09382844  0.33968356  1.8534002   0.41442183]\n","Parameters before update (layer 0 sample): [0.14592749 0.00717795 0.15775955 0.04945176 0.03561635]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692748 0.00617799 0.15675956 0.04845177 0.03461636]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 338\n","Worker 0 gradients (layer 0 sample): [ 0.32591966 -0.36651224 -0.18516497 -1.0629063  -0.41081774]\n","Worker 1 processing data batch 338\n","Worker 1 gradients (layer 0 sample): [ 0.43378645 -0.4127342  -0.3973903  -1.922374   -0.47358498]\n","Aggregated gradients (layer 0 sample): [ 0.37985307 -0.38962322 -0.29127765 -1.4926401  -0.44220138]\n","Parameters before update (layer 0 sample): [0.14692748 0.00617799 0.15675956 0.04845177 0.03461636]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592749 0.00717798 0.15775955 0.04945176 0.03561635]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 339\n","Worker 0 gradients (layer 0 sample): [-0.49247113 -0.42196405  0.7474434   3.208992    0.18688421]\n","Worker 1 processing data batch 339\n","Worker 1 gradients (layer 0 sample): [-0.3669275  -0.39841834  0.6831244   3.0484414   0.03449888]\n","Aggregated gradients (layer 0 sample): [-0.4296993  -0.41019118  0.7152839   3.1287167   0.11069155]\n","Parameters before update (layer 0 sample): [0.14592749 0.00717798 0.15775955 0.04945176 0.03561635]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692748 0.00817796 0.15675956 0.04845177 0.03461638]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 340\n","Worker 0 gradients (layer 0 sample): [ 0.7206726   0.6187501  -0.49561912 -3.1137848  -0.0886411 ]\n","Worker 1 processing data batch 340\n","Worker 1 gradients (layer 0 sample): [ 0.2256676   0.62467146 -0.34489143 -1.4842336   0.13539822]\n","Aggregated gradients (layer 0 sample): [ 0.4731701   0.6217108  -0.42025527 -2.2990093   0.02337856]\n","Parameters before update (layer 0 sample): [0.14692748 0.00817796 0.15675956 0.04845177 0.03461638]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592749 0.00717797 0.15775955 0.04945176 0.03361653]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 340, Losses: [2.3566864, 2.074806]\n","Worker 0 processing data batch 341\n","Worker 0 gradients (layer 0 sample): [-0.11512885  0.6559385  -0.0039769   0.12652406  0.18729189]\n","Worker 1 processing data batch 341\n","Worker 1 gradients (layer 0 sample): [-0.15569705 -0.00879622  0.07096389  0.786674   -0.00692481]\n","Aggregated gradients (layer 0 sample): [-0.13541295  0.32357115  0.0334935   0.45659906  0.09018354]\n","Parameters before update (layer 0 sample): [0.14592749 0.00717797 0.15775955 0.04945176 0.03361653]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692746 0.00617799 0.15675965 0.04845177 0.03261657]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 342\n","Worker 0 gradients (layer 0 sample): [ 0.15740252 -1.0200928   0.45106447  0.66115737 -0.28956568]\n","Worker 1 processing data batch 342\n","Worker 1 gradients (layer 0 sample): [ 0.10293917 -1.2431209   0.31320512  0.79889405 -0.26932067]\n","Aggregated gradients (layer 0 sample): [ 0.13017084 -1.1316068   0.3821348   0.7300257  -0.27944317]\n","Parameters before update (layer 0 sample): [0.14692746 0.00617799 0.15675965 0.04845177 0.03261657]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592749 0.00717798 0.15575966 0.04745179 0.03361655]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 343\n","Worker 0 gradients (layer 0 sample): [ 0.37873316 -0.5658263  -0.44882673 -1.7354448  -0.5612657 ]\n","Worker 1 processing data batch 343\n","Worker 1 gradients (layer 0 sample): [ 0.9611243  -0.7632386  -0.46403456 -2.9930346  -0.8015425 ]\n","Aggregated gradients (layer 0 sample): [ 0.6699287  -0.6645324  -0.45643064 -2.3642397  -0.6814041 ]\n","Parameters before update (layer 0 sample): [0.14592749 0.00717798 0.15575966 0.04745179 0.03361655]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449275  0.00817797 0.15675965 0.04845178 0.03461654]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 344\n","Worker 0 gradients (layer 0 sample): [-0.4514743   1.7231812   0.4642473   2.172912    0.84780824]\n","Worker 1 processing data batch 344\n","Worker 1 gradients (layer 0 sample): [ 0.06403251  0.3958484  -0.36182314 -1.0932639   0.05512707]\n","Aggregated gradients (layer 0 sample): [-0.1937209   1.0595148   0.05121207  0.539824    0.45146766]\n","Parameters before update (layer 0 sample): [0.1449275  0.00817797 0.15675965 0.04845178 0.03461654]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592747 0.00717798 0.15575972 0.04745179 0.03361655]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 345\n","Worker 0 gradients (layer 0 sample): [-0.01226422 -1.3731253   0.6103845   1.5248404  -0.27516946]\n","Worker 1 processing data batch 345\n","Worker 1 gradients (layer 0 sample): [ 0.4067781  -0.766822   -0.37664753 -1.5331117  -0.5276439 ]\n","Aggregated gradients (layer 0 sample): [ 0.19725694 -1.0699737   0.1168685  -0.00413567 -0.4014067 ]\n","Parameters before update (layer 0 sample): [0.14592747 0.00717798 0.15575972 0.04745179 0.03361655]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449275  0.00817797 0.15475975 0.04845102 0.03461654]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 346\n","Worker 0 gradients (layer 0 sample): [ 0.09653997  0.9586239  -0.42091262 -1.0508863   0.19845355]\n","Worker 1 processing data batch 346\n","Worker 1 gradients (layer 0 sample): [-0.17699912  1.3381145  -0.2888296  -0.30796364  0.462887  ]\n","Aggregated gradients (layer 0 sample): [-0.04022957  1.1483692  -0.3548711  -0.67942494  0.33067027]\n","Parameters before update (layer 0 sample): [0.1449275  0.00817797 0.15475975 0.04845102 0.03461654]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592741 0.00717798 0.15575974 0.04945101 0.03361656]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 347\n","Worker 0 gradients (layer 0 sample): [-7.2337687e-04 -8.8814908e-01  6.5697598e-01  2.0163107e+00\n"," -2.2949558e-01]\n","Worker 1 processing data batch 347\n","Worker 1 gradients (layer 0 sample): [-0.21551569 -0.5373017   0.37297988  1.2016543  -0.0488485 ]\n","Aggregated gradients (layer 0 sample): [-0.10811953 -0.7127254   0.51497793  1.6089826  -0.13917205]\n","Parameters before update (layer 0 sample): [0.14592741 0.00717798 0.15575974 0.04945101 0.03361656]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692737 0.00817797 0.15475975 0.04845102 0.03461653]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 348\n","Worker 0 gradients (layer 0 sample): [ 0.1578749   0.50009084 -0.24335209 -0.6611604   0.12132534]\n","Worker 1 processing data batch 348\n","Worker 1 gradients (layer 0 sample): [ 0.07970662  1.3459526  -0.30654693 -0.8917234   0.42412767]\n","Aggregated gradients (layer 0 sample): [ 0.11879076  0.92302173 -0.2749495  -0.77644193  0.2727265 ]\n","Parameters before update (layer 0 sample): [0.14692737 0.00817797 0.15475975 0.04845102 0.03461653]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1459274  0.00717798 0.15575974 0.04945101 0.03361655]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 349\n","Worker 0 gradients (layer 0 sample): [-0.0573483  -0.7791299   0.6393082   1.8601832  -0.05305614]\n","Worker 1 processing data batch 349\n","Worker 1 gradients (layer 0 sample): [-0.1803362  -0.67430973  0.8231517   2.759081   -0.11756112]\n","Aggregated gradients (layer 0 sample): [-0.11884225 -0.72671986  0.73122996  2.309632   -0.08530863]\n","Parameters before update (layer 0 sample): [0.1459274  0.00717798 0.15575974 0.04945101 0.03361655]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692737 0.00817797 0.15475975 0.04845102 0.0346165 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 350\n","Worker 0 gradients (layer 0 sample): [ 0.77476466  1.6249596  -1.1341023  -4.134194   -0.03726004]\n","Worker 1 processing data batch 350\n","Worker 1 gradients (layer 0 sample): [ 0.44929564  0.58100176 -0.9037828  -3.2166274  -0.2707067 ]\n","Aggregated gradients (layer 0 sample): [ 0.61203015  1.1029806  -1.0189426  -3.6754107  -0.15398338]\n","Parameters before update (layer 0 sample): [0.14692737 0.00817797 0.15475975 0.04845102 0.0346165 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.00717798 0.15575974 0.04945101 0.03561648]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 350, Losses: [2.3984919, 2.002013]\n","Worker 0 processing data batch 351\n","Worker 0 gradients (layer 0 sample): [-0.38139534 -0.4119859   0.59660596  2.5903654   0.41887596]\n","Worker 1 processing data batch 351\n","Worker 1 gradients (layer 0 sample): [-0.15678453 -0.05392084  0.03559718  0.434116    0.32677758]\n","Aggregated gradients (layer 0 sample): [-0.26908994 -0.23295337  0.31610155  1.5122406   0.37282676]\n","Parameters before update (layer 0 sample): [0.14592738 0.00717798 0.15575974 0.04945101 0.03561648]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692737 0.00817796 0.15475975 0.04845102 0.03461649]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 352\n","Worker 0 gradients (layer 0 sample): [ 0.12904221  1.1284939  -0.4012298  -1.3105329   0.08526484]\n","Worker 1 processing data batch 352\n","Worker 1 gradients (layer 0 sample): [ 0.55807936  0.08778708 -0.41709217 -1.835022   -0.40024284]\n","Aggregated gradients (layer 0 sample): [ 0.34356079  0.60814047 -0.40916097 -1.5727775  -0.157489  ]\n","Parameters before update (layer 0 sample): [0.14692737 0.00817796 0.15475975 0.04845102 0.03461649]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.00717797 0.15575974 0.04945101 0.03561646]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 353\n","Worker 0 gradients (layer 0 sample): [-0.06562887 -0.1338338   0.2881017   1.1202041   0.32373238]\n","Worker 1 processing data batch 353\n","Worker 1 gradients (layer 0 sample): [-0.22257364 -0.37448746  0.48445156  2.147646    0.11577845]\n","Aggregated gradients (layer 0 sample): [-0.14410126 -0.25416064  0.38627663  1.633925    0.21975541]\n","Parameters before update (layer 0 sample): [0.14592738 0.00717797 0.15575974 0.04945101 0.03561646]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692736 0.00817795 0.15475975 0.04845102 0.03461649]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 354\n","Worker 0 gradients (layer 0 sample): [ 0.27749217 -0.02447471 -0.18673326 -1.1749887  -0.3057446 ]\n","Worker 1 processing data batch 354\n","Worker 1 gradients (layer 0 sample): [ 0.26830098  0.8518576  -0.5429789  -1.9972064  -0.16435167]\n","Aggregated gradients (layer 0 sample): [ 0.2728966   0.41369146 -0.36485606 -1.5860976  -0.23504813]\n","Parameters before update (layer 0 sample): [0.14692736 0.00817795 0.15475975 0.04845102 0.03461649]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592737 0.00717796 0.15575974 0.04945101 0.03561646]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 355\n","Worker 0 gradients (layer 0 sample): [-0.4793802   0.7838759   0.30590177  2.0526388   0.88648605]\n","Worker 1 processing data batch 355\n","Worker 1 gradients (layer 0 sample): [-0.57640386 -0.08810616  0.9215257   3.97438     0.6380675 ]\n","Aggregated gradients (layer 0 sample): [-0.527892    0.34788486  0.61371374  3.0135093   0.76227677]\n","Parameters before update (layer 0 sample): [0.14592737 0.00717796 0.15575974 0.04945101 0.03561646]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692736 0.00617798 0.15475975 0.04845102 0.03461647]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 356\n","Worker 0 gradients (layer 0 sample): [ 0.94460267 -1.2219112  -0.36981037 -2.6872811  -1.2524575 ]\n","Worker 1 processing data batch 356\n","Worker 1 gradients (layer 0 sample): [ 0.2087675   0.20687881 -0.39421213 -1.2826428  -0.05738893]\n","Aggregated gradients (layer 0 sample): [ 0.5766851  -0.5075162  -0.38201123 -1.984962   -0.6549232 ]\n","Parameters before update (layer 0 sample): [0.14692736 0.00617798 0.15475975 0.04845102 0.03461647]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592737 0.00717797 0.15575974 0.04945101 0.03561646]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 357\n","Worker 0 gradients (layer 0 sample): [-0.43934745  0.14696968  0.6604058   2.8143518   0.7273115 ]\n","Worker 1 processing data batch 357\n","Worker 1 gradients (layer 0 sample): [ 0.03181956 -0.99079347  0.5881932   1.4355642  -0.4232755 ]\n","Aggregated gradients (layer 0 sample): [-0.20376395 -0.4219119   0.6242995   2.124958    0.152018  ]\n","Parameters before update (layer 0 sample): [0.14592737 0.00717797 0.15575974 0.04945101 0.03561646]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692734 0.00817795 0.15475975 0.04845102 0.03461649]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 358\n","Worker 0 gradients (layer 0 sample): [ 0.31493688  1.3255588  -0.81794465 -2.6507638   0.11964107]\n","Worker 1 processing data batch 358\n","Worker 1 gradients (layer 0 sample): [-0.19940132  1.7840215  -0.3712327  -0.5300778   0.7711193 ]\n","Aggregated gradients (layer 0 sample): [ 0.05776778  1.5547901  -0.59458864 -1.5904207   0.44538018]\n","Parameters before update (layer 0 sample): [0.14692734 0.00817795 0.15475975 0.04845102 0.03461649]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1459274  0.00717796 0.15575974 0.04945101 0.0336165 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 359\n","Worker 0 gradients (layer 0 sample): [ 0.5546312  -1.3802547   0.04581275 -1.3525404  -0.7152158 ]\n","Worker 1 processing data batch 359\n","Worker 1 gradients (layer 0 sample): [-0.19722717 -0.8009126   0.49512175  1.9212363  -0.09197602]\n","Aggregated gradients (layer 0 sample): [ 0.178702   -1.0905837   0.27046725  0.28434795 -0.40359592]\n","Parameters before update (layer 0 sample): [0.1459274  0.00717796 0.15575974 0.04945101 0.0336165 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492743 0.00817795 0.15475975 0.04845103 0.03461649]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 360\n","Worker 0 gradients (layer 0 sample): [ 0.19998738  1.0119354  -0.40910262 -1.7090737  -0.18179442]\n","Worker 1 processing data batch 360\n","Worker 1 gradients (layer 0 sample): [-0.10407428  0.910956    0.01167575 -0.05960144  0.6136036 ]\n","Aggregated gradients (layer 0 sample): [ 0.04795655  0.9614457  -0.19871344 -0.88433754  0.2159046 ]\n","Parameters before update (layer 0 sample): [0.14492743 0.00817795 0.15475975 0.04845103 0.03461649]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1439275  0.00717796 0.15575972 0.04945102 0.03361651]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 360, Losses: [2.1412427, 2.0689034]\n","Worker 0 processing data batch 361\n","Worker 0 gradients (layer 0 sample): [ 0.03964718 -0.3316018   0.11962993  0.073679   -0.32366312]\n","Worker 1 processing data batch 361\n","Worker 1 gradients (layer 0 sample): [ 0.12275593 -0.49320295  0.22745506  0.30674195 -0.23841515]\n","Aggregated gradients (layer 0 sample): [ 0.08120155 -0.4124024   0.1735425   0.19021048 -0.28103912]\n","Parameters before update (layer 0 sample): [0.1439275  0.00717796 0.15575972 0.04945102 0.03361651]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292754 0.00817795 0.15475975 0.04845104 0.03461649]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 362\n","Worker 0 gradients (layer 0 sample): [-0.2129841   1.0644308  -0.22999261 -0.29336977  0.42840424]\n","Worker 1 processing data batch 362\n","Worker 1 gradients (layer 0 sample): [-0.20142701  1.3281116  -0.2545656  -0.6012233   0.5759841 ]\n","Aggregated gradients (layer 0 sample): [-0.20720556  1.1962712  -0.24227911 -0.44729653  0.50219417]\n","Parameters before update (layer 0 sample): [0.14292754 0.00817795 0.15475975 0.04845104 0.03461649]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392751 0.00717796 0.15575974 0.04945103 0.03361651]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 363\n","Worker 0 gradients (layer 0 sample): [-0.4227941  -0.8574518   1.13656     4.045762    0.25637373]\n","Worker 1 processing data batch 363\n","Worker 1 gradients (layer 0 sample): [-0.04938874 -1.5530018   0.82458967  2.4071915  -0.28765756]\n","Aggregated gradients (layer 0 sample): [-0.23609142 -1.2052268   0.98057485  3.2264767  -0.01564191]\n","Parameters before update (layer 0 sample): [0.14392751 0.00717796 0.15575974 0.04945103 0.03361651]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449275  0.00817795 0.15475975 0.04845104 0.0346163 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 364\n","Worker 0 gradients (layer 0 sample): [ 0.20122977  0.9284723  -0.59992504 -1.9454154   0.08917759]\n","Worker 1 processing data batch 364\n","Worker 1 gradients (layer 0 sample): [-0.2366491  2.298376  -0.548352  -1.1732426  0.8519931]\n","Aggregated gradients (layer 0 sample): [-0.01770967  1.6134242  -0.5741385  -1.559329    0.47058535]\n","Parameters before update (layer 0 sample): [0.1449275  0.00817795 0.15475975 0.04845104 0.0346163 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592731 0.00717796 0.15575974 0.04945103 0.03361631]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 365\n","Worker 0 gradients (layer 0 sample): [ 0.41076028 -1.0599376   0.07180238 -0.5769347  -0.5411789 ]\n","Worker 1 processing data batch 365\n","Worker 1 gradients (layer 0 sample): [ 0.2982821 -1.6019635  0.8085378  1.1655226 -0.4973396]\n","Aggregated gradients (layer 0 sample): [ 0.3545212  -1.3309505   0.44017008  0.29429394 -0.5192592 ]\n","Parameters before update (layer 0 sample): [0.14592731 0.00717796 0.15575974 0.04945103 0.03361631]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492732 0.00817795 0.15475975 0.04845104 0.0346163 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 366\n","Worker 0 gradients (layer 0 sample): [ 0.6561578  -0.63902664 -0.5604726  -2.4116952  -0.7963385 ]\n","Worker 1 processing data batch 366\n","Worker 1 gradients (layer 0 sample): [ 0.1143847   2.3009129  -0.7783338  -2.3005013   0.44826365]\n","Aggregated gradients (layer 0 sample): [ 0.38527125  0.8309431  -0.6694032  -2.3560982  -0.17403743]\n","Parameters before update (layer 0 sample): [0.14492732 0.00817795 0.15475975 0.04845104 0.0346163 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392734 0.00717796 0.15575974 0.04945103 0.03561627]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 367\n","Worker 0 gradients (layer 0 sample): [ 0.04592358  0.03451633  0.2441633   0.11769617 -0.29957575]\n","Worker 1 processing data batch 367\n","Worker 1 gradients (layer 0 sample): [-0.10298495 -0.85119355  0.72173375  2.2763214  -0.21140766]\n","Aggregated gradients (layer 0 sample): [-0.02853068 -0.4083386   0.48294854  1.1970088  -0.2554917 ]\n","Parameters before update (layer 0 sample): [0.14392734 0.00717796 0.15575974 0.04945103 0.03561627]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492722 0.00817794 0.15475975 0.04845104 0.03661625]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 368\n","Worker 0 gradients (layer 0 sample): [-0.19523518  1.0371066  -0.34396583 -0.47536105  0.6146056 ]\n","Worker 1 processing data batch 368\n","Worker 1 gradients (layer 0 sample): [-0.02629188  2.554669   -0.42143932 -1.4857875   0.85138106]\n","Aggregated gradients (layer 0 sample): [-0.11076353  1.7958877  -0.3827026  -0.98057425  0.73299336]\n","Parameters before update (layer 0 sample): [0.14492722 0.00817794 0.15475975 0.04845104 0.03661625]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592719 0.00717795 0.15575974 0.04945103 0.03561626]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 369\n","Worker 0 gradients (layer 0 sample): [ 0.04293781 -0.99507743  0.7230907   1.8060733  -0.2089779 ]\n","Worker 1 processing data batch 369\n","Worker 1 gradients (layer 0 sample): [-0.24905545 -1.4155604   1.1427943   3.5758886  -0.2896327 ]\n","Aggregated gradients (layer 0 sample): [-0.10305882 -1.2053189   0.9329425   2.690981   -0.24930531]\n","Parameters before update (layer 0 sample): [0.14592719 0.00717795 0.15575974 0.04945103 0.03561626]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692715 0.00817794 0.15475975 0.04845104 0.03661624]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 370\n","Worker 0 gradients (layer 0 sample): [-0.02542163  1.7871149  -0.4335277  -0.9927044   0.7081877 ]\n","Worker 1 processing data batch 370\n","Worker 1 gradients (layer 0 sample): [-0.0119582   1.0645794  -0.41440117 -1.1070042   0.30361468]\n","Aggregated gradients (layer 0 sample): [-0.01868992  1.425847   -0.42396444 -1.0498543   0.5059012 ]\n","Parameters before update (layer 0 sample): [0.14692715 0.00817794 0.15475975 0.04845104 0.03661624]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792697 0.00717795 0.15575974 0.04945103 0.03561625]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 370, Losses: [2.080153, 1.8699342]\n","Worker 0 processing data batch 371\n","Worker 0 gradients (layer 0 sample): [-0.29839885 -0.24556749  0.57884467  2.5399208   0.3442119 ]\n","Worker 1 processing data batch 371\n","Worker 1 gradients (layer 0 sample): [-0.04195972 -0.34558985  0.02605493  0.4444823  -0.18843655]\n","Aggregated gradients (layer 0 sample): [-0.17017928 -0.29557866  0.3024498   1.4922016   0.07788768]\n","Parameters before update (layer 0 sample): [0.14792697 0.00717795 0.15575974 0.04945103 0.03561625]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892694 0.00817793 0.15475975 0.04845104 0.0346163 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 372\n","Worker 0 gradients (layer 0 sample): [-0.05924715 -0.71182984  0.021962    0.5734138  -0.24849728]\n","Worker 1 processing data batch 372\n","Worker 1 gradients (layer 0 sample): [ 0.2044704  -0.09176889 -0.12969078 -0.9971414  -0.37493378]\n","Aggregated gradients (layer 0 sample): [ 0.07261162 -0.40179938 -0.05386439 -0.21186382 -0.31171554]\n","Parameters before update (layer 0 sample): [0.14892694 0.00817793 0.15475975 0.04845104 0.0346163 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.147927   0.00917792 0.15575968 0.04945102 0.03561628]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 373\n","Worker 0 gradients (layer 0 sample): [-0.06237327  3.0380611  -0.48424208 -1.4023591   0.86449414]\n","Worker 1 processing data batch 373\n","Worker 1 gradients (layer 0 sample): [ 0.046928    4.404195   -0.86201406 -2.7476716   0.9837918 ]\n","Aggregated gradients (layer 0 sample): [-0.00772263  3.721128   -0.67312807 -2.0750153   0.92414296]\n","Parameters before update (layer 0 sample): [0.147927   0.00917792 0.15575968 0.04945102 0.03561628]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892659 0.00817793 0.15675966 0.05045101 0.03461629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 374\n","Worker 0 gradients (layer 0 sample): [-0.1970661  -0.41353944  0.7753632   2.6620326   0.15694402]\n","Worker 1 processing data batch 374\n","Worker 1 gradients (layer 0 sample): [-0.33093017 -0.97698975  1.1949067   4.6387987   0.37097636]\n","Aggregated gradients (layer 0 sample): [-0.26399815 -0.6952646   0.98513496  3.6504157   0.26396018]\n","Parameters before update (layer 0 sample): [0.14892659 0.00817793 0.15675966 0.05045101 0.03461629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992657 0.00917792 0.15575968 0.04945102 0.03361631]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 375\n","Worker 0 gradients (layer 0 sample): [ 0.09930447  1.5272112  -0.32730782 -1.4558431   0.07629289]\n","Worker 1 processing data batch 375\n","Worker 1 gradients (layer 0 sample): [ 0.15907507  1.3280349  -0.6067438  -1.4655383   0.02224873]\n","Aggregated gradients (layer 0 sample): [ 0.12918976  1.427623   -0.46702582 -1.4606907   0.04927081]\n","Parameters before update (layer 0 sample): [0.14992657 0.00917792 0.15575968 0.04945102 0.03361631]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489266  0.00817792 0.15675966 0.05045101 0.03261638]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 376\n","Worker 0 gradients (layer 0 sample): [-0.730639   -0.17354998  1.1293392   5.174443    0.2465347 ]\n","Worker 1 processing data batch 376\n","Worker 1 gradients (layer 0 sample): [-0.5381746  -2.0969443   1.2887181   5.779359    0.02710684]\n","Aggregated gradients (layer 0 sample): [-0.6344068  -1.1352471   1.2090287   5.476901    0.13682078]\n","Parameters before update (layer 0 sample): [0.1489266  0.00817792 0.15675966 0.05045101 0.03261638]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992659 0.00917792 0.15575968 0.04945102 0.03161642]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 377\n","Worker 0 gradients (layer 0 sample): [ 0.36556873  2.850674   -0.7946789  -3.3569021  -0.06496465]\n","Worker 1 processing data batch 377\n","Worker 1 gradients (layer 0 sample): [ 0.5019736   2.35268    -0.8618483  -3.614304   -0.16042162]\n","Aggregated gradients (layer 0 sample): [ 0.4337712   2.601677   -0.82826364 -3.485603   -0.11269314]\n","Parameters before update (layer 0 sample): [0.14992659 0.00917792 0.15575968 0.04945102 0.03161642]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489266  0.00817792 0.15675966 0.05045101 0.03261638]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 378\n","Worker 0 gradients (layer 0 sample): [-0.03797638 -1.8777809   0.89564776  3.2228842  -0.09895529]\n","Worker 1 processing data batch 378\n","Worker 1 gradients (layer 0 sample): [-0.4434669  -1.3323138   0.97917414  4.622227    0.07178204]\n","Aggregated gradients (layer 0 sample): [-0.24072164 -1.6050473   0.93741095  3.9225557  -0.01358663]\n","Parameters before update (layer 0 sample): [0.1489266  0.00817792 0.15675966 0.05045101 0.03261638]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992659 0.00917792 0.15575968 0.04945102 0.03361614]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 379\n","Worker 0 gradients (layer 0 sample): [ 0.46961045  1.5816628  -0.6268472  -3.3485255  -0.2117475 ]\n","Worker 1 processing data batch 379\n","Worker 1 gradients (layer 0 sample): [ 0.097074    2.2398722  -0.42087466 -1.7883928   0.25489363]\n","Aggregated gradients (layer 0 sample): [ 0.28334224  1.9107676  -0.52386093 -2.568459    0.02157307]\n","Parameters before update (layer 0 sample): [0.14992659 0.00917792 0.15575968 0.04945102 0.03361614]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489266  0.00817792 0.15675966 0.05045101 0.03261629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 380\n","Worker 0 gradients (layer 0 sample): [ 0.17870897 -1.2822185   0.42315394  0.9738128  -0.29297718]\n","Worker 1 processing data batch 380\n","Worker 1 gradients (layer 0 sample): [-0.3805492  -1.4742236   1.2660997   5.2229495   0.12183005]\n","Aggregated gradients (layer 0 sample): [-0.10092011 -1.378221    0.8446268   3.098381   -0.08557357]\n","Parameters before update (layer 0 sample): [0.1489266  0.00817792 0.15675966 0.05045101 0.03261629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992656 0.00917791 0.15575968 0.04945102 0.03361625]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 380, Losses: [1.9379513, 2.103381]\n","Worker 0 processing data batch 381\n","Worker 0 gradients (layer 0 sample): [-0.04278505  2.44067    -0.3249708  -0.66873384  0.23109499]\n","Worker 1 processing data batch 381\n","Worker 1 gradients (layer 0 sample): [ 0.27699056  2.524999   -0.5800199  -2.6808465  -0.058126  ]\n","Aggregated gradients (layer 0 sample): [ 0.11710276  2.4828343  -0.45249534 -1.6747901   0.08648449]\n","Parameters before update (layer 0 sample): [0.14992656 0.00917791 0.15575968 0.04945102 0.03361625]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892659 0.00817792 0.15675966 0.05045101 0.03261629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 382\n","Worker 0 gradients (layer 0 sample): [-0.22369042 -1.5211724   1.4151359   4.818614    0.01916188]\n","Worker 1 processing data batch 382\n","Worker 1 gradients (layer 0 sample): [-0.3885097  -0.40985316  0.9405155   3.4208047   0.11114666]\n","Aggregated gradients (layer 0 sample): [-0.30610007 -0.96551275  1.1778257   4.1197095   0.06515427]\n","Parameters before update (layer 0 sample): [0.14892659 0.00817792 0.15675966 0.05045101 0.03261629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992657 0.00917791 0.15575968 0.04945102 0.03161635]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 383\n","Worker 0 gradients (layer 0 sample): [ 0.60082185  0.17667316 -0.48078293 -2.9954507  -0.33038077]\n","Worker 1 processing data batch 383\n","Worker 1 gradients (layer 0 sample): [ 1.1112448  -0.44605717 -0.7258214  -4.9551034  -0.58012015]\n","Aggregated gradients (layer 0 sample): [ 0.8560333  -0.13469201 -0.6033021  -3.975277   -0.45525044]\n","Parameters before update (layer 0 sample): [0.14992657 0.00917791 0.15575968 0.04945102 0.03161635]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892659 0.01017788 0.15675966 0.05045101 0.03261634]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 384\n","Worker 0 gradients (layer 0 sample): [-0.3589098 -1.4679654  1.3515469  4.5781426  0.2005234]\n","Worker 1 processing data batch 384\n","Worker 1 gradients (layer 0 sample): [-0.3769459   0.6114644   0.583565    2.3570175   0.29396677]\n","Aggregated gradients (layer 0 sample): [-0.36792785 -0.4282505   0.96755594  3.46758     0.24724509]\n","Parameters before update (layer 0 sample): [0.14892659 0.01017788 0.15675966 0.05045101 0.03261634]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992657 0.01117787 0.15575968 0.04945102 0.03161636]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 385\n","Worker 0 gradients (layer 0 sample): [ 0.3089167   2.6045985  -0.7949772  -3.3911307  -0.12022649]\n","Worker 1 processing data batch 385\n","Worker 1 gradients (layer 0 sample): [ 2.0841317e-01  2.0369480e+00 -3.7755769e-01 -1.6889472e+00\n","  6.7336857e-04]\n","Aggregated gradients (layer 0 sample): [ 0.25866494  2.3207731  -0.5862675  -2.540039   -0.05977656]\n","Parameters before update (layer 0 sample): [0.14992657 0.01117787 0.15575968 0.04945102 0.03161636]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892659 0.01017788 0.15675966 0.05045101 0.03261629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 386\n","Worker 0 gradients (layer 0 sample): [-0.50942105 -1.2847708   0.70553124  3.8317633   0.02225943]\n","Worker 1 processing data batch 386\n","Worker 1 gradients (layer 0 sample): [-0.09774029 -0.90322256  0.53078985  2.3660383  -0.0607146 ]\n","Aggregated gradients (layer 0 sample): [-0.30358067 -1.0939968   0.61816055  3.0989008  -0.01922759]\n","Parameters before update (layer 0 sample): [0.14892659 0.01017788 0.15675966 0.05045101 0.03261629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992657 0.01117787 0.15575968 0.04945102 0.03361613]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 387\n","Worker 0 gradients (layer 0 sample): [-0.02236462  2.5688632  -0.39942294 -1.3884988   0.10564219]\n","Worker 1 processing data batch 387\n","Worker 1 gradients (layer 0 sample): [ 0.01953679  3.4922802  -0.8208561  -2.5298893   0.02457105]\n","Aggregated gradients (layer 0 sample): [-1.4139153e-03  3.0305717e+00 -6.1013949e-01 -1.9591941e+00\n","  6.5106615e-02]\n","Parameters before update (layer 0 sample): [0.14992657 0.01117787 0.15575968 0.04945102 0.03361613]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092434 0.01017787 0.15675966 0.05045101 0.03261618]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 388\n","Worker 0 gradients (layer 0 sample): [ 0.10859817 -1.6214753   0.07431429  0.5446625  -0.22235562]\n","Worker 1 processing data batch 388\n","Worker 1 gradients (layer 0 sample): [ 0.23319367 -2.9690833   0.65765846  1.4690518  -0.26375017]\n","Aggregated gradients (layer 0 sample): [ 0.17089592 -2.2952793   0.36598638  1.0068572  -0.2430529 ]\n","Parameters before update (layer 0 sample): [0.15092434 0.01017787 0.15675966 0.05045101 0.03261618]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992437 0.01117787 0.15575968 0.04945102 0.03361616]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 389\n","Worker 0 gradients (layer 0 sample): [ 3.4462434e-01  2.9129305e+00 -6.6810346e-01 -2.8582768e+00\n"," -1.8596649e-03]\n","Worker 1 processing data batch 389\n","Worker 1 gradients (layer 0 sample): [-0.10844067  1.0114225  -0.2490653  -0.26606888  0.04457104]\n","Aggregated gradients (layer 0 sample): [ 0.11809184  1.9621766  -0.45858437 -1.5621729   0.02135569]\n","Parameters before update (layer 0 sample): [0.14992437 0.01117787 0.15575968 0.04945102 0.03361616]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489244  0.01017787 0.15675966 0.05045101 0.03261631]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 390\n","Worker 0 gradients (layer 0 sample): [-0.27419403  1.1549588   0.33715338  2.0452678   0.22851557]\n","Worker 1 processing data batch 390\n","Worker 1 gradients (layer 0 sample): [-0.7225597  -0.30594718  0.92925763  4.9876623   0.3641689 ]\n","Aggregated gradients (layer 0 sample): [-0.49837685  0.42450583  0.63320553  3.5164652   0.29634225]\n","Parameters before update (layer 0 sample): [0.1489244  0.01017787 0.15675966 0.05045101 0.03261631]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992438 0.00917789 0.15575968 0.04945102 0.03161633]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 390, Losses: [2.065114, 1.8523078]\n","Worker 0 processing data batch 391\n","Worker 0 gradients (layer 0 sample): [ 0.7533947  -0.02948701 -0.37018687 -3.0305164  -0.44144183]\n","Worker 1 processing data batch 391\n","Worker 1 gradients (layer 0 sample): [ 0.8597723  -0.35262343 -0.67166686 -3.4780483  -0.56103176]\n","Aggregated gradients (layer 0 sample): [ 0.8065835  -0.19105522 -0.52092683 -3.2542825  -0.5012368 ]\n","Parameters before update (layer 0 sample): [0.14992438 0.00917789 0.15575968 0.04945102 0.03161633]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489244  0.01017787 0.15675966 0.05045101 0.03261632]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 392\n","Worker 0 gradients (layer 0 sample): [-0.751878    0.29253477  1.1468503   5.4049177   0.4729864 ]\n","Worker 1 processing data batch 392\n","Worker 1 gradients (layer 0 sample): [-0.5916704  -0.44644916  0.9959579   5.155423    0.25295934]\n","Aggregated gradients (layer 0 sample): [-0.6717742  -0.0769572   1.0714041   5.2801704   0.36297286]\n","Parameters before update (layer 0 sample): [0.1489244  0.01017787 0.15675966 0.05045101 0.03261632]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992438 0.01117782 0.15575968 0.04945102 0.03161633]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 393\n","Worker 0 gradients (layer 0 sample): [ 0.9934791  -0.88519496 -0.47945666 -3.5909204  -0.4547857 ]\n","Worker 1 processing data batch 393\n","Worker 1 gradients (layer 0 sample): [ 0.79628474 -0.5513459  -0.6517576  -3.5266385  -0.4880919 ]\n","Aggregated gradients (layer 0 sample): [ 0.8948819  -0.7182704  -0.56560713 -3.5587795  -0.4714388 ]\n","Parameters before update (layer 0 sample): [0.14992438 0.01117782 0.15575968 0.04945102 0.03161633]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489244  0.01217781 0.15675966 0.05045101 0.03261632]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 394\n","Worker 0 gradients (layer 0 sample): [-0.50385654  1.6363647   0.3665079   2.1727967   0.1607519 ]\n","Worker 1 processing data batch 394\n","Worker 1 gradients (layer 0 sample): [-0.6878263   2.799241    0.45188844  2.9016473   0.38287085]\n","Aggregated gradients (layer 0 sample): [-0.5958414   2.217803    0.40919816  2.537222    0.27181137]\n","Parameters before update (layer 0 sample): [0.1489244  0.01217781 0.15675966 0.05045101 0.03261632]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992438 0.01117782 0.15575968 0.04945102 0.03161634]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 395\n","Worker 0 gradients (layer 0 sample): [ 0.6573914  -0.8495017  -0.47556174 -2.5235186  -0.44850272]\n","Worker 1 processing data batch 395\n","Worker 1 gradients (layer 0 sample): [ 0.52939343 -2.6228464   0.1535016  -0.7682237  -0.5503335 ]\n","Aggregated gradients (layer 0 sample): [ 0.59339243 -1.7361741  -0.16103007 -1.6458712  -0.4994181 ]\n","Parameters before update (layer 0 sample): [0.14992438 0.01117782 0.15575968 0.04945102 0.03161634]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489244  0.01217781 0.15675965 0.05045101 0.03261632]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 396\n","Worker 0 gradients (layer 0 sample): [-0.40440142  1.5129515   0.51554847  2.196783    0.3716085 ]\n","Worker 1 processing data batch 396\n","Worker 1 gradients (layer 0 sample): [-0.6587186   1.5570374   0.61573476  3.649256    0.46991235]\n","Aggregated gradients (layer 0 sample): [-0.53156     1.5349944   0.56564164  2.9230194   0.42076042]\n","Parameters before update (layer 0 sample): [0.1489244  0.01217781 0.15675965 0.05045101 0.03261632]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992438 0.01117782 0.15575966 0.04945102 0.03161634]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 397\n","Worker 0 gradients (layer 0 sample): [ 0.54877615 -1.2454807  -0.10346891 -1.9501116  -0.47292042]\n","Worker 1 processing data batch 397\n","Worker 1 gradients (layer 0 sample): [ 0.29196978 -1.1164315   0.05720359 -0.31823757 -0.23502782]\n","Aggregated gradients (layer 0 sample): [ 0.42037296 -1.1809561  -0.02313266 -1.1341746  -0.3539741 ]\n","Parameters before update (layer 0 sample): [0.14992438 0.01117782 0.15575966 0.04945102 0.03161634]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489244  0.01217781 0.15675952 0.05045101 0.03261632]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 398\n","Worker 0 gradients (layer 0 sample): [-0.32578155  3.2938871  -0.12404282  0.17301908  0.37193415]\n","Worker 1 processing data batch 398\n","Worker 1 gradients (layer 0 sample): [-0.65756345  3.1053505   0.26013505  2.4562604   0.4382467 ]\n","Aggregated gradients (layer 0 sample): [-0.49167252  3.1996188   0.06804612  1.3146398   0.40509042]\n","Parameters before update (layer 0 sample): [0.1489244  0.01217781 0.15675952 0.05045101 0.03261632]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992438 0.01117781 0.15575957 0.04945102 0.03161633]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 399\n","Worker 0 gradients (layer 0 sample): [ 1.09134    -2.1595793  -0.49129644 -3.5234797  -0.6626989 ]\n","Worker 1 processing data batch 399\n","Worker 1 gradients (layer 0 sample): [ 0.5208339  -0.8420211  -0.19526243 -2.1092305  -0.3568539 ]\n","Aggregated gradients (layer 0 sample): [ 0.8060869  -1.5008001  -0.34327942 -2.8163552  -0.5097764 ]\n","Parameters before update (layer 0 sample): [0.14992438 0.01117781 0.15575957 0.04945102 0.03161633]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489244  0.0121778  0.15675956 0.05045101 0.03261632]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 400\n","Worker 0 gradients (layer 0 sample): [-0.6144339   1.1826783   0.5938393   3.535463    0.28692722]\n","Worker 1 processing data batch 400\n","Worker 1 gradients (layer 0 sample): [-0.24403958 -0.09786645  0.3343407   1.5588361   0.08770943]\n","Aggregated gradients (layer 0 sample): [-0.42923674  0.54240596  0.46409     2.5471497   0.18731833]\n","Parameters before update (layer 0 sample): [0.1489244  0.0121778  0.15675956 0.05045101 0.03261632]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992438 0.01117782 0.15575957 0.04945102 0.03161635]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 400, Losses: [1.8965027, 1.8385801]\n","Worker 0 processing data batch 401\n","Worker 0 gradients (layer 0 sample): [ 0.7209141  -0.3794745  -0.698427   -3.6493878  -0.49349836]\n","Worker 1 processing data batch 401\n","Worker 1 gradients (layer 0 sample): [ 0.17004769  1.7941083  -0.5426614  -1.6931579  -0.03093288]\n","Aggregated gradients (layer 0 sample): [ 0.4454809  0.7073169 -0.6205442 -2.6712728 -0.2622156]\n","Parameters before update (layer 0 sample): [0.14992438 0.01117782 0.15575957 0.04945102 0.03161635]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489244  0.01017783 0.15675956 0.05045101 0.03261632]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 402\n","Worker 0 gradients (layer 0 sample): [ 0.05066741 -1.813794    0.70640177  2.549814   -0.28440818]\n","Worker 1 processing data batch 402\n","Worker 1 gradients (layer 0 sample): [-0.08309326 -2.0226874   1.1136698   3.6724463  -0.11049829]\n","Aggregated gradients (layer 0 sample): [-0.01621292 -1.9182408   0.9100357   3.1111302  -0.19745323]\n","Parameters before update (layer 0 sample): [0.1489244  0.01017783 0.15675956 0.05045101 0.03261632]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992419 0.01117782 0.15575957 0.04945102 0.0336163 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 403\n","Worker 0 gradients (layer 0 sample): [-0.05253027  2.6606252  -0.4697249  -1.2757833   0.24465647]\n","Worker 1 processing data batch 403\n","Worker 1 gradients (layer 0 sample): [ 0.5503602 -0.7401086 -0.5920867 -2.4479175 -0.6327892]\n","Aggregated gradients (layer 0 sample): [ 0.24891496  0.9602583  -0.5309058  -1.8618504  -0.19406636]\n","Parameters before update (layer 0 sample): [0.14992419 0.01117782 0.15575957 0.04945102 0.0336163 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489242  0.01017783 0.15675956 0.05045101 0.03461628]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 404\n","Worker 0 gradients (layer 0 sample): [-0.44413787  0.68928874  0.69160753  3.5073214   0.753445  ]\n","Worker 1 processing data batch 404\n","Worker 1 gradients (layer 0 sample): [-0.6362023   1.6874975   0.41899884  3.4395008   1.1446397 ]\n","Aggregated gradients (layer 0 sample): [-0.5401701  1.1883931  0.5553032  3.473411   0.9490424]\n","Parameters before update (layer 0 sample): [0.1489242  0.01017783 0.15675956 0.05045101 0.03461628]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992419 0.00917784 0.15575957 0.04945102 0.03361629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 405\n","Worker 0 gradients (layer 0 sample): [ 0.85156906 -1.9452641  -0.3936193  -2.9870114  -1.1138067 ]\n","Worker 1 processing data batch 405\n","Worker 1 gradients (layer 0 sample): [ 0.45537674 -1.2157999   0.01146744 -1.0032953  -0.6185756 ]\n","Aggregated gradients (layer 0 sample): [ 0.6534729  -1.5805321  -0.19107592 -1.9951534  -0.86619115]\n","Parameters before update (layer 0 sample): [0.14992419 0.00917784 0.15575957 0.04945102 0.03361629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489242  0.01017783 0.15675955 0.05045101 0.03461628]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 406\n","Worker 0 gradients (layer 0 sample): [-0.2619029   0.95401895  0.49730384  1.974444    0.59304285]\n","Worker 1 processing data batch 406\n","Worker 1 gradients (layer 0 sample): [-0.12413004  2.1542842   0.12374454  0.492224    0.84358644]\n","Aggregated gradients (layer 0 sample): [-0.19301647  1.5541515   0.3105242   1.2333341   0.71831465]\n","Parameters before update (layer 0 sample): [0.1489242  0.01017783 0.15675955 0.05045101 0.03461628]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992417 0.00917784 0.15575956 0.04945102 0.03361629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 407\n","Worker 0 gradients (layer 0 sample): [ 0.75382805 -1.4113913  -0.19635549 -2.5915303  -0.81179416]\n","Worker 1 processing data batch 407\n","Worker 1 gradients (layer 0 sample): [ 0.8176794  -2.351861    0.18753591 -1.4282526  -0.93611014]\n","Aggregated gradients (layer 0 sample): [ 0.7857537  -1.8816261  -0.00440979 -2.0098915  -0.87395215]\n","Parameters before update (layer 0 sample): [0.14992417 0.00917784 0.15575956 0.04945102 0.03361629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892419 0.01017783 0.15675883 0.05045101 0.03461628]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 408\n","Worker 0 gradients (layer 0 sample): [ 0.04216087 -0.7328904   0.27632397  0.29008403 -0.58485234]\n","Worker 1 processing data batch 408\n","Worker 1 gradients (layer 0 sample): [-0.468652    2.5105166   0.28435892  1.9980576   0.907878  ]\n","Aggregated gradients (layer 0 sample): [-0.21324557  0.88881314  0.28034145  1.1440709   0.16151282]\n","Parameters before update (layer 0 sample): [0.14892419 0.01017783 0.15675883 0.05045101 0.03461628]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992416 0.00917784 0.15575884 0.04945102 0.03361631]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 409\n","Worker 0 gradients (layer 0 sample): [ 0.1814827  -0.13896242 -0.19706148 -1.170044   -0.22944245]\n","Worker 1 processing data batch 409\n","Worker 1 gradients (layer 0 sample): [ 0.05939407  0.88747287 -0.39947096 -1.2910436   0.13461904]\n","Aggregated gradients (layer 0 sample): [ 0.12043838  0.37425524 -0.29826623 -1.2305439  -0.0474117 ]\n","Parameters before update (layer 0 sample): [0.14992416 0.00917784 0.15575884 0.04945102 0.03361631]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892419 0.00817786 0.15675883 0.05045101 0.03461624]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 410\n","Worker 0 gradients (layer 0 sample): [-0.33727464 -0.5648937   0.84379494  3.5311937  -0.14259522]\n","Worker 1 processing data batch 410\n","Worker 1 gradients (layer 0 sample): [ 0.37401748 -1.2542689   0.16325173 -0.58450276 -0.8248335 ]\n","Aggregated gradients (layer 0 sample): [ 0.01837142 -0.9095813   0.50352335  1.4733455  -0.48371437]\n","Parameters before update (layer 0 sample): [0.14892419 0.00817786 0.15675883 0.05045101 0.03461624]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792436 0.00917784 0.15575884 0.04945102 0.03561622]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 410, Losses: [1.8773568, 1.9903862]\n","Worker 0 processing data batch 411\n","Worker 0 gradients (layer 0 sample): [-0.15534025  1.0143961   0.1728372   0.70705163  0.40275502]\n","Worker 1 processing data batch 411\n","Worker 1 gradients (layer 0 sample): [-0.15313807  1.8662187  -0.34153575 -0.88135946  1.2315545 ]\n","Aggregated gradients (layer 0 sample): [-0.15423916  1.4403074  -0.08434927 -0.08715391  0.81715477]\n","Parameters before update (layer 0 sample): [0.14792436 0.00917784 0.15575884 0.04945102 0.03561622]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892434 0.00817785 0.1567588  0.05045098 0.03461623]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 412\n","Worker 0 gradients (layer 0 sample): [ 0.01046748 -1.9834272   1.0207027   3.2396333  -1.0026433 ]\n","Worker 1 processing data batch 412\n","Worker 1 gradients (layer 0 sample): [ 0.1969648  -0.7002098  -0.00461942 -0.45060956 -0.7278317 ]\n","Aggregated gradients (layer 0 sample): [ 0.10371614 -1.3418185   0.5080416   1.3945119  -0.86523753]\n","Parameters before update (layer 0 sample): [0.14892434 0.00817785 0.1567588  0.05045098 0.03461623]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792438 0.00917784 0.15575881 0.04945099 0.03561622]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 413\n","Worker 0 gradients (layer 0 sample): [ 0.19489115  1.328549   -0.634482   -2.5289736   0.38387227]\n","Worker 1 processing data batch 413\n","Worker 1 gradients (layer 0 sample): [-0.23148462  1.4806284  -0.14546253  0.25084102  0.72753334]\n","Aggregated gradients (layer 0 sample): [-0.01829673  1.4045887  -0.38997227 -1.1390662   0.5557028 ]\n","Parameters before update (layer 0 sample): [0.14792438 0.00917784 0.15575881 0.04945099 0.03561622]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489242  0.00817785 0.1567588  0.05045098 0.03461624]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 414\n","Worker 0 gradients (layer 0 sample): [-0.08847538 -0.95590013  0.52671415  2.5132833  -0.45574096]\n","Worker 1 processing data batch 414\n","Worker 1 gradients (layer 0 sample): [-0.12401876 -1.5148987   0.8570129   4.1982794  -0.36255676]\n","Aggregated gradients (layer 0 sample): [-0.10624707 -1.2353994   0.69186354  3.3557813  -0.40914887]\n","Parameters before update (layer 0 sample): [0.1489242  0.00817785 0.1567588  0.05045098 0.03461624]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992416 0.00917784 0.15575881 0.04945099 0.03561622]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 415\n","Worker 0 gradients (layer 0 sample): [ 0.3992832   1.517288   -0.62626076 -2.7553325   0.0957631 ]\n","Worker 1 processing data batch 415\n","Worker 1 gradients (layer 0 sample): [ 0.30045775  0.9028523  -0.55176    -2.3725448   0.36890882]\n","Aggregated gradients (layer 0 sample): [ 0.34987047  1.2100701  -0.58901036 -2.5639386   0.23233595]\n","Parameters before update (layer 0 sample): [0.14992416 0.00917784 0.15575881 0.04945099 0.03561622]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892417 0.00817785 0.1567588  0.05045098 0.03461624]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 416\n","Worker 0 gradients (layer 0 sample): [-0.3687933  -1.3187546   0.8171579   4.2205563   0.06950191]\n","Worker 1 processing data batch 416\n","Worker 1 gradients (layer 0 sample): [-0.18448688 -0.8921529   0.9807761   3.1887174  -0.02960757]\n","Aggregated gradients (layer 0 sample): [-0.2766401  -1.1054537   0.898967    3.7046368   0.01994717]\n","Parameters before update (layer 0 sample): [0.14892417 0.00817785 0.1567588  0.05045098 0.03461624]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992416 0.00917784 0.15575881 0.04945099 0.03361641]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 417\n","Worker 0 gradients (layer 0 sample): [ 0.5148136   0.36626154 -0.6744811  -2.8222942  -0.38759938]\n","Worker 1 processing data batch 417\n","Worker 1 gradients (layer 0 sample): [-0.14649805  2.3544664  -0.5165324  -1.2261833   0.86466366]\n","Aggregated gradients (layer 0 sample): [ 0.18415777  1.360364   -0.5955068  -2.0242388   0.23853214]\n","Parameters before update (layer 0 sample): [0.14992416 0.00917784 0.15575881 0.04945099 0.03361641]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892419 0.00817785 0.1567588  0.05045098 0.03261643]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 418\n","Worker 0 gradients (layer 0 sample): [-0.3358572  -0.43573818  0.72504157  3.4837534   0.05410437]\n","Worker 1 processing data batch 418\n","Worker 1 gradients (layer 0 sample): [-0.07540398 -0.43227983  0.08854149  0.68528974 -0.13075355]\n","Aggregated gradients (layer 0 sample): [-0.2056306  -0.43400902  0.40679154  2.0845215  -0.03832459]\n","Parameters before update (layer 0 sample): [0.14892419 0.00817785 0.1567588  0.05045098 0.03261643]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992416 0.00917784 0.15575881 0.04945099 0.03361634]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 419\n","Worker 0 gradients (layer 0 sample): [ 0.9920558  0.4476682 -1.286024  -6.304472  -0.9461276]\n","Worker 1 processing data batch 419\n","Worker 1 gradients (layer 0 sample): [ 0.5827104   1.6759814  -0.9814416  -4.9529514  -0.09564981]\n","Aggregated gradients (layer 0 sample): [ 0.7873831  1.0618248 -1.1337328 -5.6287117 -0.5208887]\n","Parameters before update (layer 0 sample): [0.14992416 0.00917784 0.15575881 0.04945099 0.03361634]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892417 0.00817785 0.1567588  0.05045098 0.03461633]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 420\n","Worker 0 gradients (layer 0 sample): [-0.8572558   0.01047623  1.0411875   5.9536247   1.2229155 ]\n","Worker 1 processing data batch 420\n","Worker 1 gradients (layer 0 sample): [-0.17254886 -0.14481896  0.2870894   1.6604925   0.5699432 ]\n","Aggregated gradients (layer 0 sample): [-0.51490235 -0.06717137  0.66413844  3.8070586   0.89642936]\n","Parameters before update (layer 0 sample): [0.14892417 0.00817785 0.1567588  0.05045098 0.03461633]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992416 0.0091778  0.15575881 0.04945099 0.03361634]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 420, Losses: [1.9263924, 1.9231142]\n","Worker 0 processing data batch 421\n","Worker 0 gradients (layer 0 sample): [ 0.16524267  0.62818235 -0.46145073 -1.5497376  -0.18775749]\n","Worker 1 processing data batch 421\n","Worker 1 gradients (layer 0 sample): [-0.31544212  1.114353    0.09067826  1.2368249   0.5826391 ]\n","Aggregated gradients (layer 0 sample): [-0.07509972  0.8712677  -0.18538624 -0.15645635  0.1974408 ]\n","Parameters before update (layer 0 sample): [0.14992416 0.0091778  0.15575881 0.04945099 0.03361634]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092412 0.00817781 0.15675879 0.05045096 0.03261636]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 422\n","Worker 0 gradients (layer 0 sample): [ 0.0235577  -1.2651076   1.1194278   3.1222634  -0.32298946]\n","Worker 1 processing data batch 422\n","Worker 1 gradients (layer 0 sample): [-0.02133013 -2.1992426   1.1546234   3.7479491  -0.49593595]\n","Aggregated gradients (layer 0 sample): [ 1.1137854e-03 -1.7321751e+00  1.1370256e+00  3.4351063e+00\n"," -4.0946269e-01]\n","Parameters before update (layer 0 sample): [0.15092412 0.00817781 0.15675879 0.05045096 0.03261636]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992696 0.0091778  0.1557588  0.04945097 0.03361635]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 423\n","Worker 0 gradients (layer 0 sample): [ 0.25064504  0.68047816 -0.4684999  -2.1077266   0.1954118 ]\n","Worker 1 processing data batch 423\n","Worker 1 gradients (layer 0 sample): [ 0.41660014 -0.38442853 -0.28119835 -2.3550594  -0.40849   ]\n","Aggregated gradients (layer 0 sample): [ 0.33362257  0.14802481 -0.37484914 -2.2313929  -0.1065391 ]\n","Parameters before update (layer 0 sample): [0.14992696 0.0091778  0.1557588  0.04945097 0.03361635]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892697 0.00817783 0.15675879 0.05045096 0.03461631]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 424\n","Worker 0 gradients (layer 0 sample): [-0.5222365   0.15633698  0.49424142  3.154935    0.23104104]\n","Worker 1 processing data batch 424\n","Worker 1 gradients (layer 0 sample): [-0.27522573 -0.6438969   1.0096445   4.030535   -0.10398009]\n","Aggregated gradients (layer 0 sample): [-0.3987311  -0.24377996  0.751943    3.592735    0.06353047]\n","Parameters before update (layer 0 sample): [0.14892697 0.00817783 0.15675879 0.05045096 0.03461631]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992696 0.00917781 0.1557588  0.04945097 0.03361637]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 425\n","Worker 0 gradients (layer 0 sample): [ 0.6496879  1.4967175 -0.8547561 -4.295931  -0.1873767]\n","Worker 1 processing data batch 425\n","Worker 1 gradients (layer 0 sample): [ 0.277343    0.6141416  -0.31721172 -1.5530345  -0.13521683]\n","Aggregated gradients (layer 0 sample): [ 0.46351546  1.0554295  -0.58598393 -2.9244828  -0.16129676]\n","Parameters before update (layer 0 sample): [0.14992696 0.00917781 0.1557588  0.04945097 0.03361637]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892697 0.00817782 0.15675879 0.05045096 0.03461634]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 426\n","Worker 0 gradients (layer 0 sample): [-0.54046214 -0.36682183  0.7340337   3.8091717   0.47628096]\n","Worker 1 processing data batch 426\n","Worker 1 gradients (layer 0 sample): [-0.41184404 -0.55019027  0.8223293   3.7166665   0.64864975]\n","Aggregated gradients (layer 0 sample): [-0.47615308 -0.45850605  0.7781815   3.762919    0.56246537]\n","Parameters before update (layer 0 sample): [0.14892697 0.00817782 0.15675879 0.05045096 0.03461634]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992696 0.0091778  0.1557588  0.04945097 0.03361636]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 427\n","Worker 0 gradients (layer 0 sample): [ 0.3266037   0.16329138 -0.7481842  -3.1039028  -0.2833106 ]\n","Worker 1 processing data batch 427\n","Worker 1 gradients (layer 0 sample): [-0.00277542  1.6796447  -0.73515475 -1.8116741   0.6672781 ]\n","Aggregated gradients (layer 0 sample): [ 0.16191414  0.921468   -0.7416695  -2.4577885   0.19198376]\n","Parameters before update (layer 0 sample): [0.14992696 0.0091778  0.1557588  0.04945097 0.03361636]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892699 0.00817781 0.15675879 0.05045096 0.03261638]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 428\n","Worker 0 gradients (layer 0 sample): [-0.13885434 -1.0352585   0.9454692   3.0214567  -0.31011167]\n","Worker 1 processing data batch 428\n","Worker 1 gradients (layer 0 sample): [-0.330737   -0.92431843  1.036759    3.653233   -0.02056308]\n","Aggregated gradients (layer 0 sample): [-0.23479566 -0.9797885   0.99111414  3.337345   -0.16533738]\n","Parameters before update (layer 0 sample): [0.14892699 0.00817781 0.15675879 0.05045096 0.03261638]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992698 0.0091778  0.1557588  0.04945097 0.03361636]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 429\n","Worker 0 gradients (layer 0 sample): [ 0.7167245   0.17298993 -0.873927   -4.1320624  -0.30503005]\n","Worker 1 processing data batch 429\n","Worker 1 gradients (layer 0 sample): [ 0.03105383  0.75786614 -0.09381741  0.03300545  0.25346455]\n","Aggregated gradients (layer 0 sample): [ 0.37388918  0.46542805 -0.4838722  -2.0495286  -0.02578275]\n","Parameters before update (layer 0 sample): [0.14992698 0.0091778  0.1557588  0.04945097 0.03361636]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892699 0.00817782 0.15675879 0.05045096 0.03461623]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 430\n","Worker 0 gradients (layer 0 sample): [ 0.37801507 -1.1431752  -0.03969786 -0.6623535  -1.0114583 ]\n","Worker 1 processing data batch 430\n","Worker 1 gradients (layer 0 sample): [-0.1906031 -1.061448   0.6138809  2.5589042 -0.266461 ]\n","Aggregated gradients (layer 0 sample): [ 0.09370598 -1.1023116   0.2870915   0.9482753  -0.63895965]\n","Parameters before update (layer 0 sample): [0.14892699 0.00817782 0.15675879 0.05045096 0.03461623]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792703 0.00917781 0.1557588  0.04945097 0.03561622]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 430, Losses: [1.7519705, 2.0089111]\n","Worker 0 processing data batch 431\n","Worker 0 gradients (layer 0 sample): [-0.12548919  1.7140815  -0.13356826 -0.3537231   0.9527868 ]\n","Worker 1 processing data batch 431\n","Worker 1 gradients (layer 0 sample): [-0.07118118  2.377811   -0.6627603  -1.6183643   1.763897  ]\n","Aggregated gradients (layer 0 sample): [-0.09833518  2.0459461  -0.39816427 -0.9860437   1.3583419 ]\n","Parameters before update (layer 0 sample): [0.14792703 0.00917781 0.1557588  0.04945097 0.03561622]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892699 0.00817782 0.15675879 0.05045096 0.03461622]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 432\n","Worker 0 gradients (layer 0 sample): [-0.26427132 -1.50388     1.0233805   4.1193337  -0.27201524]\n","Worker 1 processing data batch 432\n","Worker 1 gradients (layer 0 sample): [ 0.02771619 -1.054242    0.3144992   1.2724224  -0.43730104]\n","Aggregated gradients (layer 0 sample): [-0.11827756 -1.2790611   0.6689398   2.695878   -0.35465813]\n","Parameters before update (layer 0 sample): [0.14892699 0.00817782 0.15675879 0.05045096 0.03461622]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992696 0.00917781 0.1557588  0.04945097 0.03561621]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 433\n","Worker 0 gradients (layer 0 sample): [ 0.21127081  0.57101816 -0.31906104 -1.5251733   0.15367055]\n","Worker 1 processing data batch 433\n","Worker 1 gradients (layer 0 sample): [ 0.37171417  1.8848615  -0.7611557  -3.2654638   0.50170565]\n","Aggregated gradients (layer 0 sample): [ 0.2914925  1.2279398 -0.5401084 -2.3953185  0.3276881]\n","Parameters before update (layer 0 sample): [0.14992696 0.00917781 0.1557588  0.04945097 0.03561621]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892697 0.00817782 0.15675879 0.05045096 0.03461622]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 434\n","Worker 0 gradients (layer 0 sample): [-0.11738373 -0.06445117  0.1485466   1.2317171   0.07082827]\n","Worker 1 processing data batch 434\n","Worker 1 gradients (layer 0 sample): [-0.18847421 -1.0568687   0.42284533  1.6135215  -0.26182145]\n","Aggregated gradients (layer 0 sample): [-0.15292896 -0.56065995  0.28569597  1.4226193  -0.09549659]\n","Parameters before update (layer 0 sample): [0.14892697 0.00817782 0.15675879 0.05045096 0.03461622]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992695 0.0091778  0.1557588  0.04945097 0.03561619]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 435\n","Worker 0 gradients (layer 0 sample): [ 0.3946728   1.0048828  -0.45726138 -2.8413465   0.07183675]\n","Worker 1 processing data batch 435\n","Worker 1 gradients (layer 0 sample): [ 0.05531541  1.6434121  -0.26784092 -1.1863003   0.7077799 ]\n","Aggregated gradients (layer 0 sample): [ 0.22499411  1.3241475  -0.36255115 -2.0138235   0.38980833]\n","Parameters before update (layer 0 sample): [0.14992695 0.0091778  0.1557588  0.04945097 0.03561619]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892697 0.00817781 0.15675879 0.05045096 0.0346162 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 436\n","Worker 0 gradients (layer 0 sample): [-0.4731785  -0.47727072  0.39268613  2.8958585   0.40873283]\n","Worker 1 processing data batch 436\n","Worker 1 gradients (layer 0 sample): [-0.47265503  0.21257979  0.70339465  3.6328273   0.73616755]\n","Aggregated gradients (layer 0 sample): [-0.47291678 -0.13234547  0.5480404   3.2643428   0.57245016]\n","Parameters before update (layer 0 sample): [0.14892697 0.00817781 0.15675879 0.05045096 0.0346162 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992696 0.00917778 0.1557588  0.04945097 0.03361621]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 437\n","Worker 0 gradients (layer 0 sample): [ 0.55495346  0.10143822 -0.38488805 -2.1483045  -0.16257316]\n","Worker 1 processing data batch 437\n","Worker 1 gradients (layer 0 sample): [ 0.36436182 -0.2582363  -0.476546   -1.7988323  -0.24057426]\n","Aggregated gradients (layer 0 sample): [ 0.45965764 -0.07839903 -0.43071702 -1.9735684  -0.2015737 ]\n","Parameters before update (layer 0 sample): [0.14992696 0.00917778 0.1557588  0.04945097 0.03361621]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892697 0.01017774 0.15675879 0.05045096 0.03461619]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 438\n","Worker 0 gradients (layer 0 sample): [-0.34807244 -1.4946148   1.1227858   4.4464736   0.0399866 ]\n","Worker 1 processing data batch 438\n","Worker 1 gradients (layer 0 sample): [-0.4287097   0.6043255   0.48712423  3.0309148   0.56659806]\n","Aggregated gradients (layer 0 sample): [-0.38839108 -0.44514468  0.804955    3.7386942   0.30329233]\n","Parameters before update (layer 0 sample): [0.14892697 0.01017774 0.15675879 0.05045096 0.03461619]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992696 0.01117772 0.1557588  0.04945097 0.03361621]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 439\n","Worker 0 gradients (layer 0 sample): [ 0.1819644   1.8598827  -0.51917195 -2.211122    0.01398928]\n","Worker 1 processing data batch 439\n","Worker 1 gradients (layer 0 sample): [ 0.48984987  1.5124635  -0.66096663 -3.5017185  -0.29316536]\n","Aggregated gradients (layer 0 sample): [ 0.33590713  1.6861731  -0.5900693  -2.8564203  -0.13958803]\n","Parameters before update (layer 0 sample): [0.14992696 0.01117772 0.1557588  0.04945097 0.03361621]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892697 0.01017773 0.15675879 0.05045096 0.03461618]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 440\n","Worker 0 gradients (layer 0 sample): [ 0.14987665 -0.8347063   0.12459922  0.28447443 -0.2855186 ]\n","Worker 1 processing data batch 440\n","Worker 1 gradients (layer 0 sample): [-0.70704263  2.827757    0.23579416  3.073269    1.7750072 ]\n","Aggregated gradients (layer 0 sample): [-0.278583    0.9965253   0.18019669  1.6788716   0.7447443 ]\n","Parameters before update (layer 0 sample): [0.14892697 0.01017773 0.15675879 0.05045096 0.03461618]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992696 0.00917774 0.15575881 0.04945097 0.03361619]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 440, Losses: [2.014675, 2.108867]\n","Worker 0 processing data batch 441\n","Worker 0 gradients (layer 0 sample): [ 0.02376948 -2.1185818   0.87885666  2.7780218  -0.6751482 ]\n","Worker 1 processing data batch 441\n","Worker 1 gradients (layer 0 sample): [ 0.8537544  -2.142898    0.24297944 -1.5155017  -1.3436935 ]\n","Aggregated gradients (layer 0 sample): [ 0.43876195 -2.13074     0.56091803  0.63126004 -1.0094209 ]\n","Parameters before update (layer 0 sample): [0.14992696 0.00917774 0.15575881 0.04945097 0.03361619]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892697 0.01017773 0.15475883 0.04845098 0.03461618]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 442\n","Worker 0 gradients (layer 0 sample): [ 0.5252594   2.6604872  -0.96947765 -4.055508    0.3588019 ]\n","Worker 1 processing data batch 442\n","Worker 1 gradients (layer 0 sample): [ 0.4870965  1.2978978 -0.8012857 -3.3316512 -0.1399037]\n","Aggregated gradients (layer 0 sample): [ 0.5061779  1.9791925 -0.8853817 -3.6935797  0.1094491]\n","Parameters before update (layer 0 sample): [0.14892697 0.01017773 0.15475883 0.04845098 0.03461618]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792699 0.00917774 0.15575881 0.04945097 0.03361621]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 443\n","Worker 0 gradients (layer 0 sample): [ 0.2733093  -1.2946471   0.1170417  -0.64689803 -0.8004128 ]\n","Worker 1 processing data batch 443\n","Worker 1 gradients (layer 0 sample): [-0.16768113 -0.32039806  0.1786781   1.2796955   0.1111038 ]\n","Aggregated gradients (layer 0 sample): [ 0.05281408 -0.8075226   0.1478599   0.31639874 -0.3446545 ]\n","Parameters before update (layer 0 sample): [0.14792699 0.00917774 0.15575881 0.04945097 0.03361621]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692706 0.01017773 0.15475884 0.04845099 0.0346162 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 444\n","Worker 0 gradients (layer 0 sample): [ 0.02760539  1.4393265  -0.41628006 -1.2720292   0.55174756]\n","Worker 1 processing data batch 444\n","Worker 1 gradients (layer 0 sample): [ 0.13996054  1.9219866  -0.37832737 -1.1543072   0.43486494]\n","Aggregated gradients (layer 0 sample): [ 0.08378296  1.6806566  -0.3973037  -1.2131681   0.49330625]\n","Parameters before update (layer 0 sample): [0.14692706 0.01017773 0.15475884 0.04845099 0.0346162 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1459271  0.00917774 0.15575883 0.04945098 0.03361621]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 445\n","Worker 0 gradients (layer 0 sample): [-0.13631397 -0.53590965  0.47572568  2.448013   -0.05186314]\n","Worker 1 processing data batch 445\n","Worker 1 gradients (layer 0 sample): [-0.01094103 -0.9360515   0.5053568   1.8370929  -0.16313237]\n","Aggregated gradients (layer 0 sample): [-0.0736275  -0.7359806   0.49054122  2.1425529  -0.10749775]\n","Parameters before update (layer 0 sample): [0.1459271  0.00917774 0.15575883 0.04945098 0.03361621]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692706 0.01017773 0.15475884 0.04845099 0.03461618]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 446\n","Worker 0 gradients (layer 0 sample): [ 0.14236224  2.027664   -0.2881037  -1.7641084   0.5099319 ]\n","Worker 1 processing data batch 446\n","Worker 1 gradients (layer 0 sample): [-0.04259312  1.3819697  -0.27587426 -1.1317728   0.12543467]\n","Aggregated gradients (layer 0 sample): [ 0.04988456  1.7048168  -0.28198898 -1.4479406   0.31768328]\n","Parameters before update (layer 0 sample): [0.14692706 0.01017773 0.15475884 0.04845099 0.03461618]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592713 0.00917774 0.15575883 0.04945098 0.03361619]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 447\n","Worker 0 gradients (layer 0 sample): [ 0.5010088 -2.1868093  0.8749141  1.409934  -0.963495 ]\n","Worker 1 processing data batch 447\n","Worker 1 gradients (layer 0 sample): [ 0.12237903 -1.7224199   0.7688669   1.8257641  -0.48795396]\n","Aggregated gradients (layer 0 sample): [ 0.3116939  -1.9546146   0.8218905   1.6178491  -0.72572446]\n","Parameters before update (layer 0 sample): [0.14592713 0.00917774 0.15575883 0.04945098 0.03361619]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492714 0.01017773 0.15475884 0.04845099 0.03461618]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 448\n","Worker 0 gradients (layer 0 sample): [-0.09442601  2.7551403  -0.7229372  -1.8327489   0.8687984 ]\n","Worker 1 processing data batch 448\n","Worker 1 gradients (layer 0 sample): [ 0.23958573  2.319794   -0.8508917  -3.2248378   0.32750055]\n","Aggregated gradients (layer 0 sample): [ 0.07257986  2.537467   -0.78691447 -2.5287933   0.5981495 ]\n","Parameters before update (layer 0 sample): [0.14492714 0.01017773 0.15475884 0.04845099 0.03461618]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392719 0.00917774 0.15575883 0.04945098 0.03361619]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 449\n","Worker 0 gradients (layer 0 sample): [-0.19009036 -2.0510821   1.4230998   4.2380543  -0.4804531 ]\n","Worker 1 processing data batch 449\n","Worker 1 gradients (layer 0 sample): [-0.23327333 -1.5309198   1.2022094   4.253845   -0.27763373]\n","Aggregated gradients (layer 0 sample): [-0.21168184 -1.791001    1.3126545   4.2459497  -0.3790434 ]\n","Parameters before update (layer 0 sample): [0.14392719 0.00917774 0.15575883 0.04945098 0.03361619]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492716 0.01017773 0.15475884 0.04845099 0.03461618]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 450\n","Worker 0 gradients (layer 0 sample): [ 0.20917758  1.0690211  -0.86079377 -2.4128363  -0.16105974]\n","Worker 1 processing data batch 450\n","Worker 1 gradients (layer 0 sample): [ 0.22734195  1.1860702  -0.559702   -2.1282136   0.11185723]\n","Aggregated gradients (layer 0 sample): [ 0.21825977  1.1275456  -0.7102479  -2.270525   -0.02460125]\n","Parameters before update (layer 0 sample): [0.14492716 0.01017773 0.15475884 0.04845099 0.03461618]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392719 0.00917774 0.15575883 0.04945098 0.03561604]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 450, Losses: [1.8866099, 2.059236]\n","Worker 0 processing data batch 451\n","Worker 0 gradients (layer 0 sample): [-0.70132995 -0.59533674  1.3899019   5.802473    0.79832274]\n","Worker 1 processing data batch 451\n","Worker 1 gradients (layer 0 sample): [-0.66260564 -0.434396    0.97703385  4.7541995   0.7222282 ]\n","Aggregated gradients (layer 0 sample): [-0.6819678  -0.51486635  1.1834679   5.2783365   0.7602755 ]\n","Parameters before update (layer 0 sample): [0.14392719 0.00917774 0.15575883 0.04945098 0.03561604]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492717 0.01017772 0.15475884 0.04845099 0.03461605]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 452\n","Worker 0 gradients (layer 0 sample): [ 0.25855866  2.1033247  -0.80268425 -3.0939822  -0.21444091]\n","Worker 1 processing data batch 452\n","Worker 1 gradients (layer 0 sample): [ 0.23868513  1.4586542  -0.8011093  -3.1481392  -0.08557479]\n","Aggregated gradients (layer 0 sample): [ 0.2486219   1.7809894  -0.8018968  -3.1210608  -0.15000784]\n","Parameters before update (layer 0 sample): [0.14492717 0.01017772 0.15475884 0.04845099 0.03461605]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392719 0.00917773 0.15575883 0.04945098 0.03561602]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 453\n","Worker 0 gradients (layer 0 sample): [-0.1244707  -0.8094508   0.41925904  1.7547859  -0.03978005]\n","Worker 1 processing data batch 453\n","Worker 1 gradients (layer 0 sample): [ 0.16144332 -0.8102173   0.11251393  0.45263347 -0.44537264]\n","Aggregated gradients (layer 0 sample): [ 0.01848631 -0.80983406  0.2658865   1.1037097  -0.24257635]\n","Parameters before update (layer 0 sample): [0.14392719 0.00917773 0.15575883 0.04945098 0.03561602]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292736 0.01017772 0.15475884 0.04845099 0.036616  ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 454\n","Worker 0 gradients (layer 0 sample): [ 0.15075958  1.6084714  -0.63799554 -2.3307197   0.60961956]\n","Worker 1 processing data batch 454\n","Worker 1 gradients (layer 0 sample): [-0.05193022  1.6039592  -0.41941822 -1.1875947   0.6356245 ]\n","Aggregated gradients (layer 0 sample): [ 0.04941468  1.6062152  -0.5287069  -1.7591572   0.622622  ]\n","Parameters before update (layer 0 sample): [0.14292736 0.01017772 0.15475884 0.04845099 0.036616  ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192744 0.00917773 0.15575883 0.04945098 0.03561601]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 455\n","Worker 0 gradients (layer 0 sample): [-5.3567308e-01 -2.0477045e+00  2.2368855e+00  7.5616922e+00\n","  2.4476647e-04]\n","Worker 1 processing data batch 455\n","Worker 1 gradients (layer 0 sample): [-0.28198317 -1.9584005   1.6814402   5.4652977  -0.18789834]\n","Aggregated gradients (layer 0 sample): [-0.40882814 -2.0030525   1.959163    6.513495   -0.09382679]\n","Parameters before update (layer 0 sample): [0.14192744 0.00917773 0.15575883 0.04945098 0.03561601]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292742 0.01017772 0.15475884 0.04845099 0.03661598]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 456\n","Worker 0 gradients (layer 0 sample): [-0.0832593   0.8902266   0.01148009 -0.30126762  0.4034745 ]\n","Worker 1 processing data batch 456\n","Worker 1 gradients (layer 0 sample): [-0.22230658  1.8878629  -0.4097348  -0.483109    0.73652035]\n","Aggregated gradients (layer 0 sample): [-0.15278295  1.3890448  -0.19912735 -0.3921883   0.56999743]\n","Parameters before update (layer 0 sample): [0.14292742 0.01017772 0.15475884 0.04845099 0.03661598]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1439274  0.00917773 0.15575881 0.04945097 0.03561599]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 457\n","Worker 0 gradients (layer 0 sample): [ 0.02880888 -0.8253384   0.37998313  1.3612809  -0.15361205]\n","Worker 1 processing data batch 457\n","Worker 1 gradients (layer 0 sample): [-0.16963926 -1.2222207   0.80007637  2.7138822  -0.20518252]\n","Aggregated gradients (layer 0 sample): [-0.07041519 -1.0237795   0.5900297   2.0375814  -0.17939728]\n","Parameters before update (layer 0 sample): [0.1439274  0.00917773 0.15575881 0.04945097 0.03561599]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492734 0.01017772 0.15475883 0.04845098 0.03661596]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 458\n","Worker 0 gradients (layer 0 sample): [ 0.24778591  2.1837037  -0.81315345 -2.636208    0.39737877]\n","Worker 1 processing data batch 458\n","Worker 1 gradients (layer 0 sample): [-0.00718781  1.6232204  -0.31381515 -0.91136944  0.48120856]\n","Aggregated gradients (layer 0 sample): [ 0.12029905  1.903462   -0.5634843  -1.7737887   0.43929368]\n","Parameters before update (layer 0 sample): [0.14492734 0.01017772 0.15475883 0.04845098 0.03661596]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392737 0.00917773 0.15575881 0.04945097 0.03561598]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 459\n","Worker 0 gradients (layer 0 sample): [ 0.005857  -1.3441889  0.8509135  2.467476  -0.4114399]\n","Worker 1 processing data batch 459\n","Worker 1 gradients (layer 0 sample): [-0.4553413  -0.16453728  0.45884228  2.6287124   0.44630718]\n","Aggregated gradients (layer 0 sample): [-0.22474216 -0.7543631   0.6548779   2.5480943   0.01743364]\n","Parameters before update (layer 0 sample): [0.14392737 0.00917773 0.15575881 0.04945097 0.03561598]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492734 0.01017772 0.15475883 0.04845098 0.03461616]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 460\n","Worker 0 gradients (layer 0 sample): [ 0.02963093  2.0006351  -0.59591246 -2.208611    0.02566206]\n","Worker 1 processing data batch 460\n","Worker 1 gradients (layer 0 sample): [ 0.49506742  0.74009186 -0.67280424 -2.9265695  -0.2810564 ]\n","Aggregated gradients (layer 0 sample): [ 0.2623492   1.3703635  -0.63435835 -2.5675902  -0.12769717]\n","Parameters before update (layer 0 sample): [0.14492734 0.01017772 0.15475883 0.04845098 0.03461616]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392735 0.00917773 0.15575881 0.04945097 0.03561613]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 460, Losses: [1.8040235, 2.0141778]\n","Worker 0 processing data batch 461\n","Worker 0 gradients (layer 0 sample): [-0.26960406 -0.1182816   0.3524066   1.6691718   0.33951867]\n","Worker 1 processing data batch 461\n","Worker 1 gradients (layer 0 sample): [ 0.15952511 -0.70021695  0.08337034  0.38503945 -0.19713178]\n","Aggregated gradients (layer 0 sample): [-0.05503947 -0.40924928  0.21788846  1.0271056   0.07119344]\n","Parameters before update (layer 0 sample): [0.14392735 0.00917773 0.15575881 0.04945097 0.03561613]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449273  0.01017771 0.15475884 0.04845098 0.03461618]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 462\n","Worker 0 gradients (layer 0 sample): [ 0.19010642  2.929244   -0.6980784  -2.5312881   0.32977852]\n","Worker 1 processing data batch 462\n","Worker 1 gradients (layer 0 sample): [ 0.21700124 -0.09458035 -0.29748797 -1.0478649  -0.16947788]\n","Aggregated gradients (layer 0 sample): [ 0.20355383  1.4173318  -0.49778318 -1.7895765   0.08015032]\n","Parameters before update (layer 0 sample): [0.1449273  0.01017771 0.15475884 0.04845098 0.03461618]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392732 0.00917772 0.15575883 0.04945097 0.03361623]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 463\n","Worker 0 gradients (layer 0 sample): [-0.0795563  -2.4517465   1.2240976   4.1314645  -0.32717898]\n","Worker 1 processing data batch 463\n","Worker 1 gradients (layer 0 sample): [-0.11928821 -2.3710322   1.5785338   5.2502995  -0.33719406]\n","Aggregated gradients (layer 0 sample): [-0.09942225 -2.4113894   1.4013157   4.6908817  -0.33218652]\n","Parameters before update (layer 0 sample): [0.14392732 0.00917772 0.15575883 0.04945097 0.03361623]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492728 0.01017771 0.15475884 0.04845098 0.03461621]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 464\n","Worker 0 gradients (layer 0 sample): [ 0.09147264  2.7084088  -0.70753586 -2.4438128   0.50087166]\n","Worker 1 processing data batch 464\n","Worker 1 gradients (layer 0 sample): [ 0.21292353  3.0932405  -0.7971065  -3.0217693   0.45872456]\n","Aggregated gradients (layer 0 sample): [ 0.15219808  2.9008245  -0.7523212  -2.732791    0.4797981 ]\n","Parameters before update (layer 0 sample): [0.14492728 0.01017771 0.15475884 0.04845098 0.03461621]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1439273  0.00917772 0.15575883 0.04945097 0.03361623]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 465\n","Worker 0 gradients (layer 0 sample): [-0.04460938 -1.1306764   0.7749702   2.4304948  -0.11054993]\n","Worker 1 processing data batch 465\n","Worker 1 gradients (layer 0 sample): [ 0.20183906 -0.07948619  0.001369   -0.77624243 -0.29496154]\n","Aggregated gradients (layer 0 sample): [ 0.07861485 -0.6050813   0.3881696   0.82712615 -0.20275573]\n","Parameters before update (layer 0 sample): [0.1439273  0.00917772 0.15575883 0.04945097 0.03361623]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292735 0.01017771 0.15475884 0.04845098 0.0346162 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 466\n","Worker 0 gradients (layer 0 sample): [-0.3659956   2.6641002  -0.19663781 -0.04057862  0.96640635]\n","Worker 1 processing data batch 466\n","Worker 1 gradients (layer 0 sample): [ 0.10700791  1.6027815  -0.6252723  -1.821826    0.24554762]\n","Aggregated gradients (layer 0 sample): [-0.12949383  2.133441   -0.41095504 -0.9312023   0.605977  ]\n","Parameters before update (layer 0 sample): [0.14292735 0.01017771 0.15475884 0.04845098 0.0346162 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392732 0.00917772 0.15575883 0.04945097 0.03361621]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 467\n","Worker 0 gradients (layer 0 sample): [-0.18188636 -1.3442986   1.1905962   4.113289   -0.03230537]\n","Worker 1 processing data batch 467\n","Worker 1 gradients (layer 0 sample): [-0.38745287 -0.708525    0.9415096   4.0302134   0.1867136 ]\n","Aggregated gradients (layer 0 sample): [-0.2846696  -1.0264118   1.0660529   4.071751    0.07720412]\n","Parameters before update (layer 0 sample): [0.14392732 0.00917772 0.15575883 0.04945097 0.03361621]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492731 0.01017771 0.15475884 0.04845098 0.03261626]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 468\n","Worker 0 gradients (layer 0 sample): [ 0.8238841   0.41742498 -0.531843   -3.3287685  -0.38021988]\n","Worker 1 processing data batch 468\n","Worker 1 gradients (layer 0 sample): [ 0.54506654  0.9677625  -0.5559057  -2.616699   -0.2877835 ]\n","Aggregated gradients (layer 0 sample): [ 0.6844753  0.6925937 -0.5438744 -2.9727337 -0.3340017]\n","Parameters before update (layer 0 sample): [0.14492731 0.01017771 0.15475884 0.04845098 0.03261626]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392732 0.00917772 0.15575883 0.04945097 0.03361624]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 469\n","Worker 0 gradients (layer 0 sample): [-0.3582285   0.4278805   0.47618145  2.1789365   0.2646594 ]\n","Worker 1 processing data batch 469\n","Worker 1 gradients (layer 0 sample): [-0.10393746 -0.26386392  0.10302983  0.9711703  -0.162539  ]\n","Aggregated gradients (layer 0 sample): [-0.23108298  0.08200829  0.28960565  1.5750535   0.0510602 ]\n","Parameters before update (layer 0 sample): [0.14392732 0.00917772 0.15575883 0.04945097 0.03361624]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449273  0.00817776 0.15475884 0.04845098 0.03261631]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 470\n","Worker 0 gradients (layer 0 sample): [ 0.88204294  0.18328322 -0.46907654 -3.6298056  -0.4915614 ]\n","Worker 1 processing data batch 470\n","Worker 1 gradients (layer 0 sample): [ 0.04335216  0.48910415 -0.29322422 -0.58877337  0.04879751]\n","Aggregated gradients (layer 0 sample): [ 0.46269757  0.33619368 -0.38115036 -2.1092894  -0.22138195]\n","Parameters before update (layer 0 sample): [0.1449273  0.00817776 0.15475884 0.04845098 0.03261631]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1439273  0.00717778 0.15575883 0.04945097 0.03361629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 470, Losses: [2.3074718, 1.9965252]\n","Worker 0 processing data batch 471\n","Worker 0 gradients (layer 0 sample): [ 0.09716024 -1.2000625   1.3141258   3.0010257  -0.3563615 ]\n","Worker 1 processing data batch 471\n","Worker 1 gradients (layer 0 sample): [ 0.01209254 -1.8883724   1.7107737   4.3258076  -0.4514302 ]\n","Aggregated gradients (layer 0 sample): [ 0.05462639 -1.5442175   1.5124497   3.6634166  -0.40389585]\n","Parameters before update (layer 0 sample): [0.1439273  0.00717778 0.15575883 0.04945097 0.03361629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292736 0.00817777 0.15475884 0.04845098 0.03461628]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 472\n","Worker 0 gradients (layer 0 sample): [ 0.16979931  0.17786527 -0.3181052  -1.3708785  -0.35938585]\n","Worker 1 processing data batch 472\n","Worker 1 gradients (layer 0 sample): [ 0.2103133  -0.06091487 -0.24732743 -1.0155058  -0.25207555]\n","Aggregated gradients (layer 0 sample): [ 0.19005631  0.0584752  -0.2827163  -1.1931921  -0.3057307 ]\n","Parameters before update (layer 0 sample): [0.14292736 0.00817777 0.15475884 0.04845098 0.03461628]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192739 0.00717783 0.15575883 0.04945097 0.03561626]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 473\n","Worker 0 gradients (layer 0 sample): [-0.37987804 -0.5465099   0.5942003   3.0288744   0.07685754]\n","Worker 1 processing data batch 473\n","Worker 1 gradients (layer 0 sample): [-0.3022582   0.07241445  0.3518533   2.4023366   0.5113494 ]\n","Aggregated gradients (layer 0 sample): [-0.34106812 -0.23704773  0.4730268   2.7156055   0.29410344]\n","Parameters before update (layer 0 sample): [0.14192739 0.00717783 0.15575883 0.04945097 0.03561626]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292738 0.00817781 0.15475884 0.04845098 0.03461628]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 474\n","Worker 0 gradients (layer 0 sample): [ 0.19798481  1.3720846  -0.43222785 -1.7870163   0.35026383]\n","Worker 1 processing data batch 474\n","Worker 1 gradients (layer 0 sample): [ 0.02702551  0.9800676  -0.67404544 -1.95803     0.07777118]\n","Aggregated gradients (layer 0 sample): [ 0.11250516  1.1760762  -0.55313665 -1.8725231   0.21401751]\n","Parameters before update (layer 0 sample): [0.14292738 0.00817781 0.15475884 0.04845098 0.03461628]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1419274  0.00717782 0.15575883 0.04945097 0.0336163 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 475\n","Worker 0 gradients (layer 0 sample): [-0.5378879  -1.1252649   1.043251    4.94547    -0.08439349]\n","Worker 1 processing data batch 475\n","Worker 1 gradients (layer 0 sample): [ 0.40363622 -2.0376744   1.1537737   2.21162    -0.7433821 ]\n","Aggregated gradients (layer 0 sample): [-0.06712583 -1.5814697   1.0985124   3.578545   -0.4138878 ]\n","Parameters before update (layer 0 sample): [0.1419274  0.00717782 0.15575883 0.04945097 0.0336163 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292735 0.00817781 0.15475884 0.04845098 0.03461629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 476\n","Worker 0 gradients (layer 0 sample): [ 0.21849309  0.10752898 -0.33350664 -1.1113204  -0.18909213]\n","Worker 1 processing data batch 476\n","Worker 1 gradients (layer 0 sample): [ 0.2094081   0.77903104 -0.19070321 -1.1819956   0.00927374]\n","Aggregated gradients (layer 0 sample): [ 0.2139506   0.44328    -0.26210493 -1.146658   -0.0899092 ]\n","Parameters before update (layer 0 sample): [0.14292735 0.00817781 0.15475884 0.04845098 0.03461629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192738 0.00717783 0.15575883 0.04945097 0.03561625]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 477\n","Worker 0 gradients (layer 0 sample): [-0.00534864 -0.39549023  0.17951989  0.92806965  0.255327  ]\n","Worker 1 processing data batch 477\n","Worker 1 gradients (layer 0 sample): [-0.45237944 -0.6747086   0.80405366  3.1345582   0.2677892 ]\n","Aggregated gradients (layer 0 sample): [-0.22886404 -0.5350994   0.49178678  2.031314    0.26155812]\n","Parameters before update (layer 0 sample): [0.14192738 0.00717783 0.15575883 0.04945097 0.03561625]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292735 0.00817781 0.15475884 0.04845098 0.03461627]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 478\n","Worker 0 gradients (layer 0 sample): [ 0.6713301   1.403269   -0.9209193  -4.3241887   0.06839478]\n","Worker 1 processing data batch 478\n","Worker 1 gradients (layer 0 sample): [-0.11142412  0.8009128  -0.32213697 -0.6762412   0.17348194]\n","Aggregated gradients (layer 0 sample): [ 0.279953    1.102091   -0.62152815 -2.500215    0.12093836]\n","Parameters before update (layer 0 sample): [0.14292735 0.00817781 0.15475884 0.04845098 0.03461627]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192736 0.00717782 0.15575883 0.04945097 0.0336163 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 479\n","Worker 0 gradients (layer 0 sample): [-0.06458168 -0.8417561   0.7226267   2.147382   -0.36204207]\n","Worker 1 processing data batch 479\n","Worker 1 gradients (layer 0 sample): [-0.27274996 -0.4148024   0.24655992  1.4993012   0.00775705]\n","Aggregated gradients (layer 0 sample): [-0.16866583 -0.62827927  0.4845933   1.8233416  -0.17714252]\n","Parameters before update (layer 0 sample): [0.14192736 0.00717782 0.15575883 0.04945097 0.0336163 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292733 0.00817781 0.15475884 0.04845098 0.03461627]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 480\n","Worker 0 gradients (layer 0 sample): [-0.16546224  0.7610377  -0.4867188  -0.60275567  0.5445199 ]\n","Worker 1 processing data batch 480\n","Worker 1 gradients (layer 0 sample): [ 0.18365715  1.2230414  -0.56173337 -2.1083655   0.10312612]\n","Aggregated gradients (layer 0 sample): [ 0.00909746  0.99203956 -0.52422607 -1.3555605   0.323823  ]\n","Parameters before update (layer 0 sample): [0.14292733 0.00817781 0.15475884 0.04845098 0.03461627]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192769 0.00717782 0.15575883 0.04945097 0.03361629]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 480, Losses: [1.8705066, 2.0062852]\n","Worker 0 processing data batch 481\n","Worker 0 gradients (layer 0 sample): [-0.14692065 -0.7741678   0.40218952  1.9727132  -0.21993701]\n","Worker 1 processing data batch 481\n","Worker 1 gradients (layer 0 sample): [-0.08782152 -1.5238471   1.0837231   3.7442894  -0.3033252 ]\n","Aggregated gradients (layer 0 sample): [-0.11737108 -1.1490074   0.7429563   2.8585014  -0.2616311 ]\n","Parameters before update (layer 0 sample): [0.14192769 0.00717782 0.15575883 0.04945097 0.03361629]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292766 0.00817781 0.15475884 0.04845098 0.03461627]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 482\n","Worker 0 gradients (layer 0 sample): [ 0.5455399   1.1412199  -0.5309172  -3.1742191   0.06162651]\n","Worker 1 processing data batch 482\n","Worker 1 gradients (layer 0 sample): [ 0.8545246   0.47469717 -0.5340505  -3.7560823  -0.9486202 ]\n","Aggregated gradients (layer 0 sample): [ 0.70003223  0.8079585  -0.5324839  -3.4651508  -0.44349685]\n","Parameters before update (layer 0 sample): [0.14292766 0.00817781 0.15475884 0.04845098 0.03461627]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192767 0.00717782 0.15575883 0.04945097 0.03561626]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 483\n","Worker 0 gradients (layer 0 sample): [-0.17588474 -0.3791077   0.24690159  1.5784018   0.06601123]\n","Worker 1 processing data batch 483\n","Worker 1 gradients (layer 0 sample): [-0.40491772 -0.28160858  0.6136327   3.153644    0.47864604]\n","Aggregated gradients (layer 0 sample): [-0.29040122 -0.33035815  0.43026713  2.366023    0.27232864]\n","Parameters before update (layer 0 sample): [0.14192767 0.00717782 0.15575883 0.04945097 0.03561626]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292766 0.00817781 0.15475884 0.04845098 0.03461627]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 484\n","Worker 0 gradients (layer 0 sample): [ 0.05296264  1.1599648  -0.47512832 -1.7716805   0.06892921]\n","Worker 1 processing data batch 484\n","Worker 1 gradients (layer 0 sample): [ 0.04055279  0.36056998 -0.28196585 -0.9795674  -0.0569112 ]\n","Aggregated gradients (layer 0 sample): [ 0.04675772  0.7602674  -0.37854707 -1.375624    0.00600901]\n","Parameters before update (layer 0 sample): [0.14292766 0.00817781 0.15475884 0.04845098 0.03461627]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192773 0.00717782 0.15575883 0.04945097 0.03361681]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 485\n","Worker 0 gradients (layer 0 sample): [ 0.07177687 -0.6497997   0.15292777  0.12544107 -0.32480586]\n","Worker 1 processing data batch 485\n","Worker 1 gradients (layer 0 sample): [-0.42555213 -0.9929676   0.8082095   4.1197762   0.08297627]\n","Aggregated gradients (layer 0 sample): [-0.17688763 -0.82138366  0.48056862  2.1226087  -0.12091479]\n","Parameters before update (layer 0 sample): [0.14192773 0.00717782 0.15575883 0.04945097 0.03361681]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1429277  0.00817781 0.15475884 0.04845098 0.03461678]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 486\n","Worker 0 gradients (layer 0 sample): [ 0.39216083  0.58355105 -0.61100143 -2.8635354  -0.32622528]\n","Worker 1 processing data batch 486\n","Worker 1 gradients (layer 0 sample): [ 0.32312107  0.62669635 -0.54123664 -2.31124     0.11337987]\n","Aggregated gradients (layer 0 sample): [ 0.35764095  0.6051237  -0.57611907 -2.5873876  -0.10642271]\n","Parameters before update (layer 0 sample): [0.1429277  0.00817781 0.15475884 0.04845098 0.03461678]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192772 0.00717782 0.15575883 0.04945097 0.03561674]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 487\n","Worker 0 gradients (layer 0 sample): [-0.27333406 -0.34581995  0.54307866  1.8615487   0.5360467 ]\n","Worker 1 processing data batch 487\n","Worker 1 gradients (layer 0 sample): [-0.69528246 -0.6284487   1.0274062   5.357442    0.62142456]\n","Aggregated gradients (layer 0 sample): [-0.48430824 -0.48713434  0.78524244  3.6094952   0.5787356 ]\n","Parameters before update (layer 0 sample): [0.14192772 0.00717782 0.15575883 0.04945097 0.03561674]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1429277  0.00817781 0.15475884 0.04845098 0.03461675]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 488\n","Worker 0 gradients (layer 0 sample): [-0.04302707  0.399745   -0.40626463 -0.90646213  0.34683642]\n","Worker 1 processing data batch 488\n","Worker 1 gradients (layer 0 sample): [ 0.43812904  0.7903726  -0.25010896 -2.285841   -0.30471027]\n","Aggregated gradients (layer 0 sample): [ 0.19755098  0.5950588  -0.3281868  -1.5961516   0.02106307]\n","Parameters before update (layer 0 sample): [0.1429277  0.00817781 0.15475884 0.04845098 0.03461675]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192773 0.00717782 0.15575883 0.04945097 0.03361691]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 489\n","Worker 0 gradients (layer 0 sample): [-0.36505637 -0.32106447  0.3052116   2.3664825   0.10141767]\n","Worker 1 processing data batch 489\n","Worker 1 gradients (layer 0 sample): [-0.06489055 -1.7572927   1.1005932   3.306471   -0.818613  ]\n","Aggregated gradients (layer 0 sample): [-0.21497345 -1.0391786   0.70290244  2.8364768  -0.35859767]\n","Parameters before update (layer 0 sample): [0.14192773 0.00717782 0.15575883 0.04945097 0.03361691]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1429277  0.00817781 0.15475884 0.04845098 0.0346169 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 490\n","Worker 0 gradients (layer 0 sample): [-0.2779425   1.6930325  -0.75345886 -1.1706092   1.4338963 ]\n","Worker 1 processing data batch 490\n","Worker 1 gradients (layer 0 sample): [-0.05558039  1.9890733  -0.51591605 -1.8036023   0.9531823 ]\n","Aggregated gradients (layer 0 sample): [-0.16676146  1.8410529  -0.6346874  -1.4871058   1.1935393 ]\n","Parameters before update (layer 0 sample): [0.1429277  0.00817781 0.15475884 0.04845098 0.0346169 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392768 0.00717782 0.15575883 0.04945097 0.0336169 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 490, Losses: [1.7463499, 2.0956616]\n","Worker 0 processing data batch 491\n","Worker 0 gradients (layer 0 sample): [-0.03458138 -0.6742815   0.21120836  0.92627615 -0.46392784]\n","Worker 1 processing data batch 491\n","Worker 1 gradients (layer 0 sample): [ 0.0631755  -0.7505968   0.29037526  0.690436   -0.4921291 ]\n","Aggregated gradients (layer 0 sample): [ 0.01429706 -0.7124392   0.25079182  0.80835605 -0.47802848]\n","Parameters before update (layer 0 sample): [0.14392768 0.00717782 0.15575883 0.04945097 0.0336169 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1429279  0.00817781 0.15475884 0.04845098 0.03461689]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 492\n","Worker 0 gradients (layer 0 sample): [ 0.44895276  1.4683447  -1.0545932  -4.0709267   0.26603022]\n","Worker 1 processing data batch 492\n","Worker 1 gradients (layer 0 sample): [ 0.2739994   0.36222273 -0.5868679  -2.0010428  -0.45636898]\n","Aggregated gradients (layer 0 sample): [ 0.36147606  0.9152837  -0.82073057 -3.0359848  -0.09516938]\n","Parameters before update (layer 0 sample): [0.1429279  0.00817781 0.15475884 0.04845098 0.03461689]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192791 0.00717782 0.15575883 0.04945097 0.03561685]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 493\n","Worker 0 gradients (layer 0 sample): [-0.39833546  0.11639157  0.04815826  1.657761    1.1667933 ]\n","Worker 1 processing data batch 493\n","Worker 1 gradients (layer 0 sample): [-0.4741421  -0.34782305  0.9508046   4.453446    1.4796212 ]\n","Aggregated gradients (layer 0 sample): [-0.43623877 -0.11571574  0.49948144  3.0556035   1.3232073 ]\n","Parameters before update (layer 0 sample): [0.14192791 0.00717782 0.15575883 0.04945097 0.03561685]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1429279  0.00817778 0.15475884 0.04845098 0.03461686]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 494\n","Worker 0 gradients (layer 0 sample): [ 0.45691752 -0.181182   -0.67923397 -2.5852332  -0.57688123]\n","Worker 1 processing data batch 494\n","Worker 1 gradients (layer 0 sample): [ 0.13771716 -0.18946938 -0.17512268 -1.0773749  -0.32238042]\n","Aggregated gradients (layer 0 sample): [ 0.29731733 -0.18532568 -0.42717832 -1.8313041  -0.44963083]\n","Parameters before update (layer 0 sample): [0.1429279  0.00817778 0.15475884 0.04845098 0.03461686]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192791 0.00917776 0.15575883 0.04945097 0.03561685]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 495\n","Worker 0 gradients (layer 0 sample): [-0.15179497 -0.67857283  0.34751052  1.272639   -0.438214  ]\n","Worker 1 processing data batch 495\n","Worker 1 gradients (layer 0 sample): [-0.3609524   0.31191418  0.2266534   1.8008488   0.70303124]\n","Aggregated gradients (layer 0 sample): [-0.2563737  -0.18332933  0.28708196  1.5367439   0.13240862]\n","Parameters before update (layer 0 sample): [0.14192791 0.00917776 0.15575883 0.04945097 0.03561685]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1429279  0.01017773 0.15475884 0.04845098 0.03461688]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 496\n","Worker 0 gradients (layer 0 sample): [-0.09223196  3.2011805  -0.5878645  -1.4678319   1.1006268 ]\n","Worker 1 processing data batch 496\n","Worker 1 gradients (layer 0 sample): [-0.47509354  2.5141764  -0.3319823  -0.22327429  1.2805945 ]\n","Aggregated gradients (layer 0 sample): [-0.28366274  2.8576784  -0.45992342 -0.84555304  1.1906106 ]\n","Parameters before update (layer 0 sample): [0.1429279  0.01017773 0.15475884 0.04845098 0.03461688]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392789 0.00917774 0.15575883 0.04945097 0.03361689]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 497\n","Worker 0 gradients (layer 0 sample): [-0.05526681 -1.0112082   0.80113083  2.0944486  -0.41515565]\n","Worker 1 processing data batch 497\n","Worker 1 gradients (layer 0 sample): [ 0.18621217 -2.8853257   0.81706965  2.3136237  -0.9430768 ]\n","Aggregated gradients (layer 0 sample): [ 0.06547268 -1.948267    0.8091003   2.2040362  -0.67911625]\n","Parameters before update (layer 0 sample): [0.14392789 0.00917774 0.15575883 0.04945097 0.03361689]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292794 0.01017773 0.15475884 0.04845098 0.03461688]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 498\n","Worker 0 gradients (layer 0 sample): [ 0.2654258   2.50836    -0.9094163  -3.7805738   0.02912289]\n","Worker 1 processing data batch 498\n","Worker 1 gradients (layer 0 sample): [-0.00148661  0.71095276 -0.40080807 -1.0432512  -0.12624824]\n","Aggregated gradients (layer 0 sample): [ 0.1319696   1.6096563  -0.6551122  -2.4119124  -0.04856268]\n","Parameters before update (layer 0 sample): [0.14292794 0.01017773 0.15475884 0.04845098 0.03461688]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192797 0.00917774 0.15575883 0.04945097 0.0356168 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 499\n","Worker 0 gradients (layer 0 sample): [-0.37835228 -0.96208215  1.2460732   4.6710734  -0.04447345]\n","Worker 1 processing data batch 499\n","Worker 1 gradients (layer 0 sample): [-0.73211026 -2.4369755   1.7677107   7.862733    0.36239934]\n","Aggregated gradients (layer 0 sample): [-0.5552313  -1.6995288   1.506892    6.266903    0.15896294]\n","Parameters before update (layer 0 sample): [0.14192797 0.00917774 0.15575883 0.04945097 0.0356168 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292796 0.01017773 0.15475884 0.04845098 0.03461683]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 500\n","Worker 0 gradients (layer 0 sample): [ 0.32005835 -0.10011618 -0.54007053 -2.15727    -0.81033826]\n","Worker 1 processing data batch 500\n","Worker 1 gradients (layer 0 sample): [ 0.01733713  3.124529   -0.6677934  -2.426339    0.699054  ]\n","Aggregated gradients (layer 0 sample): [ 0.16869773  1.5122063  -0.60393196 -2.2918043  -0.05564213]\n","Parameters before update (layer 0 sample): [0.14292796 0.01017773 0.15475884 0.04845098 0.03461683]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192799 0.00917774 0.15575883 0.04945097 0.03561677]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 500, Losses: [1.865494, 2.1176991]\n","Worker 0 processing data batch 501\n","Worker 0 gradients (layer 0 sample): [-0.4567927   0.44803786  0.32254395  2.5080624   0.8025184 ]\n","Worker 1 processing data batch 501\n","Worker 1 gradients (layer 0 sample): [ 0.06556594 -1.7315712   0.32289076  1.1180265  -0.746958  ]\n","Aggregated gradients (layer 0 sample): [-0.19561338 -0.64176667  0.32271737  1.8130444   0.02778021]\n","Parameters before update (layer 0 sample): [0.14192799 0.00917774 0.15575883 0.04945097 0.03561677]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292796 0.01017773 0.15475884 0.04845098 0.03461689]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 502\n","Worker 0 gradients (layer 0 sample): [ 0.22105251  1.1052713  -0.33266047 -1.5113435   0.10786825]\n","Worker 1 processing data batch 502\n","Worker 1 gradients (layer 0 sample): [ 0.22341529  3.1068487  -1.2212758  -4.2427516   0.31711826]\n","Aggregated gradients (layer 0 sample): [ 0.22223389  2.10606    -0.7769681  -2.8770475   0.21249326]\n","Parameters before update (layer 0 sample): [0.14292796 0.01017773 0.15475884 0.04845098 0.03461689]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192799 0.00917774 0.15575883 0.04945097 0.03361691]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 503\n","Worker 0 gradients (layer 0 sample): [-0.14492063 -1.1019028   0.6278628   2.6874561  -0.11617823]\n","Worker 1 processing data batch 503\n","Worker 1 gradients (layer 0 sample): [-0.27156818  0.196897    0.54148155  2.0585513   0.3289525 ]\n","Aggregated gradients (layer 0 sample): [-0.20824441 -0.4525029   0.5846722   2.3730037   0.10638713]\n","Parameters before update (layer 0 sample): [0.14192799 0.00917774 0.15575883 0.04945097 0.03361691]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292796 0.01017772 0.15475884 0.04845098 0.03261694]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 504\n","Worker 0 gradients (layer 0 sample): [ 0.06860265  2.4790146  -0.6402345  -2.2291405   0.18953091]\n","Worker 1 processing data batch 504\n","Worker 1 gradients (layer 0 sample): [ 0.05812737  0.79388547 -0.15320413 -1.0316939  -0.02526332]\n","Aggregated gradients (layer 0 sample): [ 0.06336501  1.63645    -0.3967193  -1.6304172   0.08213379]\n","Parameters before update (layer 0 sample): [0.14292796 0.01017772 0.15475884 0.04845098 0.03261694]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192802 0.00917773 0.15575883 0.04945097 0.03161699]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 505\n","Worker 0 gradients (layer 0 sample): [ 0.10783412 -2.2943845   0.53649163  1.5997099  -0.35797217]\n","Worker 1 processing data batch 505\n","Worker 1 gradients (layer 0 sample): [ 0.00422609 -0.9441292   0.40735966  1.3690751  -0.2654638 ]\n","Aggregated gradients (layer 0 sample): [ 0.0560301  -1.6192569   0.47192565  1.4843924  -0.311718  ]\n","Parameters before update (layer 0 sample): [0.14192802 0.00917773 0.15575883 0.04945097 0.03161699]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14092807 0.01017772 0.15475884 0.04845098 0.03261697]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 506\n","Worker 0 gradients (layer 0 sample): [-0.04390649  1.4261775  -0.25596976 -0.49641812  0.1086524 ]\n","Worker 1 processing data batch 506\n","Worker 1 gradients (layer 0 sample): [-0.23869792  2.497953   -0.11353429 -0.05109158  0.29381993]\n","Aggregated gradients (layer 0 sample): [-0.1413022   1.9620652  -0.18475202 -0.27375484  0.20123616]\n","Parameters before update (layer 0 sample): [0.14092807 0.01017772 0.15475884 0.04845098 0.03261697]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192805 0.00917773 0.15575881 0.04945096 0.031617  ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 507\n","Worker 0 gradients (layer 0 sample): [-0.00754878 -1.97946     0.87922794  2.5527582  -0.2477376 ]\n","Worker 1 processing data batch 507\n","Worker 1 gradients (layer 0 sample): [-0.01400004 -0.86450803  0.4767332   1.2191379  -0.17521635]\n","Aggregated gradients (layer 0 sample): [-0.01077441 -1.421984    0.67798054  1.8859481  -0.21147698]\n","Parameters before update (layer 0 sample): [0.14192805 0.00917773 0.15575881 0.04945096 0.031617  ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292775 0.01017772 0.15475883 0.04845097 0.03261698]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 508\n","Worker 0 gradients (layer 0 sample): [ 0.2963305   1.2883524  -0.5200787  -2.1829872  -0.05646469]\n","Worker 1 processing data batch 508\n","Worker 1 gradients (layer 0 sample): [ 0.38759953  1.3576322  -0.6387294  -2.844288   -0.16168594]\n","Aggregated gradients (layer 0 sample): [ 0.34196502  1.3229923  -0.57940406 -2.5136375  -0.10907532]\n","Parameters before update (layer 0 sample): [0.14292775 0.01017772 0.15475883 0.04845097 0.03261698]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192776 0.00917773 0.15575881 0.04945096 0.03361694]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 509\n","Worker 0 gradients (layer 0 sample): [-0.32620898 -0.59044766  0.5798133   3.0172303  -0.10297859]\n","Worker 1 processing data batch 509\n","Worker 1 gradients (layer 0 sample): [-0.01159    -1.9312818   0.7723657   2.5057418  -0.49248588]\n","Aggregated gradients (layer 0 sample): [-0.16889949 -1.2608647   0.6760895   2.761486   -0.29773223]\n","Parameters before update (layer 0 sample): [0.14192776 0.00917773 0.15575881 0.04945096 0.03361694]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292774 0.01017772 0.15475883 0.04845097 0.03461692]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 510\n","Worker 0 gradients (layer 0 sample): [-0.2542728   1.279485   -0.03003757  0.15897778  0.42833027]\n","Worker 1 processing data batch 510\n","Worker 1 gradients (layer 0 sample): [-0.05910921  3.4309578  -1.0213687  -2.9164114   0.5725138 ]\n","Aggregated gradients (layer 0 sample): [-0.156691    2.3552213  -0.52570313 -1.3787168   0.50042206]\n","Parameters before update (layer 0 sample): [0.14292774 0.01017772 0.15475883 0.04845097 0.03461692]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392771 0.00917773 0.15575881 0.04945096 0.03361694]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 510, Losses: [1.9291575, 1.7531934]\n","Worker 0 processing data batch 511\n","Worker 0 gradients (layer 0 sample): [-0.13129245 -1.3837391   1.1344655   3.7280498  -0.16112386]\n","Worker 1 processing data batch 511\n","Worker 1 gradients (layer 0 sample): [-0.172282   -0.89667463  0.867322    2.5926552  -0.06650102]\n","Aggregated gradients (layer 0 sample): [-0.15178722 -1.1402068   1.0008937   3.1603525  -0.11381244]\n","Parameters before update (layer 0 sample): [0.14392771 0.00917773 0.15575881 0.04945096 0.03361694]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492768 0.01017772 0.15475883 0.04845097 0.0346169 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 512\n","Worker 0 gradients (layer 0 sample): [ 0.14340347  4.675399   -0.8355935  -3.235069    1.1222658 ]\n","Worker 1 processing data batch 512\n","Worker 1 gradients (layer 0 sample): [ 0.1748116   2.682694   -0.8952613  -2.8641717   0.36146805]\n","Aggregated gradients (layer 0 sample): [ 0.15910754  3.6790464  -0.8654274  -3.0496204   0.74186695]\n","Parameters before update (layer 0 sample): [0.14492768 0.01017772 0.15475883 0.04845097 0.0346169 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392771 0.00917773 0.15575881 0.04945096 0.03361691]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 513\n","Worker 0 gradients (layer 0 sample): [-0.03376684 -1.494859    0.8141381   2.5605123  -0.41490716]\n","Worker 1 processing data batch 513\n","Worker 1 gradients (layer 0 sample): [-0.3784968  -1.4352074   1.30406     5.198327   -0.04077639]\n","Aggregated gradients (layer 0 sample): [-0.20613182 -1.4650332   1.0590991   3.8794198  -0.22784176]\n","Parameters before update (layer 0 sample): [0.14392771 0.00917773 0.15575881 0.04945096 0.03361691]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492768 0.01017772 0.15475883 0.04845097 0.03461689]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 514\n","Worker 0 gradients (layer 0 sample): [ 0.18198705  1.1409717  -0.4910914  -1.9198891  -0.04980683]\n","Worker 1 processing data batch 514\n","Worker 1 gradients (layer 0 sample): [ 0.4494506   2.291567   -1.0012875  -3.7503426   0.04622974]\n","Aggregated gradients (layer 0 sample): [ 3.1571883e-01  1.7162694e+00 -7.4618942e-01 -2.8351159e+00\n"," -1.7885491e-03]\n","Parameters before update (layer 0 sample): [0.14492768 0.01017772 0.15475883 0.04845097 0.03461689]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1439277  0.00917773 0.15575881 0.04945096 0.03561512]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 515\n","Worker 0 gradients (layer 0 sample): [-0.5324753 -0.1118966  0.7489455  3.8198736  0.8253702]\n","Worker 1 processing data batch 515\n","Worker 1 gradients (layer 0 sample): [-0.40446496 -1.5214183   1.0658878   5.002982    0.06870814]\n","Aggregated gradients (layer 0 sample): [-0.46847013 -0.8166575   0.90741664  4.411428    0.44703916]\n","Parameters before update (layer 0 sample): [0.1439277  0.00917773 0.15575881 0.04945096 0.03561512]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492768 0.01017772 0.15475883 0.04845097 0.03461513]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 516\n","Worker 0 gradients (layer 0 sample): [ 0.3687664  2.4203176 -0.8101729 -3.263287   0.114083 ]\n","Worker 1 processing data batch 516\n","Worker 1 gradients (layer 0 sample): [ 0.44923243  2.0041454  -0.77306885 -3.5475602  -0.35185334]\n","Aggregated gradients (layer 0 sample): [ 0.4089994   2.2122316  -0.79162085 -3.4054236  -0.11888517]\n","Parameters before update (layer 0 sample): [0.14492768 0.01017772 0.15475883 0.04845097 0.03461513]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1439277  0.00917772 0.15575881 0.04945096 0.0356151 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 517\n","Worker 0 gradients (layer 0 sample): [ 0.38896954 -2.289611    0.6877739   0.8077171  -1.4444711 ]\n","Worker 1 processing data batch 517\n","Worker 1 gradients (layer 0 sample): [-0.2671708  -1.5112783   1.3297383   5.268757    0.10523845]\n","Aggregated gradients (layer 0 sample): [ 0.06089938 -1.9004447   1.008756    3.038237   -0.66961634]\n","Parameters before update (layer 0 sample): [0.1439277  0.00917772 0.15575881 0.04945096 0.0356151 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292775 0.01017772 0.15475883 0.04845097 0.03661509]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 518\n","Worker 0 gradients (layer 0 sample): [ 0.19983411  3.6481862  -1.0773163  -3.9683      0.9970879 ]\n","Worker 1 processing data batch 518\n","Worker 1 gradients (layer 0 sample): [ 0.14006282  3.1467252  -0.75469166 -3.2190487   0.8824312 ]\n","Aggregated gradients (layer 0 sample): [ 0.16994846  3.3974557  -0.91600394 -3.5936744   0.93975955]\n","Parameters before update (layer 0 sample): [0.14292775 0.01017772 0.15475883 0.04845097 0.03661509]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14192778 0.00917772 0.15575881 0.04945096 0.0356151 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 519\n","Worker 0 gradients (layer 0 sample): [-0.24471658 -1.0630661   1.0633035   4.141896    0.10974894]\n","Worker 1 processing data batch 519\n","Worker 1 gradients (layer 0 sample): [-0.2568759  -1.6054878   0.77238417  3.3967152  -0.6409199 ]\n","Aggregated gradients (layer 0 sample): [-0.25079626 -1.3342769   0.9178438   3.7693055  -0.26558548]\n","Parameters before update (layer 0 sample): [0.14192778 0.00917772 0.15575881 0.04945096 0.0356151 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292777 0.01017771 0.15475883 0.04845097 0.03661508]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 520\n","Worker 0 gradients (layer 0 sample): [-0.48758525  2.0783904  -0.06820772  0.5599275   1.137481  ]\n","Worker 1 processing data batch 520\n","Worker 1 gradients (layer 0 sample): [-0.39414868  2.7620683  -0.08484922  0.32870102  1.4003806 ]\n","Aggregated gradients (layer 0 sample): [-0.44086695  2.4202294  -0.07652847  0.44431427  1.2689308 ]\n","Parameters before update (layer 0 sample): [0.14292777 0.01017771 0.15475883 0.04845097 0.03661508]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392775 0.00917772 0.15575878 0.04745098 0.03561509]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 520, Losses: [1.7268716, 1.9771826]\n","Worker 0 processing data batch 521\n","Worker 0 gradients (layer 0 sample): [ 0.48422578 -0.01511773 -0.3141321  -1.9901637  -0.6677813 ]\n","Worker 1 processing data batch 521\n","Worker 1 gradients (layer 0 sample): [ 0.5875139  -1.6172183   0.46558976  0.09257352 -0.89622456]\n","Aggregated gradients (layer 0 sample): [ 0.53586984 -0.816168    0.07572883 -0.9487951  -0.7820029 ]\n","Parameters before update (layer 0 sample): [0.14392775 0.00917772 0.15575878 0.04745098 0.03561509]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292777 0.01017771 0.15475883 0.04845097 0.03661508]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 522\n","Worker 0 gradients (layer 0 sample): [-0.19504228  0.8593228  -0.39233828 -0.30839705  0.4259824 ]\n","Worker 1 processing data batch 522\n","Worker 1 gradients (layer 0 sample): [ 0.06612539  0.87400675 -0.8218676  -2.12513    -0.18683992]\n","Aggregated gradients (layer 0 sample): [-0.06445844  0.86666477 -0.60710293 -1.2167635   0.11957123]\n","Parameters before update (layer 0 sample): [0.14292777 0.01017771 0.15475883 0.04845097 0.03661508]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392771 0.00917772 0.15575881 0.04945096 0.03561511]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 523\n","Worker 0 gradients (layer 0 sample): [-0.8541221  -1.4281417   2.4075942   9.448951    0.66875243]\n","Worker 1 processing data batch 523\n","Worker 1 gradients (layer 0 sample): [-0.27571425 -1.7080263   1.9858222   5.903449   -0.24579318]\n","Aggregated gradients (layer 0 sample): [-0.56491816 -1.568084    2.1967082   7.6762      0.21147963]\n","Parameters before update (layer 0 sample): [0.14392771 0.00917772 0.15575881 0.04945096 0.03561511]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449277  0.01017771 0.15475883 0.04845097 0.03461513]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 524\n","Worker 0 gradients (layer 0 sample): [ 0.35615212  0.9449834  -0.6367606  -2.4409404  -0.3645212 ]\n","Worker 1 processing data batch 524\n","Worker 1 gradients (layer 0 sample): [ 0.0313217   0.6691594  -0.3797476  -1.3168397   0.01564011]\n","Aggregated gradients (layer 0 sample): [ 0.19373691  0.80707145 -0.5082541  -1.87889    -0.17444055]\n","Parameters before update (layer 0 sample): [0.1449277  0.01017771 0.15475883 0.04845097 0.03461513]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392772 0.00917773 0.15575881 0.04945096 0.03561511]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 525\n","Worker 0 gradients (layer 0 sample): [-0.40936255 -1.2862598   1.7690669   6.945649    0.19953123]\n","Worker 1 processing data batch 525\n","Worker 1 gradients (layer 0 sample): [-0.3717924  -1.6972569   1.5655951   6.07273     0.10503577]\n","Aggregated gradients (layer 0 sample): [-0.3905775  -1.4917583   1.667331    6.5091896   0.15228349]\n","Parameters before update (layer 0 sample): [0.14392772 0.00917773 0.15575881 0.04945096 0.03561511]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492771 0.01017772 0.15475883 0.04845097 0.03461513]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 526\n","Worker 0 gradients (layer 0 sample): [-0.33044845  1.0116922  -0.03329317  0.7745015   0.40120736]\n","Worker 1 processing data batch 526\n","Worker 1 gradients (layer 0 sample): [ 0.1156496   2.2199912  -0.6939058  -2.3581207  -0.02554578]\n","Aggregated gradients (layer 0 sample): [-0.10739943  1.6158416  -0.36359948 -0.79180956  0.18783079]\n","Parameters before update (layer 0 sample): [0.14492771 0.01017772 0.15475883 0.04845097 0.03461513]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592767 0.00917772 0.15575881 0.04945096 0.03361516]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 527\n","Worker 0 gradients (layer 0 sample): [-0.34692532 -1.4614952   1.4149446   5.2549114  -0.03278347]\n","Worker 1 processing data batch 527\n","Worker 1 gradients (layer 0 sample): [ 0.04524009 -2.405252    1.0503182   3.0943174  -0.42399067]\n","Aggregated gradients (layer 0 sample): [-0.1508426  -1.9333736   1.2326314   4.1746144  -0.22838707]\n","Parameters before update (layer 0 sample): [0.14592767 0.00917772 0.15575881 0.04945096 0.03361516]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692764 0.01017772 0.15475883 0.04845097 0.03461514]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 528\n","Worker 0 gradients (layer 0 sample): [ 0.50207806  1.1500778  -0.64061326 -3.0711207  -0.20614797]\n","Worker 1 processing data batch 528\n","Worker 1 gradients (layer 0 sample): [ 0.22183718  1.2669536  -0.5412614  -2.160283   -0.325058  ]\n","Aggregated gradients (layer 0 sample): [ 0.3619576  1.2085156 -0.5909373 -2.615702  -0.265603 ]\n","Parameters before update (layer 0 sample): [0.14692764 0.01017772 0.15475883 0.04845097 0.03461514]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592765 0.00917773 0.15575881 0.04945096 0.03561512]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 529\n","Worker 0 gradients (layer 0 sample): [-0.33311102 -0.77071035  1.1196887   4.089679    0.11458636]\n","Worker 1 processing data batch 529\n","Worker 1 gradients (layer 0 sample): [-0.05029843 -2.68801     1.4862635   5.312498   -0.6117883 ]\n","Aggregated gradients (layer 0 sample): [-0.19170472 -1.7293601   1.3029761   4.7010884  -0.24860096]\n","Parameters before update (layer 0 sample): [0.14592765 0.00917773 0.15575881 0.04945096 0.03561512]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692762 0.01017772 0.15475883 0.04845097 0.0366151 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 530\n","Worker 0 gradients (layer 0 sample): [ 0.62356985  3.1683087  -1.4050088  -5.5678205   0.210498  ]\n","Worker 1 processing data batch 530\n","Worker 1 gradients (layer 0 sample): [ 0.5110987   1.4922459  -1.0041907  -4.6122117  -0.17928633]\n","Aggregated gradients (layer 0 sample): [ 0.5673343   2.3302774  -1.2045997  -5.0900164   0.01560584]\n","Parameters before update (layer 0 sample): [0.14692762 0.01017772 0.15475883 0.04845097 0.0366151 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592764 0.00917773 0.15575881 0.04945096 0.03561531]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 530, Losses: [1.99753, 1.9052049]\n","Worker 0 processing data batch 531\n","Worker 0 gradients (layer 0 sample): [ 0.00863649 -1.8337145   1.3420691   4.117861   -0.3906315 ]\n","Worker 1 processing data batch 531\n","Worker 1 gradients (layer 0 sample): [-0.12084188 -1.5129927   0.8662406   3.242117   -0.22751226]\n","Aggregated gradients (layer 0 sample): [-0.05610269 -1.6733537   1.1041548   3.6799889  -0.30907187]\n","Parameters before update (layer 0 sample): [0.14592764 0.00917773 0.15575881 0.04945096 0.03561531]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692758 0.01017772 0.15475883 0.04845097 0.03661529]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 532\n","Worker 0 gradients (layer 0 sample): [ 0.29721278  3.5004363  -1.1117544  -3.7643893   1.0396383 ]\n","Worker 1 processing data batch 532\n","Worker 1 gradients (layer 0 sample): [-0.1300842  1.7594986 -0.8462272 -1.6939044  0.7241547]\n","Aggregated gradients (layer 0 sample): [ 0.08356429  2.6299675  -0.9789908  -2.729147    0.8818965 ]\n","Parameters before update (layer 0 sample): [0.14692758 0.01017772 0.15475883 0.04845097 0.03661529]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592762 0.00917773 0.15575881 0.04945096 0.0356153 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 533\n","Worker 0 gradients (layer 0 sample): [-0.36688566 -1.5017347   1.2572365   4.6867867  -0.05400866]\n","Worker 1 processing data batch 533\n","Worker 1 gradients (layer 0 sample): [-0.01315051  0.19972122  0.08005663  0.6024733   0.2095423 ]\n","Aggregated gradients (layer 0 sample): [-0.19001809 -0.65100676  0.6686466   2.64463     0.07776682]\n","Parameters before update (layer 0 sample): [0.14592762 0.00917773 0.15575881 0.04945096 0.0356153 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469276  0.01017771 0.15475883 0.04845097 0.03461535]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 534\n","Worker 0 gradients (layer 0 sample): [ 0.02935281  1.28767    -0.48274112 -1.1999444   0.12300321]\n","Worker 1 processing data batch 534\n","Worker 1 gradients (layer 0 sample): [ 0.18258789  0.38629684 -0.26509607 -1.4682608  -0.27980733]\n","Aggregated gradients (layer 0 sample): [ 0.10597035  0.83698344 -0.3739186  -1.3341026  -0.07840206]\n","Parameters before update (layer 0 sample): [0.1469276  0.01017771 0.15475883 0.04845097 0.03461535]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592764 0.00917772 0.15575881 0.04945096 0.0356153 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 535\n","Worker 0 gradients (layer 0 sample): [-0.05985821 -0.5376011   0.57116723  1.8017907  -0.37893057]\n","Worker 1 processing data batch 535\n","Worker 1 gradients (layer 0 sample): [-0.10383317 -1.1648883   0.46756864  1.6225326  -0.14912021]\n","Aggregated gradients (layer 0 sample): [-0.08184569 -0.8512447   0.51936793  1.7121617  -0.2640254 ]\n","Parameters before update (layer 0 sample): [0.14592764 0.00917772 0.15575881 0.04945096 0.0356153 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469276  0.01017771 0.15475883 0.04845097 0.03661528]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 536\n","Worker 0 gradients (layer 0 sample): [ 0.2774987   2.4356012  -1.0374079  -3.4199889   0.46404696]\n","Worker 1 processing data batch 536\n","Worker 1 gradients (layer 0 sample): [ 0.06424823  4.1791835  -1.1676801  -3.7624106   1.4818399 ]\n","Aggregated gradients (layer 0 sample): [ 0.17087346  3.3073924  -1.1025441  -3.5911999   0.9729434 ]\n","Parameters before update (layer 0 sample): [0.1469276  0.01017771 0.15475883 0.04845097 0.03661528]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592762 0.00917772 0.15575881 0.04945096 0.03561529]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 537\n","Worker 0 gradients (layer 0 sample): [-0.24141672 -1.8576355   1.4237044   4.736833   -0.26349375]\n","Worker 1 processing data batch 537\n","Worker 1 gradients (layer 0 sample): [-0.24361989 -0.6151879   0.65986955  2.7619925   0.17390218]\n","Aggregated gradients (layer 0 sample): [-0.2425183  -1.2364117   1.0417869   3.7494128  -0.04479578]\n","Parameters before update (layer 0 sample): [0.14592762 0.00917772 0.15575881 0.04945096 0.03561529]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692761 0.01017771 0.15475883 0.04845097 0.03661521]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 538\n","Worker 0 gradients (layer 0 sample): [ 0.73925805  0.15570378 -0.8136012  -3.7910657  -0.85668504]\n","Worker 1 processing data batch 538\n","Worker 1 gradients (layer 0 sample): [-0.2568136   3.9873388  -0.83746314 -2.0147016   1.3429909 ]\n","Aggregated gradients (layer 0 sample): [ 0.24122223  2.0715213  -0.8255322  -2.9028835   0.24315292]\n","Parameters before update (layer 0 sample): [0.14692761 0.01017771 0.15475883 0.04845097 0.03661521]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592762 0.00917772 0.15575881 0.04945096 0.03561523]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 539\n","Worker 0 gradients (layer 0 sample): [ 0.17170478 -1.6136469   0.8946072   1.8233888  -0.61473715]\n","Worker 1 processing data batch 539\n","Worker 1 gradients (layer 0 sample): [-0.29625013 -1.6487527   1.2723238   4.5762706  -0.2622114 ]\n","Aggregated gradients (layer 0 sample): [-0.06227268 -1.6311998   1.0834656   3.1998296  -0.4384743 ]\n","Parameters before update (layer 0 sample): [0.14592762 0.00917772 0.15575881 0.04945096 0.03561523]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692757 0.01017771 0.15475883 0.04845097 0.03661522]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 540\n","Worker 0 gradients (layer 0 sample): [-0.20637079  4.897188   -1.1822468  -3.2318952   1.7909744 ]\n","Worker 1 processing data batch 540\n","Worker 1 gradients (layer 0 sample): [-0.17926247  1.1321118  -0.12840603 -0.30723533  0.63649887]\n","Aggregated gradients (layer 0 sample): [-0.19281663  3.0146499  -0.6553264  -1.7695652   1.2137367 ]\n","Parameters before update (layer 0 sample): [0.14692757 0.01017771 0.15475883 0.04845097 0.03661522]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792754 0.00917772 0.15575881 0.04945096 0.03561523]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 540, Losses: [1.8603966, 1.9354858]\n","Worker 0 processing data batch 541\n","Worker 0 gradients (layer 0 sample): [-0.00319618 -2.2997885   0.9925372   2.8934429  -0.66796553]\n","Worker 1 processing data batch 541\n","Worker 1 gradients (layer 0 sample): [ 0.47899508 -2.212655    0.6871274   0.25773567 -1.2963244 ]\n","Aggregated gradients (layer 0 sample): [ 0.23789945 -2.2562218   0.8398323   1.5755893  -0.98214495]\n","Parameters before update (layer 0 sample): [0.14792754 0.00917772 0.15575881 0.04945096 0.03561523]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692755 0.01017771 0.15475883 0.04845097 0.03661522]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 542\n","Worker 0 gradients (layer 0 sample): [ 0.20176266  2.0630376  -0.9351901  -3.0059588   0.1932159 ]\n","Worker 1 processing data batch 542\n","Worker 1 gradients (layer 0 sample): [-0.04346259  3.313762   -1.1944423  -3.0588355   1.2915329 ]\n","Aggregated gradients (layer 0 sample): [ 0.07915004  2.6883998  -1.0648162  -3.0323973   0.7423744 ]\n","Parameters before update (layer 0 sample): [0.14692755 0.01017771 0.15475883 0.04845097 0.03661522]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1459276  0.00917772 0.15575881 0.04945096 0.03561523]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 543\n","Worker 0 gradients (layer 0 sample): [-0.58696604 -1.0020161   1.4870989   6.2152796   0.37930983]\n","Worker 1 processing data batch 543\n","Worker 1 gradients (layer 0 sample): [-0.383417   -0.37568825  0.9869267   4.1508446   0.51535094]\n","Aggregated gradients (layer 0 sample): [-0.48519152 -0.6888522   1.2370129   5.183062    0.4473304 ]\n","Parameters before update (layer 0 sample): [0.1459276  0.00917772 0.15575881 0.04945096 0.03561523]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692758 0.01017771 0.15475883 0.04845097 0.03461524]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 544\n","Worker 0 gradients (layer 0 sample): [ 0.40793914  0.42684084 -0.84215814 -3.414705   -0.5070762 ]\n","Worker 1 processing data batch 544\n","Worker 1 gradients (layer 0 sample): [-0.07054795  3.3401902  -0.57977587 -2.1605992   0.3632353 ]\n","Aggregated gradients (layer 0 sample): [ 0.1686956   1.8835155  -0.710967   -2.787652   -0.07192045]\n","Parameters before update (layer 0 sample): [0.14692758 0.01017771 0.15475883 0.04845097 0.03461524]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592761 0.00917772 0.15575881 0.04945096 0.03561519]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 545\n","Worker 0 gradients (layer 0 sample): [ 0.04542971 -2.1700578   1.3271914   4.0094     -0.41665873]\n","Worker 1 processing data batch 545\n","Worker 1 gradients (layer 0 sample): [-0.20659745 -1.5863168   1.4369018   4.748197   -0.13346763]\n","Aggregated gradients (layer 0 sample): [-0.08058387 -1.8781873   1.3820466   4.3787985  -0.2750632 ]\n","Parameters before update (layer 0 sample): [0.14592761 0.00917772 0.15575881 0.04945096 0.03561519]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692757 0.01017771 0.15475883 0.04845097 0.03661517]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 546\n","Worker 0 gradients (layer 0 sample): [-0.07742292  3.736826   -0.81877136 -2.6677723   0.94080645]\n","Worker 1 processing data batch 546\n","Worker 1 gradients (layer 0 sample): [ 0.05028296  3.005199   -0.8413944  -2.6165314   0.83050305]\n","Aggregated gradients (layer 0 sample): [-0.01356998  3.3710124  -0.8300829  -2.6421518   0.88565475]\n","Parameters before update (layer 0 sample): [0.14692757 0.01017771 0.15475883 0.04845097 0.03661517]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792733 0.00917772 0.15575881 0.04945096 0.03561518]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 547\n","Worker 0 gradients (layer 0 sample): [-0.16639306 -2.6928465   1.7406101   5.877263   -0.30588698]\n","Worker 1 processing data batch 547\n","Worker 1 gradients (layer 0 sample): [-0.5745247  -1.934535    1.5076594   6.764675    0.47079888]\n","Aggregated gradients (layer 0 sample): [-0.37045887 -2.3136907   1.6241348   6.320969    0.08245595]\n","Parameters before update (layer 0 sample): [0.14792733 0.00917772 0.15575881 0.04945096 0.03561518]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892732 0.01017771 0.15475883 0.04845096 0.03461523]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 548\n","Worker 0 gradients (layer 0 sample): [ 0.8089944   1.1961273  -0.6515393  -3.893431   -0.57886845]\n","Worker 1 processing data batch 548\n","Worker 1 gradients (layer 0 sample): [ 0.54869145  2.3155985  -1.3667352  -4.796851   -0.35507643]\n","Aggregated gradients (layer 0 sample): [ 0.6788429   1.755863   -1.0091373  -4.345141   -0.46697244]\n","Parameters before update (layer 0 sample): [0.14892732 0.01017771 0.15475883 0.04845096 0.03461523]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792733 0.00917772 0.15575881 0.04945095 0.03561522]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 549\n","Worker 0 gradients (layer 0 sample): [-0.3900506  -1.8638923   1.56023     6.4587584   0.08456886]\n","Worker 1 processing data batch 549\n","Worker 1 gradients (layer 0 sample): [-0.31611335 -1.0879564   1.1566542   3.6476922   0.07056752]\n","Aggregated gradients (layer 0 sample): [-0.35308197 -1.4759244   1.3584421   5.0532255   0.07756819]\n","Parameters before update (layer 0 sample): [0.14792733 0.00917772 0.15575881 0.04945095 0.03561522]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892732 0.01017771 0.15475883 0.04845096 0.03461526]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 550\n","Worker 0 gradients (layer 0 sample): [ 0.32097936  1.1518869  -0.63201654 -2.9755983  -0.32750618]\n","Worker 1 processing data batch 550\n","Worker 1 gradients (layer 0 sample): [ 0.7301592   0.8851598  -1.0796368  -5.1123924  -0.74748254]\n","Aggregated gradients (layer 0 sample): [ 0.5255693   1.0185233  -0.8558267  -4.0439954  -0.53749436]\n","Parameters before update (layer 0 sample): [0.14892732 0.01017771 0.15475883 0.04845096 0.03461526]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792733 0.00917772 0.15575881 0.04945095 0.03561525]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 550, Losses: [1.8033597, 1.9497592]\n","Worker 0 processing data batch 551\n","Worker 0 gradients (layer 0 sample): [-0.770223  -1.1601732  1.7558503  8.308411   0.8236865]\n","Worker 1 processing data batch 551\n","Worker 1 gradients (layer 0 sample): [-0.59507245 -0.60911906  1.4987073   6.691058    0.6934398 ]\n","Aggregated gradients (layer 0 sample): [-0.6826477  -0.8846461   1.6272788   7.4997344   0.75856316]\n","Parameters before update (layer 0 sample): [0.14792733 0.00917772 0.15575881 0.04945095 0.03561525]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892732 0.01017771 0.15475883 0.04845096 0.03461526]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 552\n","Worker 0 gradients (layer 0 sample): [ 0.48823088  2.0893981  -0.7454692  -3.3255343   0.08556271]\n","Worker 1 processing data batch 552\n","Worker 1 gradients (layer 0 sample): [ 0.6507573   0.5995362  -0.84600633 -3.4761412  -0.32172692]\n","Aggregated gradients (layer 0 sample): [ 0.5694941   1.3444672  -0.79573774 -3.400838   -0.11808211]\n","Parameters before update (layer 0 sample): [0.14892732 0.01017771 0.15475883 0.04845096 0.03461526]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792733 0.00917772 0.15575881 0.04945095 0.03561523]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 553\n","Worker 0 gradients (layer 0 sample): [-0.40334028 -1.8842015   1.9253643   6.6245832  -0.19815153]\n","Worker 1 processing data batch 553\n","Worker 1 gradients (layer 0 sample): [ 0.08825292 -1.2529795   0.6234884   2.2637928  -0.6462736 ]\n","Aggregated gradients (layer 0 sample): [-0.15754369 -1.5685905   1.2744263   4.444188   -0.42221257]\n","Parameters before update (layer 0 sample): [0.14792733 0.00917772 0.15575881 0.04945095 0.03561523]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489273  0.01017771 0.15475883 0.04845096 0.03661522]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 554\n","Worker 0 gradients (layer 0 sample): [ 0.05484992  4.518732   -1.1127472  -3.3119597   1.7232323 ]\n","Worker 1 processing data batch 554\n","Worker 1 gradients (layer 0 sample): [-0.11813375  2.6545408  -0.5474982  -1.712722    0.9181045 ]\n","Aggregated gradients (layer 0 sample): [-0.03164192  3.5866365  -0.8301227  -2.5123408   1.3206685 ]\n","Parameters before update (layer 0 sample): [0.1489273  0.01017771 0.15475883 0.04845096 0.03661522]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499272  0.00917771 0.15575881 0.04945095 0.03561522]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 555\n","Worker 0 gradients (layer 0 sample): [-0.5544842 -1.0683566  1.3339537  5.893827   0.2758363]\n","Worker 1 processing data batch 555\n","Worker 1 gradients (layer 0 sample): [-0.27887666 -1.3977742   1.1703156   4.5541525  -0.04130641]\n","Aggregated gradients (layer 0 sample): [-0.41668043 -1.2330654   1.2521347   5.2239895   0.11726494]\n","Parameters before update (layer 0 sample): [0.1499272  0.00917771 0.15575881 0.04945095 0.03561522]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092719 0.01017771 0.15475883 0.04845096 0.03461526]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 556\n","Worker 0 gradients (layer 0 sample): [ 1.14491    -0.55273724 -0.64983404 -4.45726    -1.3162984 ]\n","Worker 1 processing data batch 556\n","Worker 1 gradients (layer 0 sample): [ 0.46955138  1.00079    -0.9121388  -3.6696799  -0.46010935]\n","Aggregated gradients (layer 0 sample): [ 0.8072307   0.22402638 -0.7809864  -4.06347    -0.88820386]\n","Parameters before update (layer 0 sample): [0.15092719 0.01017771 0.15475883 0.04845096 0.03461526]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499272  0.00917773 0.15575881 0.04945095 0.03561525]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 557\n","Worker 0 gradients (layer 0 sample): [-0.01902995 -0.67532796  0.21290304  0.7237561  -0.5822418 ]\n","Worker 1 processing data batch 557\n","Worker 1 gradients (layer 0 sample): [-0.45556176 -1.1020743   1.350537    5.3622446   0.5390432 ]\n","Aggregated gradients (layer 0 sample): [-0.23729585 -0.8887011   0.78172     3.0430002  -0.02159929]\n","Parameters before update (layer 0 sample): [0.1499272  0.00917773 0.15575881 0.04945095 0.03561525]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092719 0.01017772 0.15475883 0.04845096 0.0366151 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 558\n","Worker 0 gradients (layer 0 sample): [ 0.489672   4.1661186 -1.7306135 -5.6745     0.807179 ]\n","Worker 1 processing data batch 558\n","Worker 1 gradients (layer 0 sample): [ 0.10883843  2.8838403  -0.68953884 -2.5528316   0.6653354 ]\n","Aggregated gradients (layer 0 sample): [ 0.29925522  3.5249796  -1.2100761  -4.1136656   0.7362572 ]\n","Parameters before update (layer 0 sample): [0.15092719 0.01017772 0.15475883 0.04845096 0.0366151 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499272  0.00917772 0.15575881 0.04945095 0.03561511]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 559\n","Worker 0 gradients (layer 0 sample): [-0.30879122 -0.25584346  0.6381546   2.889391    0.2332155 ]\n","Worker 1 processing data batch 559\n","Worker 1 gradients (layer 0 sample): [-0.17827171 -0.6996856   0.7704263   2.9290376   0.13367265]\n","Aggregated gradients (layer 0 sample): [-0.24353147 -0.47776452  0.70429045  2.9092143   0.18344408]\n","Parameters before update (layer 0 sample): [0.1499272  0.00917772 0.15575881 0.04945095 0.03561511]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092719 0.01017771 0.15475883 0.04845096 0.03461513]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 560\n","Worker 0 gradients (layer 0 sample): [ 0.542438   1.3343875 -0.6814113 -3.9693947 -0.2960801]\n","Worker 1 processing data batch 560\n","Worker 1 gradients (layer 0 sample): [ 0.22833622  0.27130225 -0.2661564  -1.4910581  -0.29990605]\n","Aggregated gradients (layer 0 sample): [ 0.3853871   0.8028449  -0.47378385 -2.7302265  -0.29799306]\n","Parameters before update (layer 0 sample): [0.15092719 0.01017771 0.15475883 0.04845096 0.03461513]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499272  0.00917772 0.15575881 0.04945095 0.03561512]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 560, Losses: [1.9793698, 1.9202622]\n","Worker 0 processing data batch 561\n","Worker 0 gradients (layer 0 sample): [-0.49892285 -0.90694404  1.0280393   5.4398093   0.12349477]\n","Worker 1 processing data batch 561\n","Worker 1 gradients (layer 0 sample): [-0.32483202 -1.0150785   0.8201959   3.4641118  -0.13315171]\n","Aggregated gradients (layer 0 sample): [-0.41187745 -0.9610113   0.9241176   4.4519606  -0.00482847]\n","Parameters before update (layer 0 sample): [0.1499272  0.00917772 0.15575881 0.04945095 0.03561512]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092719 0.01017771 0.15475883 0.04845096 0.03661446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 562\n","Worker 0 gradients (layer 0 sample): [ 0.351268   2.6585748 -0.9092167 -3.9928904  0.4257608]\n","Worker 1 processing data batch 562\n","Worker 1 gradients (layer 0 sample): [ 0.36362416  1.6544325  -0.8940918  -3.195234   -0.05068485]\n","Aggregated gradients (layer 0 sample): [ 0.35744607  2.1565037  -0.90165424 -3.5940623   0.18753798]\n","Parameters before update (layer 0 sample): [0.15092719 0.01017771 0.15475883 0.04845096 0.03661446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499272  0.00917772 0.15575881 0.04945095 0.03561448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 563\n","Worker 0 gradients (layer 0 sample): [-0.3402205 -2.0287466  1.7461348  6.223839  -0.2974919]\n","Worker 1 processing data batch 563\n","Worker 1 gradients (layer 0 sample): [-0.12242597 -0.73762935  0.6026062   2.2305775   0.05958945]\n","Aggregated gradients (layer 0 sample): [-0.23132324 -1.383188    1.1743705   4.227208   -0.11895123]\n","Parameters before update (layer 0 sample): [0.1499272  0.00917772 0.15575881 0.04945095 0.03561448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092719 0.01017771 0.15475883 0.04845096 0.03661445]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 564\n","Worker 0 gradients (layer 0 sample): [ 0.46093014  1.7250224  -0.8460487  -3.7673626  -0.05066082]\n","Worker 1 processing data batch 564\n","Worker 1 gradients (layer 0 sample): [ 0.1011001  3.5190392 -0.7094263 -2.9947276  1.0529275]\n","Aggregated gradients (layer 0 sample): [ 0.28101513  2.6220307  -0.7777375  -3.381045    0.5011333 ]\n","Parameters before update (layer 0 sample): [0.15092719 0.01017771 0.15475883 0.04845096 0.03661445]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499272  0.00917772 0.15575881 0.04945095 0.03561446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 565\n","Worker 0 gradients (layer 0 sample): [-0.2239225  -1.8147545   1.1407893   4.2688236  -0.16885954]\n","Worker 1 processing data batch 565\n","Worker 1 gradients (layer 0 sample): [ 0.06152165 -2.322901    1.2387872   3.5211668  -0.9759382 ]\n","Aggregated gradients (layer 0 sample): [-0.08120043 -2.0688276   1.1897882   3.8949952  -0.5723989 ]\n","Parameters before update (layer 0 sample): [0.1499272  0.00917772 0.15575881 0.04945095 0.03561446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092716 0.01017771 0.15475883 0.04845096 0.03661445]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 566\n","Worker 0 gradients (layer 0 sample): [ 0.4202735  3.7344406 -1.1273593 -4.2756324  0.9974259]\n","Worker 1 processing data batch 566\n","Worker 1 gradients (layer 0 sample): [ 0.40826228  3.975024   -1.4237468  -4.817174    0.9145669 ]\n","Aggregated gradients (layer 0 sample): [ 0.4142679  3.8547323 -1.275553  -4.546403   0.9559964]\n","Parameters before update (layer 0 sample): [0.15092716 0.01017771 0.15475883 0.04845096 0.03661445]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992717 0.00917772 0.15575881 0.04945095 0.03561446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 567\n","Worker 0 gradients (layer 0 sample): [-0.19094804  0.02064902  0.22865158  1.6164453   0.32439923]\n","Worker 1 processing data batch 567\n","Worker 1 gradients (layer 0 sample): [-0.27556628 -2.1416547   1.3389003   4.8851905  -0.2563506 ]\n","Aggregated gradients (layer 0 sample): [-0.23325716 -1.0605029   0.7837759   3.2508178   0.03402431]\n","Parameters before update (layer 0 sample): [0.14992717 0.00917772 0.15575881 0.04945095 0.03561446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092716 0.01017771 0.15475883 0.04845096 0.03461456]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 568\n","Worker 0 gradients (layer 0 sample): [ 0.6277889  1.4774234 -1.0105491 -4.338523  -0.7536204]\n","Worker 1 processing data batch 568\n","Worker 1 gradients (layer 0 sample): [ 0.5243678   0.8202491  -0.74363756 -2.7682517  -0.44895327]\n","Aggregated gradients (layer 0 sample): [ 0.57607836  1.1488363  -0.8770933  -3.5533872  -0.6012868 ]\n","Parameters before update (layer 0 sample): [0.15092716 0.01017771 0.15475883 0.04845096 0.03461456]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992717 0.00917772 0.15575881 0.04945095 0.03561454]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 569\n","Worker 0 gradients (layer 0 sample): [-0.21516292 -0.861295    0.71046287  2.9736993  -0.23379686]\n","Worker 1 processing data batch 569\n","Worker 1 gradients (layer 0 sample): [-0.6538651  -2.242041    2.0074844   7.7882338  -0.16736808]\n","Aggregated gradients (layer 0 sample): [-0.43451402 -1.551668    1.3589736   5.3809667  -0.20058247]\n","Parameters before update (layer 0 sample): [0.14992717 0.00917772 0.15575881 0.04945095 0.03561454]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092716 0.01017771 0.15475883 0.04845095 0.03661452]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 570\n","Worker 0 gradients (layer 0 sample): [ 0.11416315  4.516219   -1.3537447  -3.5991054   1.5607252 ]\n","Worker 1 processing data batch 570\n","Worker 1 gradients (layer 0 sample): [ 0.7363361   3.3961768  -1.2257534  -6.379758    0.09604722]\n","Aggregated gradients (layer 0 sample): [ 0.42524964  3.956198   -1.2897491  -4.9894314   0.8283862 ]\n","Parameters before update (layer 0 sample): [0.15092716 0.01017771 0.15475883 0.04845095 0.03661452]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992717 0.00917771 0.15575881 0.04945095 0.03561453]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 570, Losses: [1.8896275, 2.2142587]\n","Worker 0 processing data batch 571\n","Worker 0 gradients (layer 0 sample): [-0.04102206 -1.8547885   0.563275    1.9418685  -0.7305666 ]\n","Worker 1 processing data batch 571\n","Worker 1 gradients (layer 0 sample): [-0.37685072 -1.5893676   1.4726874   5.7854724   0.14139389]\n","Aggregated gradients (layer 0 sample): [-0.2089364  -1.7220781   1.0179812   3.8636703  -0.29458636]\n","Parameters before update (layer 0 sample): [0.14992717 0.00917771 0.15575881 0.04945095 0.03561453]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092714 0.01017771 0.15475883 0.04845096 0.03661451]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 572\n","Worker 0 gradients (layer 0 sample): [ 0.6963732   3.6494746  -1.8135482  -6.332921    0.05499712]\n","Worker 1 processing data batch 572\n","Worker 1 gradients (layer 0 sample): [ 0.06290203  2.440699   -0.5583945  -2.2876196   0.6441463 ]\n","Aggregated gradients (layer 0 sample): [ 0.37963763  3.0450869  -1.1859714  -4.3102703   0.3495717 ]\n","Parameters before update (layer 0 sample): [0.15092714 0.01017771 0.15475883 0.04845096 0.03661451]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992715 0.00917771 0.15575881 0.04945095 0.03561453]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 573\n","Worker 0 gradients (layer 0 sample): [-0.39910802 -1.5015566   0.9909384   4.0981655   0.03279911]\n","Worker 1 processing data batch 573\n","Worker 1 gradients (layer 0 sample): [-0.4116532  -0.6747328   1.1861248   4.157382    0.03398056]\n","Aggregated gradients (layer 0 sample): [-0.4053806  -1.0881448   1.0885316   4.127774    0.03338984]\n","Parameters before update (layer 0 sample): [0.14992715 0.00917771 0.15575881 0.04945095 0.03561453]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092714 0.01017771 0.15475883 0.04845096 0.03461463]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 574\n","Worker 0 gradients (layer 0 sample): [ 0.31108242  3.8689754  -1.6246686  -5.0768886   0.07045954]\n","Worker 1 processing data batch 574\n","Worker 1 gradients (layer 0 sample): [ 0.24179997  3.4873035  -1.1113592  -3.5150332  -0.11375139]\n","Aggregated gradients (layer 0 sample): [ 0.2764412   3.6781394  -1.3680139  -4.295961   -0.02164592]\n","Parameters before update (layer 0 sample): [0.15092714 0.01017771 0.15475883 0.04845096 0.03461463]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992715 0.00917771 0.15575881 0.04945095 0.03561448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 575\n","Worker 0 gradients (layer 0 sample): [-0.19671927 -1.3859462   1.133915    3.361893   -0.02327657]\n","Worker 1 processing data batch 575\n","Worker 1 gradients (layer 0 sample): [ 0.1269221 -2.4723134  0.883801   1.9541445 -0.6756018]\n","Aggregated gradients (layer 0 sample): [-0.03489859 -1.9291298   1.008858    2.6580186  -0.34943917]\n","Parameters before update (layer 0 sample): [0.14992715 0.00917771 0.15575881 0.04945095 0.03561448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.15092705 0.0101777  0.15475883 0.04845096 0.03661446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 576\n","Worker 0 gradients (layer 0 sample): [ 0.1163184   2.1166518  -0.6605009  -1.8761752   0.10991216]\n","Worker 1 processing data batch 576\n","Worker 1 gradients (layer 0 sample): [ 0.2327281   3.6152415  -0.92442036 -2.998238    0.2496794 ]\n","Aggregated gradients (layer 0 sample): [ 0.17452325  2.8659468  -0.7924606  -2.4372067   0.17979577]\n","Parameters before update (layer 0 sample): [0.15092705 0.0101777  0.15475883 0.04845096 0.03661446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917771 0.15575881 0.04945095 0.03561448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 577\n","Worker 0 gradients (layer 0 sample): [ 0.38225755 -0.89109385  0.06443135 -0.8567094  -0.29009512]\n","Worker 1 processing data batch 577\n","Worker 1 gradients (layer 0 sample): [ 0.36417264 -2.5768003   0.81162655  0.29333854 -0.9494561 ]\n","Aggregated gradients (layer 0 sample): [ 0.37321508 -1.733947    0.43802896 -0.28168544 -0.6197756 ]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917771 0.15575881 0.04945095 0.03561448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.0101777  0.15475883 0.05045093 0.03661447]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 578\n","Worker 0 gradients (layer 0 sample): [-0.5711092  1.6223625  0.3547204  4.324461   1.0634449]\n","Worker 1 processing data batch 578\n","Worker 1 gradients (layer 0 sample): [-0.6449271   1.2112583   0.90430164  6.6955304   1.1581507 ]\n","Aggregated gradients (layer 0 sample): [-0.60801816  1.4168104   0.629511    5.5099955   1.1107978 ]\n","Parameters before update (layer 0 sample): [0.1489271  0.0101777  0.15475883 0.05045093 0.03661447]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917771 0.15375884 0.04945094 0.03561448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 579\n","Worker 0 gradients (layer 0 sample): [ 0.46081322  0.88730985 -0.9424316  -3.886674   -0.20353015]\n","Worker 1 processing data batch 579\n","Worker 1 gradients (layer 0 sample): [ 0.5813494   1.2866809  -0.8526232  -4.0894184  -0.39884573]\n","Aggregated gradients (layer 0 sample): [ 0.5210813   1.0869954  -0.89752746 -3.9880462  -0.30118793]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917771 0.15375884 0.04945094 0.03561448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.00817772 0.15475883 0.05045093 0.03661446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 580\n","Worker 0 gradients (layer 0 sample): [-0.5459139  -1.0739397   1.2058347   6.673779    0.18863034]\n","Worker 1 processing data batch 580\n","Worker 1 gradients (layer 0 sample): [-0.46483386 -0.45102924  0.9343201   4.2625213   0.3580489 ]\n","Aggregated gradients (layer 0 sample): [-0.50537384 -0.76248443  1.0700774   5.46815     0.27333963]\n","Parameters before update (layer 0 sample): [0.1489271  0.00817772 0.15475883 0.05045093 0.03661446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917771 0.15375884 0.04945093 0.03561448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 580, Losses: [1.9040222, 2.0003538]\n","Worker 0 processing data batch 581\n","Worker 0 gradients (layer 0 sample): [ 0.10685699  0.0062007  -0.05343601 -0.5390477  -0.31298056]\n","Worker 1 processing data batch 581\n","Worker 1 gradients (layer 0 sample): [ 0.35594308  1.3933046  -0.6675948  -3.9993277  -0.06748344]\n","Aggregated gradients (layer 0 sample): [ 0.23140004  0.6997526  -0.36051542 -2.2691877  -0.19023201]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917771 0.15375884 0.04945093 0.03561448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045092 0.03661446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 582\n","Worker 0 gradients (layer 0 sample): [-0.3377722  -0.6764742   0.87012464  4.152691    0.02219985]\n","Worker 1 processing data batch 582\n","Worker 1 gradients (layer 0 sample): [-0.56063306 -0.2800471   0.7253511   5.475066    0.39970914]\n","Aggregated gradients (layer 0 sample): [-0.44920263 -0.47826064  0.79773784  4.8138785   0.21095449]\n","Parameters before update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045092 0.03661446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499271  0.00917771 0.15375884 0.04945093 0.03561448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 583\n","Worker 0 gradients (layer 0 sample): [ 0.40229294  1.1803031  -0.65775883 -3.2566867  -0.03076699]\n","Worker 1 processing data batch 583\n","Worker 1 gradients (layer 0 sample): [ 0.461563    2.9640293  -1.2671769  -5.560055    0.17736694]\n","Aggregated gradients (layer 0 sample): [ 0.43192798  2.0721662  -0.96246785 -4.408371    0.07329997]\n","Parameters before update (layer 0 sample): [0.1499271  0.00917771 0.15375884 0.04945093 0.03561448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045092 0.03461453]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 584\n","Worker 0 gradients (layer 0 sample): [-0.39305353 -1.3433728   0.709903    4.2115307  -0.03997531]\n","Worker 1 processing data batch 584\n","Worker 1 gradients (layer 0 sample): [-0.20031309 -0.75351095  0.2130799   1.6885602  -0.21133016]\n","Aggregated gradients (layer 0 sample): [-0.2966833  -1.0484419   0.46149147  2.9500456  -0.12565273]\n","Parameters before update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045092 0.03461453]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499271  0.00917771 0.15375884 0.04945093 0.03561449]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 585\n","Worker 0 gradients (layer 0 sample): [ 0.6570029   2.2112772  -1.282474   -6.1952047   0.10863426]\n","Worker 1 processing data batch 585\n","Worker 1 gradients (layer 0 sample): [ 0.99587524  1.2003323  -0.98529124 -6.345996   -0.4699447 ]\n","Aggregated gradients (layer 0 sample): [ 0.8264391   1.7058048  -1.1338826  -6.2706003  -0.18065521]\n","Parameters before update (layer 0 sample): [0.1499271  0.00917771 0.15375884 0.04945093 0.03561449]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045093 0.03661447]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 586\n","Worker 0 gradients (layer 0 sample): [-0.64631087 -2.1436238   2.0921216   8.768517   -0.3092348 ]\n","Worker 1 processing data batch 586\n","Worker 1 gradients (layer 0 sample): [-0.7818433 -1.2759893  1.7606362  8.330345   0.5101684]\n","Aggregated gradients (layer 0 sample): [-0.7140771  -1.7098066   1.926379    8.549431    0.10046679]\n","Parameters before update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045093 0.03661447]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499271  0.00917771 0.15375884 0.04945093 0.03561451]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 587\n","Worker 0 gradients (layer 0 sample): [ 0.16118681  3.3586583  -0.73363125 -2.6863813   0.5014876 ]\n","Worker 1 processing data batch 587\n","Worker 1 gradients (layer 0 sample): [ 0.6819519   0.9318578  -0.9032805  -4.366925   -0.22170539]\n","Aggregated gradients (layer 0 sample): [ 0.42156935  2.145258   -0.8184559  -3.526653    0.13989112]\n","Parameters before update (layer 0 sample): [0.1499271  0.00917771 0.15375884 0.04945093 0.03561451]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045092 0.03461454]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 588\n","Worker 0 gradients (layer 0 sample): [-0.33809674 -2.0011919   1.4441811   5.612381   -0.38427287]\n","Worker 1 processing data batch 588\n","Worker 1 gradients (layer 0 sample): [-0.52059877 -1.9282488   1.6810117   7.0755444  -0.1013989 ]\n","Aggregated gradients (layer 0 sample): [-0.42934775 -1.9647202   1.5625963   6.3439627  -0.24283588]\n","Parameters before update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045092 0.03461454]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1499271  0.00917771 0.15375884 0.04945093 0.03561452]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 589\n","Worker 0 gradients (layer 0 sample): [ 0.67880034  1.3398918  -1.1424084  -5.909755   -0.23939556]\n","Worker 1 processing data batch 589\n","Worker 1 gradients (layer 0 sample): [ 0.5573784   1.2860229  -0.5516149  -3.8012562  -0.28115362]\n","Aggregated gradients (layer 0 sample): [ 0.6180894  1.3129573 -0.8470116 -4.855506  -0.2602746]\n","Parameters before update (layer 0 sample): [0.1499271  0.00917771 0.15375884 0.04945093 0.03561452]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045092 0.0366145 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 590\n","Worker 0 gradients (layer 0 sample): [ 0.0098273  -0.14544392  0.15085939  0.88392484 -0.3409805 ]\n","Worker 1 processing data batch 590\n","Worker 1 gradients (layer 0 sample): [-0.390251  -0.2621619  0.9744139  4.784169   0.2295048]\n","Aggregated gradients (layer 0 sample): [-0.19021186 -0.20380291  0.5626366   2.834047   -0.05573785]\n","Parameters before update (layer 0 sample): [0.14892711 0.00817772 0.15475883 0.05045092 0.0366145 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945093 0.03761443]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 590, Losses: [1.8776331, 2.2087462]\n","Worker 0 processing data batch 591\n","Worker 0 gradients (layer 0 sample): [ 0.39629707  0.42760783 -0.44534063 -2.8096638   0.5651742 ]\n","Worker 1 processing data batch 591\n","Worker 1 gradients (layer 0 sample): [ 0.40466768  2.4059966  -1.039838   -4.7877774   0.85731816]\n","Aggregated gradients (layer 0 sample): [ 0.40048236  1.4168022  -0.7425893  -3.7987206   0.7112462 ]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945093 0.03761443]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045092 0.03661444]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 592\n","Worker 0 gradients (layer 0 sample): [-0.63797647 -0.7154238   1.5149004   7.0992165   0.4925471 ]\n","Worker 1 processing data batch 592\n","Worker 1 gradients (layer 0 sample): [-0.62937766 -0.255061    0.9143231   6.2297487   0.2807802 ]\n","Aggregated gradients (layer 0 sample): [-0.63367707 -0.48524243  1.2146118   6.6644826   0.38666365]\n","Parameters before update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045092 0.03661444]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945093 0.03561446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 593\n","Worker 0 gradients (layer 0 sample): [ 0.7957439   0.36488098 -0.7808642  -5.3923435  -0.5636174 ]\n","Worker 1 processing data batch 593\n","Worker 1 gradients (layer 0 sample): [ 0.40386802  1.0134833  -0.6620171  -3.8557072  -0.29661953]\n","Aggregated gradients (layer 0 sample): [ 0.59980595  0.68918216 -0.7214407  -4.6240253  -0.43011847]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945093 0.03561446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045092 0.03661445]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 594\n","Worker 0 gradients (layer 0 sample): [-0.26566643 -0.6987053   0.5527026   3.3210726   0.05800465]\n","Worker 1 processing data batch 594\n","Worker 1 gradients (layer 0 sample): [-0.9121035  -0.81314063  1.281078    8.293629    0.59180963]\n","Aggregated gradients (layer 0 sample): [-0.58888495 -0.755923    0.91689026  5.8073506   0.32490712]\n","Parameters before update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045092 0.03661445]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945093 0.03561446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 595\n","Worker 0 gradients (layer 0 sample): [ 0.22622582  0.9148151  -0.5854254  -1.9558399   0.24581864]\n","Worker 1 processing data batch 595\n","Worker 1 gradients (layer 0 sample): [ 0.34032688  0.0260011  -0.72338134 -3.5150912  -0.48336983]\n","Aggregated gradients (layer 0 sample): [ 0.28327635  0.4704081  -0.6544033  -2.7354655  -0.11877559]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945093 0.03561446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045092 0.03661443]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 596\n","Worker 0 gradients (layer 0 sample): [-0.15101853 -0.4945272   0.48965898  2.1956682  -0.12611718]\n","Worker 1 processing data batch 596\n","Worker 1 gradients (layer 0 sample): [-0.56551504 -0.51742727  0.92307806  5.93226     0.55048925]\n","Aggregated gradients (layer 0 sample): [-0.35826677 -0.5059772   0.7063685   4.063964    0.21218604]\n","Parameters before update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045092 0.03661443]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945093 0.03561445]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 597\n","Worker 0 gradients (layer 0 sample): [ 0.4882819   1.8847219  -1.1700852  -5.6653824  -0.15449914]\n","Worker 1 processing data batch 597\n","Worker 1 gradients (layer 0 sample): [ 0.09391656  0.65885633 -0.19424632 -0.69601107  0.10718963]\n","Aggregated gradients (layer 0 sample): [ 0.29109922  1.2717891  -0.68216574 -3.1806967  -0.02365476]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945093 0.03561445]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045092 0.03661431]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 598\n","Worker 0 gradients (layer 0 sample): [-0.49658942 -1.7427047   1.5203211   6.4516563  -0.1926848 ]\n","Worker 1 processing data batch 598\n","Worker 1 gradients (layer 0 sample): [-0.49436158 -1.6153147   1.7675489   7.555538   -0.11517935]\n","Aggregated gradients (layer 0 sample): [-0.4954755  -1.6790097   1.643935    7.0035973  -0.15393208]\n","Parameters before update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045092 0.03661431]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945092 0.03761429]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 599\n","Worker 0 gradients (layer 0 sample): [ 0.39184642  2.8319607  -0.93051255 -4.1374054   1.2864428 ]\n","Worker 1 processing data batch 599\n","Worker 1 gradients (layer 0 sample): [ 0.48975188  1.6272666  -0.92973965 -4.127663    0.12674004]\n","Aggregated gradients (layer 0 sample): [ 0.44079915  2.2296138  -0.9301261  -4.132534    0.70659137]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945092 0.03761429]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045091 0.0366143 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 600\n","Worker 0 gradients (layer 0 sample): [-0.42117065 -1.2505282   1.3620338   6.352471   -0.18334343]\n","Worker 1 processing data batch 600\n","Worker 1 gradients (layer 0 sample): [-0.25915986 -0.8328366   0.3902194   2.732551   -0.28042603]\n","Aggregated gradients (layer 0 sample): [-0.34016526 -1.0416825   0.87612665  4.542511   -0.23188472]\n","Parameters before update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045091 0.0366143 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945092 0.03761428]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 600, Losses: [2.146212, 1.6783321]\n","Worker 0 processing data batch 601\n","Worker 0 gradients (layer 0 sample): [ 0.27410758  1.7196128  -0.6024859  -3.1823468   0.5574439 ]\n","Worker 1 processing data batch 601\n","Worker 1 gradients (layer 0 sample): [ 0.37815762  1.1755348  -0.5658839  -2.8773954   0.21796077]\n","Aggregated gradients (layer 0 sample): [ 0.3261326   1.4475739  -0.5841849  -3.029871    0.38770235]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945092 0.03761428]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045091 0.0366143 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 602\n","Worker 0 gradients (layer 0 sample): [-0.7727244  -2.1594172   2.1961126  10.417072    0.10219239]\n","Worker 1 processing data batch 602\n","Worker 1 gradients (layer 0 sample): [-0.2501075  -0.8990714   1.0380923   4.966972   -0.28135744]\n","Aggregated gradients (layer 0 sample): [-0.51141596 -1.5292443   1.6171024   7.6920223  -0.08958252]\n","Parameters before update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045091 0.0366143 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945092 0.03761425]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 603\n","Worker 0 gradients (layer 0 sample): [ 0.5732403   2.5818582  -1.1323007  -5.586996    0.52864873]\n","Worker 1 processing data batch 603\n","Worker 1 gradients (layer 0 sample): [ 0.00511056  2.6195583  -0.7671084  -2.1132445   1.5389866 ]\n","Aggregated gradients (layer 0 sample): [ 0.28917542  2.6007082  -0.9497045  -3.8501203   1.0338176 ]\n","Parameters before update (layer 0 sample): [0.14992708 0.00917769 0.15375884 0.04945092 0.03761425]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045091 0.03661426]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 604\n","Worker 0 gradients (layer 0 sample): [-0.25435224 -1.1718227   0.70433676  3.3151007  -0.41131574]\n","Worker 1 processing data batch 604\n","Worker 1 gradients (layer 0 sample): [ 0.27903968 -1.6345474   0.6506375   0.44353116 -1.2252219 ]\n","Aggregated gradients (layer 0 sample): [ 0.01234372 -1.403185    0.67748713  1.8793159  -0.8182688 ]\n","Parameters before update (layer 0 sample): [0.1489271  0.0081777  0.15475883 0.05045091 0.03661426]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792736 0.00917769 0.15375884 0.04945092 0.03761425]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 605\n","Worker 0 gradients (layer 0 sample): [ 0.43905884  1.7674558  -0.72033966 -4.0201797   0.39157996]\n","Worker 1 processing data batch 605\n","Worker 1 gradients (layer 0 sample): [ 0.73310995  1.7981665  -0.8085254  -4.1014214  -0.03009263]\n","Aggregated gradients (layer 0 sample): [ 0.58608437  1.7828112  -0.76443255 -4.0608006   0.18074366]\n","Parameters before update (layer 0 sample): [0.14792736 0.00917769 0.15375884 0.04945092 0.03761425]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692737 0.0081777  0.15475883 0.05045091 0.03661428]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 606\n","Worker 0 gradients (layer 0 sample): [-0.36949226 -0.43962356  0.6511375   3.7178898   0.01199672]\n","Worker 1 processing data batch 606\n","Worker 1 gradients (layer 0 sample): [-0.8532152  -2.1886513   2.0536115   9.604637    0.02242404]\n","Aggregated gradients (layer 0 sample): [-0.61135375 -1.3141375   1.3523746   6.6612635   0.01721038]\n","Parameters before update (layer 0 sample): [0.14692737 0.0081777  0.15475883 0.05045091 0.03661428]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792736 0.00917769 0.15375884 0.04945092 0.03561447]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 607\n","Worker 0 gradients (layer 0 sample): [ 0.8170992   2.2076125  -1.1496167  -6.3347435   0.26564044]\n","Worker 1 processing data batch 607\n","Worker 1 gradients (layer 0 sample): [ 0.8872483   2.1712115  -1.1996456  -6.7764435  -0.03144708]\n","Aggregated gradients (layer 0 sample): [ 0.85217375  2.189412   -1.1746311  -6.5555935   0.11709668]\n","Parameters before update (layer 0 sample): [0.14792736 0.00917769 0.15375884 0.04945092 0.03561447]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692737 0.0081777  0.15475883 0.05045091 0.0346145 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 608\n","Worker 0 gradients (layer 0 sample): [-0.56247777 -0.8447113   1.3269126   6.4795485   0.27687106]\n","Worker 1 processing data batch 608\n","Worker 1 gradients (layer 0 sample): [-0.5059681  -1.5524518   1.5501852   7.6321144   0.12312941]\n","Aggregated gradients (layer 0 sample): [-0.53422296 -1.1985816   1.4385489   7.0558314   0.20000023]\n","Parameters before update (layer 0 sample): [0.14692737 0.0081777  0.15475883 0.05045091 0.0346145 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792736 0.00917769 0.15375884 0.04945092 0.03361453]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 609\n","Worker 0 gradients (layer 0 sample): [ 0.5784614   0.3032786  -0.89566374 -4.757533   -0.4930405 ]\n","Worker 1 processing data batch 609\n","Worker 1 gradients (layer 0 sample): [ 0.13501984  0.53134894 -0.33431256 -1.6613511  -0.16011827]\n","Aggregated gradients (layer 0 sample): [ 0.35674062  0.41731375 -0.61498815 -3.2094421  -0.3265794 ]\n","Parameters before update (layer 0 sample): [0.14792736 0.00917769 0.15375884 0.04945092 0.03361453]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692737 0.0081777  0.15475883 0.05045091 0.03461451]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 610\n","Worker 0 gradients (layer 0 sample): [-0.38618514 -0.7501447   1.1562182   6.0882998   0.28290114]\n","Worker 1 processing data batch 610\n","Worker 1 gradients (layer 0 sample): [-0.30364847 -1.7173145   1.4255122   5.696939   -0.55513704]\n","Aggregated gradients (layer 0 sample): [-0.34491682 -1.2337296   1.2908652   5.892619   -0.13611795]\n","Parameters before update (layer 0 sample): [0.14692737 0.0081777  0.15475883 0.05045091 0.03461451]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792736 0.00917769 0.15375884 0.04945091 0.03561448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 610, Losses: [2.1607842, 2.004465]\n","Worker 0 processing data batch 611\n","Worker 0 gradients (layer 0 sample): [ 0.29490834 -0.10892204 -0.65446794 -2.2293596  -0.15128827]\n","Worker 1 processing data batch 611\n","Worker 1 gradients (layer 0 sample): [-0.16395019  1.8580415  -0.364048   -0.7207154   0.7060286 ]\n","Aggregated gradients (layer 0 sample): [ 0.06547908  0.87455976 -0.509258   -1.4750376   0.27737015]\n","Parameters before update (layer 0 sample): [0.14792736 0.00917769 0.15375884 0.04945091 0.03561448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692742 0.0081777  0.15475883 0.0504509  0.0346145 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 612\n","Worker 0 gradients (layer 0 sample): [-0.20815198 -1.5047541   0.7211969   3.4965377  -0.30018628]\n","Worker 1 processing data batch 612\n","Worker 1 gradients (layer 0 sample): [-0.43893594 -1.7838389   1.1450666   5.831644   -0.09797656]\n","Aggregated gradients (layer 0 sample): [-0.32354397 -1.6442964   0.93313175  4.664091   -0.19908142]\n","Parameters before update (layer 0 sample): [0.14692742 0.0081777  0.15475883 0.0504509  0.0346145 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1479274  0.00917769 0.15375884 0.04945091 0.03561447]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 613\n","Worker 0 gradients (layer 0 sample): [ 0.59250504  3.3410764  -1.2014169  -5.665193    0.86234903]\n","Worker 1 processing data batch 613\n","Worker 1 gradients (layer 0 sample): [ 0.5175586   1.6769098  -0.78413534 -4.460758    0.15658595]\n","Aggregated gradients (layer 0 sample): [ 0.5550318  2.5089931 -0.9927761 -5.062976   0.5094675]\n","Parameters before update (layer 0 sample): [0.1479274  0.00917769 0.15375884 0.04945091 0.03561447]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692742 0.0081777  0.15475883 0.0504509  0.03461448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 614\n","Worker 0 gradients (layer 0 sample): [-0.28067347 -2.1502986   1.496125    5.0958757  -0.56032944]\n","Worker 1 processing data batch 614\n","Worker 1 gradients (layer 0 sample): [-0.36444193 -1.6259084   1.5030596   6.2576923  -0.16982085]\n","Aggregated gradients (layer 0 sample): [-0.3225577  -1.8881035   1.4995923   5.676784   -0.36507514]\n","Parameters before update (layer 0 sample): [0.14692742 0.0081777  0.15475883 0.0504509  0.03461448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1479274  0.00917769 0.15375884 0.04945091 0.03561447]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 615\n","Worker 0 gradients (layer 0 sample): [ 0.5435547   2.9091282  -0.8850212  -6.2244253   0.08385575]\n","Worker 1 processing data batch 615\n","Worker 1 gradients (layer 0 sample): [ 0.45649505  2.3888452  -1.0403748  -4.8722363   0.33147207]\n","Aggregated gradients (layer 0 sample): [ 0.5000249   2.6489868  -0.962698   -5.548331    0.20766391]\n","Parameters before update (layer 0 sample): [0.1479274  0.00917769 0.15375884 0.04945091 0.03561447]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692742 0.0081777  0.15475883 0.05045091 0.03461449]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 616\n","Worker 0 gradients (layer 0 sample): [-0.06397564 -0.99592984  0.38875005  1.4108858  -0.47620672]\n","Worker 1 processing data batch 616\n","Worker 1 gradients (layer 0 sample): [-0.2994799  -0.15265363  0.4864669   4.1147704   0.2130611 ]\n","Aggregated gradients (layer 0 sample): [-0.18172777 -0.5742917   0.43760848  2.762828   -0.13157281]\n","Parameters before update (layer 0 sample): [0.14692742 0.0081777  0.15475883 0.05045091 0.03461449]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792739 0.00917769 0.15375884 0.04945092 0.03561446]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 617\n","Worker 0 gradients (layer 0 sample): [ 0.6811725   1.450558   -0.98841035 -6.051811   -0.7935406 ]\n","Worker 1 processing data batch 617\n","Worker 1 gradients (layer 0 sample): [ 0.38119695  1.6999874  -0.9430842  -4.1537066   0.6606173 ]\n","Aggregated gradients (layer 0 sample): [ 0.53118473  1.5752727  -0.96574724 -5.102759   -0.06646165]\n","Parameters before update (layer 0 sample): [0.14792739 0.00917769 0.15375884 0.04945092 0.03561446]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469274  0.0081777  0.15475883 0.05045091 0.0366144 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 618\n","Worker 0 gradients (layer 0 sample): [-0.33753082 -0.7701459   0.911778    5.037826    0.32593068]\n","Worker 1 processing data batch 618\n","Worker 1 gradients (layer 0 sample): [-0.76894057 -0.44072405  0.58311874  5.9058237   1.0714935 ]\n","Aggregated gradients (layer 0 sample): [-0.5532357  -0.60543495  0.7474483   5.4718246   0.6987121 ]\n","Parameters before update (layer 0 sample): [0.1469274  0.0081777  0.15475883 0.05045091 0.0366144 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792739 0.00917769 0.15375884 0.04945091 0.03561442]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 619\n","Worker 0 gradients (layer 0 sample): [ 0.8694986  -0.18055117 -0.78422564 -5.1511903  -1.4897151 ]\n","Worker 1 processing data batch 619\n","Worker 1 gradients (layer 0 sample): [ 1.1670923   0.90485674 -0.86327547 -7.668061   -1.2777746 ]\n","Aggregated gradients (layer 0 sample): [ 1.0182955   0.36215279 -0.82375056 -6.4096255  -1.3837448 ]\n","Parameters before update (layer 0 sample): [0.14792739 0.00917769 0.15375884 0.04945091 0.03561442]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469274  0.0081777  0.15475883 0.05045091 0.03661441]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 620\n","Worker 0 gradients (layer 0 sample): [-0.90111065 -0.13828197  0.9907427   7.648944    1.5013949 ]\n","Worker 1 processing data batch 620\n","Worker 1 gradients (layer 0 sample): [-0.67750835 -0.7866873   1.332511    6.1886654   0.49519902]\n","Aggregated gradients (layer 0 sample): [-0.7893095  -0.46248466  1.1616268   6.9188046   0.998297  ]\n","Parameters before update (layer 0 sample): [0.1469274  0.0081777  0.15475883 0.05045091 0.03661441]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792739 0.00917769 0.15375884 0.04945091 0.03561442]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 620, Losses: [1.9236416, 1.9204782]\n","Worker 0 processing data batch 621\n","Worker 0 gradients (layer 0 sample): [ 0.66761106  1.317713   -1.2459803  -5.1950684  -0.2017073 ]\n","Worker 1 processing data batch 621\n","Worker 1 gradients (layer 0 sample): [ 0.3614077   1.2103784  -0.62382996 -2.6825743   0.01388092]\n","Aggregated gradients (layer 0 sample): [ 0.5145094   1.2640457  -0.9349051  -3.9388213  -0.09391319]\n","Parameters before update (layer 0 sample): [0.14792739 0.00917769 0.15375884 0.04945091 0.03561442]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469274  0.0081777  0.15475883 0.0504509  0.03661438]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 622\n","Worker 0 gradients (layer 0 sample): [-0.81453705 -1.3849998   1.327556    7.831169    0.5749327 ]\n","Worker 1 processing data batch 622\n","Worker 1 gradients (layer 0 sample): [-0.8880732 -0.9418847  1.873282   9.471998   1.141466 ]\n","Aggregated gradients (layer 0 sample): [-0.8513051  -1.1634423   1.600419    8.651584    0.85819936]\n","Parameters before update (layer 0 sample): [0.1469274  0.0081777  0.15475883 0.0504509  0.03661438]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792739 0.00917769 0.15375884 0.04945091 0.03561439]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 623\n","Worker 0 gradients (layer 0 sample): [ 0.58144814  1.5899624  -0.6858917  -4.8425574  -0.3083229 ]\n","Worker 1 processing data batch 623\n","Worker 1 gradients (layer 0 sample): [ 0.74583346  2.2387     -1.1428051  -6.8772163   0.07941043]\n","Aggregated gradients (layer 0 sample): [ 0.6636408   1.9143312  -0.91434836 -5.859887   -0.11445624]\n","Parameters before update (layer 0 sample): [0.14792739 0.00917769 0.15375884 0.04945091 0.03561439]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469274  0.0081777  0.15475883 0.0504509  0.03661435]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 624\n","Worker 0 gradients (layer 0 sample): [-0.61498916 -0.6242336   1.0577035   5.82284     0.7498528 ]\n","Worker 1 processing data batch 624\n","Worker 1 gradients (layer 0 sample): [-0.4610843   0.01247567  0.44153798  3.6667724   0.5827448 ]\n","Aggregated gradients (layer 0 sample): [-0.5380367  -0.30587897  0.74962074  4.7448063   0.66629875]\n","Parameters before update (layer 0 sample): [0.1469274  0.0081777  0.15475883 0.0504509  0.03661435]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792739 0.00917768 0.15375884 0.04945091 0.03561436]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 625\n","Worker 0 gradients (layer 0 sample): [ 1.2224833   0.83198494 -0.8842864  -7.220274   -0.80133784]\n","Worker 1 processing data batch 625\n","Worker 1 gradients (layer 0 sample): [ 0.08729401  0.8998063  -0.41299164 -0.8822535   0.219522  ]\n","Aggregated gradients (layer 0 sample): [ 0.65488863  0.8658956  -0.648639   -4.051264   -0.29090792]\n","Parameters before update (layer 0 sample): [0.14792739 0.00917768 0.15375884 0.04945091 0.03561436]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1469274  0.00817769 0.15475883 0.0504509  0.03661435]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 626\n","Worker 0 gradients (layer 0 sample): [-0.38943905 -1.6055765   1.4241464   6.2750616  -0.01985329]\n","Worker 1 processing data batch 626\n","Worker 1 gradients (layer 0 sample): [-0.16145867 -0.7570376   0.5045042   2.1648288  -0.42754793]\n","Aggregated gradients (layer 0 sample): [-0.27544886 -1.1813071   0.9643253   4.219945   -0.22370061]\n","Parameters before update (layer 0 sample): [0.1469274  0.00817769 0.15475883 0.0504509  0.03661435]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14792739 0.00917768 0.15375884 0.04945091 0.03761433]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 627\n","Worker 0 gradients (layer 0 sample): [ 0.4821539  1.3627865 -0.6570996 -4.1444817  0.2646442]\n","Worker 1 processing data batch 627\n","Worker 1 gradients (layer 0 sample): [-0.2755695   3.0588439  -0.6008199  -0.89719415  2.5717413 ]\n","Aggregated gradients (layer 0 sample): [ 0.1032922  2.2108152 -0.6289598 -2.5208378  1.4181927]\n","Parameters before update (layer 0 sample): [0.14792739 0.00917768 0.15375884 0.04945091 0.03761433]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14692743 0.00817769 0.15475883 0.0504509  0.03661434]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 628\n","Worker 0 gradients (layer 0 sample): [ 0.16445254 -0.56123483  0.40700352  0.38310218 -0.7444297 ]\n","Worker 1 processing data batch 628\n","Worker 1 gradients (layer 0 sample): [ 0.0631934 -1.9816431  0.8824128  1.8809009 -1.2975552]\n","Aggregated gradients (layer 0 sample): [ 0.11382297 -1.271439    0.64470816  1.1320015  -1.0209925 ]\n","Parameters before update (layer 0 sample): [0.14692743 0.00817769 0.15475883 0.0504509  0.03661434]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.04945091 0.03761433]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 629\n","Worker 0 gradients (layer 0 sample): [ 0.6146439   2.6935115  -0.97066545 -4.456679    0.9446758 ]\n","Worker 1 processing data batch 629\n","Worker 1 gradients (layer 0 sample): [ 0.4537981  2.0762968 -0.8135179 -3.6376088  1.2163371]\n","Aggregated gradients (layer 0 sample): [ 0.534221   2.3849041 -0.8920917 -4.047144   1.0805064]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.04945091 0.03761433]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.0504509  0.03661434]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 630\n","Worker 0 gradients (layer 0 sample): [-0.2822776  -1.211426    1.215704    4.982443   -0.43903565]\n","Worker 1 processing data batch 630\n","Worker 1 gradients (layer 0 sample): [-0.74592304 -1.3221902   1.4762793   7.982238    0.56180423]\n","Aggregated gradients (layer 0 sample): [-0.5141003  -1.266808    1.3459916   6.4823403   0.06138429]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.0504509  0.03661434]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.04945091 0.03561439]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 630, Losses: [2.218472, 1.8962824]\n","Worker 0 processing data batch 631\n","Worker 0 gradients (layer 0 sample): [ 0.73196054 -0.12055305 -0.5409385  -4.737611   -0.8260126 ]\n","Worker 1 processing data batch 631\n","Worker 1 gradients (layer 0 sample): [ 0.68075466  0.8814921  -0.68018985 -4.2485933  -0.6692029 ]\n","Aggregated gradients (layer 0 sample): [ 0.7063576   0.3804695  -0.6105642  -4.493102   -0.74760777]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.04945091 0.03561439]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.0504509  0.03661438]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 632\n","Worker 0 gradients (layer 0 sample): [-0.5214649  -1.5947442   1.8856944   8.061939   -0.29057932]\n","Worker 1 processing data batch 632\n","Worker 1 gradients (layer 0 sample): [-0.7533057 -1.4290978  1.3108467  7.9055233  0.5852732]\n","Aggregated gradients (layer 0 sample): [-0.63738525 -1.5119209   1.5982705   7.9837313   0.14734694]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.0504509  0.03661438]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917769 0.15375884 0.0494509  0.03561441]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 633\n","Worker 0 gradients (layer 0 sample): [ 0.31495184  1.019939   -0.51861966 -3.5779502  -0.31277147]\n","Worker 1 processing data batch 633\n","Worker 1 gradients (layer 0 sample): [ 0.8013141   1.0061722  -0.79129046 -5.411681   -0.38045642]\n","Aggregated gradients (layer 0 sample): [ 0.558133    1.0130556  -0.654955   -4.494816   -0.34661394]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917769 0.15375884 0.0494509  0.03561441]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.0081777  0.15475883 0.05045089 0.0366144 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 634\n","Worker 0 gradients (layer 0 sample): [-0.9903846  -0.42692316  0.91737175  7.938825    1.627753  ]\n","Worker 1 processing data batch 634\n","Worker 1 gradients (layer 0 sample): [-0.8495412  -1.7099339   1.967587    9.530626    0.28642932]\n","Aggregated gradients (layer 0 sample): [-0.9199629  -1.0684285   1.4424794   8.734726    0.95709115]\n","Parameters before update (layer 0 sample): [0.14492747 0.0081777  0.15475883 0.05045089 0.0366144 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917769 0.15375884 0.0494509  0.0356144 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 635\n","Worker 0 gradients (layer 0 sample): [ 0.94794744  1.0210669  -1.0375516  -6.863305   -0.73469573]\n","Worker 1 processing data batch 635\n","Worker 1 gradients (layer 0 sample): [ 0.57485795  0.89451206 -0.90796936 -4.475788   -0.38107306]\n","Aggregated gradients (layer 0 sample): [ 0.7614027  0.9577895 -0.9727605 -5.6695466 -0.5578844]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917769 0.15375884 0.0494509  0.0356144 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.0081777  0.15475883 0.05045089 0.03661439]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 636\n","Worker 0 gradients (layer 0 sample): [-0.49103296 -0.6174658   1.1520554   5.8247614   0.53010666]\n","Worker 1 processing data batch 636\n","Worker 1 gradients (layer 0 sample): [-0.54644156 -0.6084044   1.1717719   6.025423    0.39521855]\n","Aggregated gradients (layer 0 sample): [-0.51873726 -0.61293507  1.1619136   5.925092    0.4626626 ]\n","Parameters before update (layer 0 sample): [0.14492747 0.0081777  0.15475883 0.05045089 0.03661439]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.0494509  0.0356144 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 637\n","Worker 0 gradients (layer 0 sample): [ 0.6623571   1.3289775  -0.46322644 -4.2423496  -0.93496597]\n","Worker 1 processing data batch 637\n","Worker 1 gradients (layer 0 sample): [-0.21373802  2.0675695  -0.73233116 -1.6161544   0.9991428 ]\n","Aggregated gradients (layer 0 sample): [ 0.22430953  1.6982734  -0.5977788  -2.9292521   0.03208843]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.0494509  0.0356144 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492749 0.00817769 0.15475883 0.05045089 0.03461451]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 638\n","Worker 0 gradients (layer 0 sample): [ 0.04199494 -2.1253033   0.716691    1.892604   -0.9788785 ]\n","Worker 1 processing data batch 638\n","Worker 1 gradients (layer 0 sample): [-0.16108158 -1.5138503   1.0997483   3.7865005  -0.40048364]\n","Aggregated gradients (layer 0 sample): [-0.05954332 -1.8195767   0.90821964  2.8395522  -0.68968105]\n","Parameters before update (layer 0 sample): [0.14492749 0.00817769 0.15475883 0.05045089 0.03461451]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592743 0.00917768 0.15375884 0.0494509  0.0356145 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 639\n","Worker 0 gradients (layer 0 sample): [ 0.4287262  2.160959  -1.1683873 -5.5476193  1.1469707]\n","Worker 1 processing data batch 639\n","Worker 1 gradients (layer 0 sample): [-0.23190317  2.4118328  -0.5820867  -0.20900042  1.8803873 ]\n","Aggregated gradients (layer 0 sample): [ 0.09841152  2.286396   -0.875237   -2.87831     1.513679  ]\n","Parameters before update (layer 0 sample): [0.14592743 0.00917768 0.15375884 0.0494509  0.0356145 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.05045089 0.03461451]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 640\n","Worker 0 gradients (layer 0 sample): [-0.36329025 -1.0904204   1.096666    5.1638155  -0.1026909 ]\n","Worker 1 processing data batch 640\n","Worker 1 gradients (layer 0 sample): [-0.4325481  -1.9610102   1.5595071   6.3368263  -0.22321358]\n","Aggregated gradients (layer 0 sample): [-0.39791918 -1.5257154   1.3280866   5.750321   -0.16295224]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.05045089 0.03461451]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.0494509  0.03561448]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 640, Losses: [2.0483527, 2.054858]\n","Worker 0 processing data batch 641\n","Worker 0 gradients (layer 0 sample): [ 0.79735005  1.030848   -0.75876606 -5.9356546  -0.37975627]\n","Worker 1 processing data batch 641\n","Worker 1 gradients (layer 0 sample): [ 0.29870474  1.3792887  -0.7519691  -2.9736834   0.4294316 ]\n","Aggregated gradients (layer 0 sample): [ 0.5480274   1.2050683  -0.7553676  -4.454669    0.02483766]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.0494509  0.03561448]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.05045089 0.03461462]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 642\n","Worker 0 gradients (layer 0 sample): [-0.18254565 -1.1468208   0.6715577   4.054641   -0.06291527]\n","Worker 1 processing data batch 642\n","Worker 1 gradients (layer 0 sample): [-0.54590905 -1.1271547   1.1082625   6.2687473   0.31950617]\n","Aggregated gradients (layer 0 sample): [-0.36422735 -1.1369877   0.8899101   5.161694    0.12829545]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.05045089 0.03461462]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.0494509  0.03361465]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 643\n","Worker 0 gradients (layer 0 sample): [ 0.43933386  0.6641771  -0.35907125 -2.2335253  -0.09034648]\n","Worker 1 processing data batch 643\n","Worker 1 gradients (layer 0 sample): [ 0.6322032   0.63498664 -0.68915445 -4.19111    -0.32094705]\n","Aggregated gradients (layer 0 sample): [ 0.5357685   0.6495819  -0.5241128  -3.2123177  -0.20564677]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.0494509  0.03361465]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.0081777  0.15475883 0.05045089 0.03461463]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 644\n","Worker 0 gradients (layer 0 sample): [-0.6471336  -0.12216794  0.7428909   5.4388285   0.37321174]\n","Worker 1 processing data batch 644\n","Worker 1 gradients (layer 0 sample): [-0.66261125 -0.39363542  0.7725651   5.719166    0.5492147 ]\n","Aggregated gradients (layer 0 sample): [-0.6548724  -0.25790167  0.757728    5.578997    0.46121323]\n","Parameters before update (layer 0 sample): [0.14492747 0.0081777  0.15475883 0.05045089 0.03461463]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.0494509  0.03361464]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 645\n","Worker 0 gradients (layer 0 sample): [ 0.8557066  0.8883507 -0.4399671 -4.3759394 -0.5061359]\n","Worker 1 processing data batch 645\n","Worker 1 gradients (layer 0 sample): [ 0.30980873  1.167039   -0.64821005 -2.593825    0.01837988]\n","Aggregated gradients (layer 0 sample): [ 0.58275765  1.027695   -0.5440886  -3.4848824  -0.243878  ]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.0494509  0.03361464]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.05045089 0.03461462]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 646\n","Worker 0 gradients (layer 0 sample): [-0.5071739  -0.8786037   1.073493    5.187709    0.01122348]\n","Worker 1 processing data batch 646\n","Worker 1 gradients (layer 0 sample): [-0.84499097 -0.7632064   1.3909417   7.9565034   0.61390424]\n","Aggregated gradients (layer 0 sample): [-0.67608243 -0.8209051   1.2322173   6.5721064   0.31256387]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.05045089 0.03461462]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.04945089 0.03361464]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 647\n","Worker 0 gradients (layer 0 sample): [ 0.5003411   0.36732358 -0.6493717  -3.668023   -0.41976136]\n","Worker 1 processing data batch 647\n","Worker 1 gradients (layer 0 sample): [ 0.25963044  2.5393152  -0.9463755  -3.7960873   0.19599901]\n","Aggregated gradients (layer 0 sample): [ 0.37998578  1.4533194  -0.7978736  -3.7320552  -0.11188117]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.04945089 0.03361464]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.05045088 0.0346146 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 648\n","Worker 0 gradients (layer 0 sample): [-0.7331065  -0.37747872  0.7687665   6.370612    0.6085963 ]\n","Worker 1 processing data batch 648\n","Worker 1 gradients (layer 0 sample): [-0.7529832 -1.0216049  1.3807528  8.135458   0.330704 ]\n","Aggregated gradients (layer 0 sample): [-0.74304485 -0.6995418   1.0747597   7.253035    0.46965015]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817769 0.15475883 0.05045088 0.0346146 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.04945089 0.03361461]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 649\n","Worker 0 gradients (layer 0 sample): [ 0.25147325  0.37148875 -0.42688376 -2.905672   -0.20149055]\n","Worker 1 processing data batch 649\n","Worker 1 gradients (layer 0 sample): [ 0.29413494  2.371203   -0.97116315 -3.82376     0.1773873 ]\n","Aggregated gradients (layer 0 sample): [ 0.27280408  1.3713459  -0.6990235  -3.364716   -0.01205163]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917768 0.15375884 0.04945089 0.03361461]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817768 0.15475883 0.05045088 0.03461434]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 650\n","Worker 0 gradients (layer 0 sample): [-0.51104844 -0.60826755  1.0026838   5.295556    0.0324595 ]\n","Worker 1 processing data batch 650\n","Worker 1 gradients (layer 0 sample): [-0.624787  -1.2039547  1.2167785  6.503772   0.1500981]\n","Aggregated gradients (layer 0 sample): [-0.5679177 -0.9061111  1.1097312  5.899664   0.0912788]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817768 0.15475883 0.05045088 0.03461434]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917767 0.15375884 0.04945089 0.03361439]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 650, Losses: [2.1245728, 1.9662029]\n","Worker 0 processing data batch 651\n","Worker 0 gradients (layer 0 sample): [ 1.0578424   1.5000418  -0.79556257 -6.6744504  -0.39016253]\n","Worker 1 processing data batch 651\n","Worker 1 gradients (layer 0 sample): [ 0.8587626  1.2085282 -0.632554  -5.1052837 -0.2503995]\n","Aggregated gradients (layer 0 sample): [ 0.9583025   1.354285   -0.7140583  -5.889867   -0.32028103]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917767 0.15375884 0.04945089 0.03361439]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817768 0.15475883 0.05045088 0.03461437]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 652\n","Worker 0 gradients (layer 0 sample): [-0.5423435 -1.3729277  1.3194041  5.611573  -0.3442669]\n","Worker 1 processing data batch 652\n","Worker 1 gradients (layer 0 sample): [-0.39278555 -1.0318544   0.7581851   4.930224    0.34039393]\n","Aggregated gradients (layer 0 sample): [-4.6756452e-01 -1.2023910e+00  1.0387946e+00  5.2708988e+00\n"," -1.9364804e-03]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817768 0.15475883 0.05045088 0.03461437]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917767 0.15375884 0.04945089 0.03561273]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 653\n","Worker 0 gradients (layer 0 sample): [ 0.99494886  0.6884367  -0.83505625 -6.704711   -1.1575434 ]\n","Worker 1 processing data batch 653\n","Worker 1 gradients (layer 0 sample): [ 0.7540337   2.5477557  -1.2789669  -6.3712745   0.48122895]\n","Aggregated gradients (layer 0 sample): [ 0.8744913   1.6180962  -1.0570116  -6.5379925  -0.33815724]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917767 0.15375884 0.04945089 0.03561273]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817768 0.15475883 0.05045088 0.03661272]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 654\n","Worker 0 gradients (layer 0 sample): [-0.9032328  -0.37077904  0.71520036  6.5869064   1.782232  ]\n","Worker 1 processing data batch 654\n","Worker 1 gradients (layer 0 sample): [-0.78554034  0.06140206  0.6987238   6.1704187   1.1628007 ]\n","Aggregated gradients (layer 0 sample): [-0.8443866  -0.15468849  0.7069621   6.3786626   1.4725163 ]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817768 0.15475883 0.05045088 0.03661272]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917766 0.15375884 0.04945089 0.03561272]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 655\n","Worker 0 gradients (layer 0 sample): [ 0.32964996  0.27079868 -0.6548036  -3.4532564  -0.41470277]\n","Worker 1 processing data batch 655\n","Worker 1 gradients (layer 0 sample): [ 0.65470886  1.7588978  -0.89070797 -5.4353685  -0.20231074]\n","Aggregated gradients (layer 0 sample): [ 0.4921794   1.0148482  -0.77275574 -4.4443126  -0.30850676]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917766 0.15375884 0.04945089 0.03561272]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817767 0.15475883 0.05045088 0.03661271]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 656\n","Worker 0 gradients (layer 0 sample): [-0.722842    0.21454966  0.9673883   6.161726    1.53504   ]\n","Worker 1 processing data batch 656\n","Worker 1 gradients (layer 0 sample): [-0.5041795  -1.6497257   0.8419943   5.8094335   0.05094052]\n","Aggregated gradients (layer 0 sample): [-0.6135107  -0.717588    0.9046913   5.9855795   0.79299027]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817767 0.15475883 0.05045088 0.03661271]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917765 0.15375884 0.04945089 0.03561272]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 657\n","Worker 0 gradients (layer 0 sample): [ 0.25991654  2.5819614  -0.6948991  -3.7678733   0.6773953 ]\n","Worker 1 processing data batch 657\n","Worker 1 gradients (layer 0 sample): [ 0.60245234  0.29254606 -0.43148348 -3.7643142  -1.0792302 ]\n","Aggregated gradients (layer 0 sample): [ 0.43118444  1.4372537  -0.5631913  -3.7660937  -0.20091745]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917765 0.15375884 0.04945089 0.03561272]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817766 0.15475883 0.05045088 0.03661269]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 658\n","Worker 0 gradients (layer 0 sample): [-0.5802338 -0.2755146  0.9144207  5.4894705  0.9164022]\n","Worker 1 processing data batch 658\n","Worker 1 gradients (layer 0 sample): [-0.08758377 -0.504873   -0.04598664  0.80973613 -0.24748826]\n","Aggregated gradients (layer 0 sample): [-0.3339088  -0.3901938   0.43421704  3.1496034   0.33445698]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817766 0.15475883 0.05045088 0.03661269]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917765 0.15375884 0.04945089 0.03561271]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 659\n","Worker 0 gradients (layer 0 sample): [ 0.57840174  0.07124022 -0.5594871  -3.159757   -0.58181715]\n","Worker 1 processing data batch 659\n","Worker 1 gradients (layer 0 sample): [ 0.9822656   0.82400596 -0.59255755 -5.282736   -0.40152955]\n","Aggregated gradients (layer 0 sample): [ 0.78033364  0.4476231  -0.5760223  -4.2212462  -0.49167335]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917765 0.15375884 0.04945089 0.03561271]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817766 0.15475883 0.05045088 0.0366127 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 660\n","Worker 0 gradients (layer 0 sample): [-0.22753562 -1.4458144   1.2116525   5.854013   -0.2618541 ]\n","Worker 1 processing data batch 660\n","Worker 1 gradients (layer 0 sample): [-0.98247516 -1.5712415   1.4883784   9.114435    0.67879486]\n","Aggregated gradients (layer 0 sample): [-0.6050054  -1.508528    1.3500154   7.4842243   0.20847037]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817766 0.15475883 0.05045088 0.0366127 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592746 0.00917765 0.15375884 0.04945088 0.03561272]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 660, Losses: [2.272645, 1.9512941]\n","Worker 0 processing data batch 661\n","Worker 0 gradients (layer 0 sample): [ 0.04156516  2.043343   -0.18286304 -1.581743    0.45083454]\n","Worker 1 processing data batch 661\n","Worker 1 gradients (layer 0 sample): [ 0.82326967  2.6344981  -1.0525787  -6.407039   -0.27451113]\n","Aggregated gradients (layer 0 sample): [ 0.43241742  2.3389206  -0.61772084 -3.994391    0.08816171]\n","Parameters before update (layer 0 sample): [0.14592746 0.00917765 0.15375884 0.04945088 0.03561272]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492747 0.00817766 0.15475883 0.05045087 0.03461276]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 662\n","Worker 0 gradients (layer 0 sample): [-0.00540281 -1.6154605   1.01954     2.5879796  -0.49413723]\n","Worker 1 processing data batch 662\n","Worker 1 gradients (layer 0 sample): [-0.07091302 -1.9297782   0.71097976  3.2247858  -0.7455027 ]\n","Aggregated gradients (layer 0 sample): [-0.03815792 -1.7726194   0.8652599   2.9063826  -0.61982   ]\n","Parameters before update (layer 0 sample): [0.14492747 0.00817766 0.15475883 0.05045087 0.03461276]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.00917765 0.15375884 0.04945088 0.03561275]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 663\n","Worker 0 gradients (layer 0 sample): [ 5.3777182e-01  1.5283211e+00 -8.7395024e-01 -4.1695542e+00\n"," -1.7607249e-03]\n","Worker 1 processing data batch 663\n","Worker 1 gradients (layer 0 sample): [ 0.605334    1.5892857  -0.56329095 -3.6440928  -0.1279293 ]\n","Aggregated gradients (layer 0 sample): [ 0.5715529   1.5588034  -0.7186206  -3.9068236  -0.06484501]\n","Parameters before update (layer 0 sample): [0.14592738 0.00917765 0.15375884 0.04945088 0.03561275]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449274  0.00817766 0.15475883 0.05045087 0.03661269]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 664\n","Worker 0 gradients (layer 0 sample): [-1.5917575  -0.13651264  1.4093931  11.97514     2.5450878 ]\n","Worker 1 processing data batch 664\n","Worker 1 gradients (layer 0 sample): [-0.99341536  0.10631171  1.1836643   8.212921    1.2867167 ]\n","Aggregated gradients (layer 0 sample): [-1.2925864  -0.01510046  1.2965287  10.09403     1.9159023 ]\n","Parameters before update (layer 0 sample): [0.1449274  0.00817766 0.15475883 0.05045087 0.03661269]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.00917745 0.15375884 0.04945088 0.0356127 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 665\n","Worker 0 gradients (layer 0 sample): [ 0.43374854  0.53126496 -0.21779327 -2.9965913  -0.37143022]\n","Worker 1 processing data batch 665\n","Worker 1 gradients (layer 0 sample): [ 0.72143567  0.6591333  -0.5914451  -5.3861313  -0.76592124]\n","Aggregated gradients (layer 0 sample): [ 0.57759213  0.5951991  -0.4046192  -4.1913614  -0.56867576]\n","Parameters before update (layer 0 sample): [0.14592738 0.00917745 0.15375884 0.04945088 0.0356127 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449274  0.00817746 0.15475883 0.05045087 0.03661269]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 666\n","Worker 0 gradients (layer 0 sample): [ 0.03066692 -0.42998728  0.2571474   1.3785318  -0.0737251 ]\n","Worker 1 processing data batch 666\n","Worker 1 gradients (layer 0 sample): [-9.6260273e-01  6.4317062e-03  2.0532721e-01  6.5920248e+00\n","  2.6039424e+00]\n","Aggregated gradients (layer 0 sample): [-0.4659679  -0.21177779  0.2312373   3.9852784   1.2651086 ]\n","Parameters before update (layer 0 sample): [0.1449274  0.00817746 0.15475883 0.05045087 0.03661269]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.00917744 0.15375885 0.04945088 0.0356127 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 667\n","Worker 0 gradients (layer 0 sample): [ 0.8953041  -0.04683539 -0.0811215  -5.0478764  -2.015696  ]\n","Worker 1 processing data batch 667\n","Worker 1 gradients (layer 0 sample): [ 0.34225824 -0.28246504  0.28656554 -1.1049988  -0.22109038]\n","Aggregated gradients (layer 0 sample): [ 0.61878115 -0.16465022  0.10272202 -3.0764375  -1.1183932 ]\n","Parameters before update (layer 0 sample): [0.14592738 0.00917744 0.15375885 0.04945088 0.0356127 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449274  0.01017741 0.1527589  0.05045087 0.03661269]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 668\n","Worker 0 gradients (layer 0 sample): [-0.3112728   0.4701378  -0.11132313  1.9934053   0.67375493]\n","Worker 1 processing data batch 668\n","Worker 1 gradients (layer 0 sample): [-0.66733456 -0.16143468  0.4912952   5.8704715   1.5826497 ]\n","Aggregated gradients (layer 0 sample): [-0.48930368  0.15435156  0.18998602  3.9319384   1.1282023 ]\n","Parameters before update (layer 0 sample): [0.1449274  0.01017741 0.1527589  0.05045087 0.03661269]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.00917744 0.15175892 0.04945088 0.0356127 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 669\n","Worker 0 gradients (layer 0 sample): [ 0.63985425  0.6921458  -0.36784348 -4.3986607  -0.95750576]\n","Worker 1 processing data batch 669\n","Worker 1 gradients (layer 0 sample): [ 0.24725682 -0.2563271  -0.19931726 -1.7727965  -0.63553333]\n","Aggregated gradients (layer 0 sample): [ 0.44355553  0.21790937 -0.28358036 -3.0857286  -0.7965195 ]\n","Parameters before update (layer 0 sample): [0.14592738 0.00917744 0.15175892 0.04945088 0.0356127 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449274  0.00817746 0.15275891 0.05045087 0.03661269]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 670\n","Worker 0 gradients (layer 0 sample): [-0.9678911  -0.919624    1.4386855   8.453093    0.73808014]\n","Worker 1 processing data batch 670\n","Worker 1 gradients (layer 0 sample): [-0.83790374  0.1018135   0.62134314  4.729247    0.95111954]\n","Aggregated gradients (layer 0 sample): [-0.9028974  -0.40890524  1.0300143   6.59117     0.84459984]\n","Parameters before update (layer 0 sample): [0.1449274  0.00817746 0.15275891 0.05045087 0.03661269]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.00917744 0.15175892 0.04945087 0.0356127 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 670, Losses: [1.7857436, 1.7931942]\n","Worker 0 processing data batch 671\n","Worker 0 gradients (layer 0 sample): [ 0.66668487  0.9483318  -0.9586052  -4.8245306   0.3439402 ]\n","Worker 1 processing data batch 671\n","Worker 1 gradients (layer 0 sample): [ 0.06500247 -0.29949066 -0.35981426 -1.2413632  -0.09071928]\n","Aggregated gradients (layer 0 sample): [ 0.36584365  0.32442057 -0.6592097  -3.0329468   0.12661046]\n","Parameters before update (layer 0 sample): [0.14592738 0.00917744 0.15175892 0.04945087 0.0356127 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449274  0.00817746 0.15275891 0.05045087 0.03461273]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 672\n","Worker 0 gradients (layer 0 sample): [-0.47790414 -1.3760943   0.8013827   4.825979   -0.05292852]\n","Worker 1 processing data batch 672\n","Worker 1 gradients (layer 0 sample): [-0.57601273 -0.26936075  1.3528792   6.334509    0.2574408 ]\n","Aggregated gradients (layer 0 sample): [-0.52695847 -0.82272756  1.0771309   5.580244    0.10225614]\n","Parameters before update (layer 0 sample): [0.1449274  0.00817746 0.15275891 0.05045087 0.03461273]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.00917745 0.15175892 0.04945087 0.03361277]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 673\n","Worker 0 gradients (layer 0 sample): [ 1.0702434  -0.8127412  -0.69923246 -6.1846514  -1.1864871 ]\n","Worker 1 processing data batch 673\n","Worker 1 gradients (layer 0 sample): [ 0.740717   -0.99363667 -0.37573916 -2.344996   -0.6281763 ]\n","Aggregated gradients (layer 0 sample): [ 0.90548015 -0.90318894 -0.53748584 -4.264824   -0.9073317 ]\n","Parameters before update (layer 0 sample): [0.14592738 0.00917745 0.15175892 0.04945087 0.03361277]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449274  0.01017744 0.15275891 0.05045087 0.03461276]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 674\n","Worker 0 gradients (layer 0 sample): [-0.66621363  0.31681687  0.455837    4.9999466   0.4396739 ]\n","Worker 1 processing data batch 674\n","Worker 1 gradients (layer 0 sample): [-0.25922576 -0.88942015  0.09824274  2.1493511  -0.08898574]\n","Aggregated gradients (layer 0 sample): [-0.46271968 -0.28630164  0.2770399   3.5746489   0.17534408]\n","Parameters before update (layer 0 sample): [0.1449274  0.01017744 0.15275891 0.05045087 0.03461276]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592738 0.01117742 0.15175892 0.04945087 0.03361278]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 675\n","Worker 0 gradients (layer 0 sample): [ 0.59297365  2.9029102  -1.0756987  -5.470997    0.27157837]\n","Worker 1 processing data batch 675\n","Worker 1 gradients (layer 0 sample): [ 0.31259805  2.2846355  -0.6304482  -3.5824637   0.16227743]\n","Aggregated gradients (layer 0 sample): [ 0.45278585  2.593773   -0.8530735  -4.5267305   0.2169279 ]\n","Parameters before update (layer 0 sample): [0.14592738 0.01117742 0.15175892 0.04945087 0.03361278]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1449274  0.01017743 0.15275891 0.05045087 0.0326128 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 676\n","Worker 0 gradients (layer 0 sample): [-0.04148431 -1.8185942   1.2494359   4.0975394  -0.24349359]\n","Worker 1 processing data batch 676\n","Worker 1 gradients (layer 0 sample): [0.09665859 0.2972543  0.02847893 0.24818164 0.1183348 ]\n","Aggregated gradients (layer 0 sample): [ 0.02758714 -0.76066995  0.63895744  2.1728606  -0.06257939]\n","Parameters before update (layer 0 sample): [0.1449274  0.01017743 0.15275891 0.05045087 0.0326128 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392751 0.01117742 0.15175892 0.04945087 0.03361275]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 677\n","Worker 0 gradients (layer 0 sample): [ 0.00624245  2.5050929  -0.7199048  -2.4603395   0.00530801]\n","Worker 1 processing data batch 677\n","Worker 1 gradients (layer 0 sample): [ 0.8378835   1.0407214  -0.80142355 -5.461486   -0.47682458]\n","Aggregated gradients (layer 0 sample): [ 0.42206296  1.7729071  -0.76066417 -3.9609127  -0.23575829]\n","Parameters before update (layer 0 sample): [0.14392751 0.01117742 0.15175892 0.04945087 0.03361275]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292753 0.01017743 0.15275891 0.05045087 0.03461273]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 678\n","Worker 0 gradients (layer 0 sample): [-0.42263314 -1.2707815   0.5333298   4.33689    -0.10153881]\n","Worker 1 processing data batch 678\n","Worker 1 gradients (layer 0 sample): [-0.36939588 -0.564312    0.5904139   4.698253    0.0589453 ]\n","Aggregated gradients (layer 0 sample): [-0.3960145  -0.91754675  0.5618719   4.5175714  -0.02129676]\n","Parameters before update (layer 0 sample): [0.14292753 0.01017743 0.15275891 0.05045087 0.03461273]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392751 0.01117742 0.15175892 0.04945087 0.03561257]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 679\n","Worker 0 gradients (layer 0 sample): [ 0.62886995  2.3599524  -1.2701528  -6.208373    0.1913133 ]\n","Worker 1 processing data batch 679\n","Worker 1 gradients (layer 0 sample): [ 0.05101206  3.275601   -0.59343255 -2.3405302   0.850695  ]\n","Aggregated gradients (layer 0 sample): [ 0.339941    2.8177767  -0.9317927  -4.2744517   0.52100414]\n","Parameters before update (layer 0 sample): [0.14392751 0.01117742 0.15175892 0.04945087 0.03561257]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292753 0.01017742 0.15275891 0.05045087 0.03461258]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 680\n","Worker 0 gradients (layer 0 sample): [-0.22448587 -2.2876973   1.3304679   5.096988   -0.32407427]\n","Worker 1 processing data batch 680\n","Worker 1 gradients (layer 0 sample): [-0.46456128 -1.3311669   1.193598    5.833681   -0.23874548]\n","Aggregated gradients (layer 0 sample): [-0.34452358 -1.809432    1.262033    5.465335   -0.28140986]\n","Parameters before update (layer 0 sample): [0.14292753 0.01017742 0.15275891 0.05045087 0.03461258]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392751 0.01117742 0.15175892 0.04945087 0.03561257]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 680, Losses: [2.0863063, 1.7596948]\n","Worker 0 processing data batch 681\n","Worker 0 gradients (layer 0 sample): [ 0.4630835  3.2536702 -1.0227085 -4.930708   0.6638662]\n","Worker 1 processing data batch 681\n","Worker 1 gradients (layer 0 sample): [ 0.05790844  2.3004296  -0.55161583 -2.4748497   0.2914362 ]\n","Aggregated gradients (layer 0 sample): [ 0.26049596  2.77705    -0.7871622  -3.7027788   0.4776512 ]\n","Parameters before update (layer 0 sample): [0.14392751 0.01117742 0.15175892 0.04945087 0.03561257]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292753 0.01017742 0.15275891 0.05045087 0.03461258]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 682\n","Worker 0 gradients (layer 0 sample): [-0.22173226 -1.4592218   0.81097806  2.782021   -0.5861745 ]\n","Worker 1 processing data batch 682\n","Worker 1 gradients (layer 0 sample): [-0.5462501  -1.5717735   1.1452098   6.693284    0.08518218]\n","Aggregated gradients (layer 0 sample): [-0.38399118 -1.5154977   0.9780939   4.737653   -0.25049615]\n","Parameters before update (layer 0 sample): [0.14292753 0.01017742 0.15275891 0.05045087 0.03461258]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392751 0.01117742 0.15175892 0.04945087 0.03561256]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 683\n","Worker 0 gradients (layer 0 sample): [ 0.05217842  1.6428131  -0.6675927  -1.2984512   0.86297077]\n","Worker 1 processing data batch 683\n","Worker 1 gradients (layer 0 sample): [ 0.1250668   2.4761732  -0.6123446  -2.3329177   0.30758184]\n","Aggregated gradients (layer 0 sample): [ 0.08862261  2.059493   -0.63996863 -1.8156844   0.5852763 ]\n","Parameters before update (layer 0 sample): [0.14392751 0.01117742 0.15175892 0.04945087 0.03561256]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292756 0.01017742 0.15275891 0.05045087 0.03461257]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 684\n","Worker 0 gradients (layer 0 sample): [-0.65457207 -1.4389079   1.3789635   7.098526    0.14639926]\n","Worker 1 processing data batch 684\n","Worker 1 gradients (layer 0 sample): [-0.52230614 -2.6962967   1.3069818   7.1205196  -0.45312333]\n","Aggregated gradients (layer 0 sample): [-0.5884391  -2.0676022   1.3429726   7.109523   -0.15336204]\n","Parameters before update (layer 0 sample): [0.14292756 0.01017742 0.15275891 0.05045087 0.03461257]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392754 0.01117742 0.15175892 0.04945087 0.03561255]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 685\n","Worker 0 gradients (layer 0 sample): [-0.10076266  0.826714   -0.26017487 -1.0254524   0.08992033]\n","Worker 1 processing data batch 685\n","Worker 1 gradients (layer 0 sample): [ 0.31861436  0.72602975 -0.5566572  -3.3264673  -0.17512685]\n","Aggregated gradients (layer 0 sample): [ 0.10892585  0.77637184 -0.40841603 -2.1759598  -0.04260326]\n","Parameters before update (layer 0 sample): [0.14392754 0.01117742 0.15175892 0.04945087 0.03561255]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292759 0.01017743 0.15275891 0.05045086 0.03661247]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 686\n","Worker 0 gradients (layer 0 sample): [-0.33242595 -1.4067385   1.2344313   4.7777443  -0.45328525]\n","Worker 1 processing data batch 686\n","Worker 1 gradients (layer 0 sample): [-0.33690035 -2.0588083   1.3748282   5.0219593  -0.7235434 ]\n","Aggregated gradients (layer 0 sample): [-0.33466315 -1.7327734   1.3046298   4.899852   -0.5884143 ]\n","Parameters before update (layer 0 sample): [0.14292759 0.01017743 0.15275891 0.05045086 0.03661247]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392757 0.01117742 0.15175892 0.04945087 0.03761245]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 687\n","Worker 0 gradients (layer 0 sample): [ 0.2618303  1.8903098 -0.7085899 -3.2737155  0.394673 ]\n","Worker 1 processing data batch 687\n","Worker 1 gradients (layer 0 sample): [ 0.5005261   2.2826066  -0.71376663 -5.13666     0.18349692]\n","Aggregated gradients (layer 0 sample): [ 0.3811782   2.0864582  -0.7111783  -4.205188    0.28908497]\n","Parameters before update (layer 0 sample): [0.14392757 0.01117742 0.15175892 0.04945087 0.03761245]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292759 0.01017743 0.15275891 0.05045086 0.03661247]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 688\n","Worker 0 gradients (layer 0 sample): [-0.16188088 -1.6528795   0.8952657   3.3517644  -0.8825907 ]\n","Worker 1 processing data batch 688\n","Worker 1 gradients (layer 0 sample): [-0.46471882 -1.5464994   0.8742246   5.5364575   0.18106964]\n","Aggregated gradients (layer 0 sample): [-0.31329983 -1.5996895   0.8847451   4.444111   -0.35076052]\n","Parameters before update (layer 0 sample): [0.14292759 0.01017743 0.15275891 0.05045086 0.03661247]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392757 0.01117742 0.15175892 0.04945087 0.03761245]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 689\n","Worker 0 gradients (layer 0 sample): [ 0.0854638  2.2490573 -0.8220738 -2.525064   1.143723 ]\n","Worker 1 processing data batch 689\n","Worker 1 gradients (layer 0 sample): [ 0.18573606  3.399405   -0.6957745  -3.1041393   1.0957688 ]\n","Aggregated gradients (layer 0 sample): [ 0.13559993  2.8242311  -0.7589241  -2.8146017   1.119746  ]\n","Parameters before update (layer 0 sample): [0.14392757 0.01117742 0.15175892 0.04945087 0.03761245]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.1429276  0.01017743 0.15275891 0.05045086 0.03661246]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 690\n","Worker 0 gradients (layer 0 sample): [-0.2747885  -1.0282412   0.34142745  2.58497    -0.03033737]\n","Worker 1 processing data batch 690\n","Worker 1 gradients (layer 0 sample): [-0.05433489 -2.156364    0.94856757  3.1112385  -1.3394134 ]\n","Aggregated gradients (layer 0 sample): [-0.16456169 -1.5923026   0.6449975   2.8481042  -0.68487537]\n","Parameters before update (layer 0 sample): [0.1429276  0.01017743 0.15275891 0.05045086 0.03661246]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392757 0.01117742 0.15175892 0.04945087 0.03761245]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 690, Losses: [1.7928162, 1.9443703]\n","Worker 0 processing data batch 691\n","Worker 0 gradients (layer 0 sample): [ 0.01660885  3.6073914  -0.89349353 -3.3170118   1.5855153 ]\n","Worker 1 processing data batch 691\n","Worker 1 gradients (layer 0 sample): [ 0.7477536   2.3954027  -1.049676   -6.7392898  -0.08435333]\n","Aggregated gradients (layer 0 sample): [ 0.38218123  3.0013971  -0.97158474 -5.0281506   0.75058097]\n","Parameters before update (layer 0 sample): [0.14392757 0.01117742 0.15175892 0.04945087 0.03761245]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292759 0.01017742 0.15275891 0.05045086 0.03661246]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 692\n","Worker 0 gradients (layer 0 sample): [-0.39798886 -1.0465524   0.99271715  5.1920385  -0.47449565]\n","Worker 1 processing data batch 692\n","Worker 1 gradients (layer 0 sample): [-0.6236327   0.04741114  0.49151024  4.511826    0.84946144]\n","Aggregated gradients (layer 0 sample): [-0.5108108  -0.49957064  0.7421137   4.8519325   0.1874829 ]\n","Parameters before update (layer 0 sample): [0.14292759 0.01017742 0.15275891 0.05045086 0.03661246]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392757 0.01117741 0.15175892 0.04945087 0.03561248]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 693\n","Worker 0 gradients (layer 0 sample): [ 0.46285388  1.6403711  -0.7139761  -3.793594   -0.11952233]\n","Worker 1 processing data batch 693\n","Worker 1 gradients (layer 0 sample): [ 0.37651828  0.7596461  -0.28960073 -2.746174   -0.2567445 ]\n","Aggregated gradients (layer 0 sample): [ 0.41968608  1.2000086  -0.5017884  -3.269884   -0.18813342]\n","Parameters before update (layer 0 sample): [0.14392757 0.01117741 0.15175892 0.04945087 0.03561248]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292759 0.01017742 0.15275891 0.05045086 0.03661246]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 694\n","Worker 0 gradients (layer 0 sample): [-1.0303762  -2.3142471   1.8688004  10.3657      0.14622112]\n","Worker 1 processing data batch 694\n","Worker 1 gradients (layer 0 sample): [-0.19183443 -1.9493836   1.2318864   4.196842   -1.0232748 ]\n","Aggregated gradients (layer 0 sample): [-0.6111053  -2.1318154   1.5503434   7.281271   -0.43852684]\n","Parameters before update (layer 0 sample): [0.14292759 0.01017742 0.15275891 0.05045086 0.03661246]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392757 0.01117741 0.15175892 0.04945086 0.03761245]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 695\n","Worker 0 gradients (layer 0 sample): [ 0.33978626  1.7901688  -0.37189686 -2.411193    0.25379407]\n","Worker 1 processing data batch 695\n","Worker 1 gradients (layer 0 sample): [ 0.8348341  2.9572458 -1.2570194 -6.846058   0.4876176]\n","Aggregated gradients (layer 0 sample): [ 0.5873102   2.3737073  -0.81445813 -4.6286254   0.37070584]\n","Parameters before update (layer 0 sample): [0.14392757 0.01117741 0.15175892 0.04945086 0.03761245]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292759 0.01017742 0.15275891 0.05045085 0.03661246]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 696\n","Worker 0 gradients (layer 0 sample): [-0.6217706  -0.1194818   0.5586617   4.7459564   0.32407808]\n","Worker 1 processing data batch 696\n","Worker 1 gradients (layer 0 sample): [-0.14053881 -1.6939993   0.6176933   3.040948   -0.3497241 ]\n","Aggregated gradients (layer 0 sample): [-0.38115472 -0.90674055  0.5881775   3.8934522  -0.01282302]\n","Parameters before update (layer 0 sample): [0.14292759 0.01017742 0.15275891 0.05045085 0.03661246]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392757 0.01117741 0.15175892 0.04945086 0.03761221]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 697\n","Worker 0 gradients (layer 0 sample): [ 0.22293141  1.4783475  -0.25101295 -1.6074466   0.10884455]\n","Worker 1 processing data batch 697\n","Worker 1 gradients (layer 0 sample): [ 0.9798814   0.6979691  -0.48224896 -5.266635   -0.89794374]\n","Aggregated gradients (layer 0 sample): [ 0.6014064   1.0881584  -0.36663097 -3.4370408  -0.3945496 ]\n","Parameters before update (layer 0 sample): [0.14392757 0.01117741 0.15175892 0.04945086 0.03761221]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292759 0.01017742 0.15275891 0.05045085 0.03861219]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 698\n","Worker 0 gradients (layer 0 sample): [-0.77641714 -0.8014842   0.83188283  6.48132     0.9110459 ]\n","Worker 1 processing data batch 698\n","Worker 1 gradients (layer 0 sample): [-0.94736093  0.9383429   0.28338966  6.3222156   2.9611163 ]\n","Aggregated gradients (layer 0 sample): [-0.861889    0.06842935  0.55763626  6.4017677   1.9360812 ]\n","Parameters before update (layer 0 sample): [0.14292759 0.01017742 0.15275891 0.05045085 0.03861219]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392757 0.00917747 0.15175892 0.04945086 0.0376122 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 699\n","Worker 0 gradients (layer 0 sample): [ 0.5456984  -0.7377463   0.05001107 -2.59093    -1.1324091 ]\n","Worker 1 processing data batch 699\n","Worker 1 gradients (layer 0 sample): [ 0.41335148 -0.33879524  0.16065952 -1.7699056  -0.86685276]\n","Aggregated gradients (layer 0 sample): [ 0.47952494 -0.5382708   0.1053353  -2.1804178  -0.9996309 ]\n","Parameters before update (layer 0 sample): [0.14392757 0.00917747 0.15175892 0.04945086 0.0376122 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292759 0.01017746 0.15075897 0.05045085 0.03861219]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 700\n","Worker 0 gradients (layer 0 sample): [-0.43269247  0.7551286  -0.3053388   1.7891601   1.1908741 ]\n","Worker 1 processing data batch 700\n","Worker 1 gradients (layer 0 sample): [-0.49627078  0.15742388 -0.46935636  2.5040195   1.9709518 ]\n","Aggregated gradients (layer 0 sample): [-0.46448162  0.45627624 -0.38734758  2.1465898   1.580913  ]\n","Parameters before update (layer 0 sample): [0.14292759 0.01017746 0.15075897 0.05045085 0.03861219]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392757 0.00917747 0.15175895 0.04945086 0.0376122 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 700, Losses: [1.7206359, 1.8860877]\n","Worker 0 processing data batch 701\n","Worker 0 gradients (layer 0 sample): [ 0.3062709  -0.6048821  -0.16944215 -1.272048   -1.10726   ]\n","Worker 1 processing data batch 701\n","Worker 1 gradients (layer 0 sample): [ 0.3135172  -0.57551426  0.22653106 -1.4182733  -1.2987258 ]\n","Aggregated gradients (layer 0 sample): [ 0.30989406 -0.59019816  0.02854446 -1.3451607  -1.2029929 ]\n","Parameters before update (layer 0 sample): [0.14392757 0.00917747 0.15175895 0.04945086 0.0376122 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14292759 0.01017746 0.15075907 0.05045085 0.03861219]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 702\n","Worker 0 gradients (layer 0 sample): [ 0.14637202  1.1312461  -0.8131418  -3.1098742   1.0335088 ]\n","Worker 1 processing data batch 702\n","Worker 1 gradients (layer 0 sample): [-0.14811967  2.0384161  -0.77380896 -1.8409123   1.1321521 ]\n","Aggregated gradients (layer 0 sample): [-8.7382644e-04  1.5848311e+00 -7.9347539e-01 -2.4753933e+00\n","  1.0828304e+00]\n","Parameters before update (layer 0 sample): [0.14292759 0.01017746 0.15075907 0.05045085 0.03861219]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392397 0.00917747 0.15175906 0.05145084 0.0376122 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 703\n","Worker 0 gradients (layer 0 sample): [-1.3019118  -1.8124225   1.7261021  12.481593    0.78189534]\n","Worker 1 processing data batch 703\n","Worker 1 gradients (layer 0 sample): [-0.98170304 -2.4984846   2.3053265  12.987454   -0.05669929]\n","Aggregated gradients (layer 0 sample): [-1.1418074  -2.1554537   2.0157142  12.734524    0.36259803]\n","Parameters before update (layer 0 sample): [0.14392397 0.00917747 0.15175906 0.05145084 0.0376122 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.01017746 0.15075907 0.05045085 0.03661222]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 704\n","Worker 0 gradients (layer 0 sample): [ 0.19039439  1.4690616  -0.566955   -3.4551597   0.09420058]\n","Worker 1 processing data batch 704\n","Worker 1 gradients (layer 0 sample): [ 0.56435704  1.4207015  -0.65865386 -4.771056    0.0210242 ]\n","Aggregated gradients (layer 0 sample): [ 0.37737572  1.4448816  -0.6128044  -4.1131077   0.05761239]\n","Parameters before update (layer 0 sample): [0.14492396 0.01017746 0.15075907 0.05045085 0.03661222]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14392397 0.00917747 0.15175906 0.05145084 0.03561228]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 705\n","Worker 0 gradients (layer 0 sample): [-0.67652655 -2.7649403   2.2905006   9.988117   -0.45733333]\n","Worker 1 processing data batch 705\n","Worker 1 gradients (layer 0 sample): [-0.56546617 -2.7865808   2.2785954  10.900217   -0.30930284]\n","Aggregated gradients (layer 0 sample): [-0.62099636 -2.7757607   2.284548   10.444167   -0.38331807]\n","Parameters before update (layer 0 sample): [0.14392397 0.00917747 0.15175906 0.05145084 0.03561228]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.01017746 0.15075907 0.05045084 0.03661226]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 706\n","Worker 0 gradients (layer 0 sample): [-0.5358552   2.0329056  -0.38311645  1.8185703   0.77929044]\n","Worker 1 processing data batch 706\n","Worker 1 gradients (layer 0 sample): [-0.19888452  0.4951353  -0.3318283   0.77814794  0.10690281]\n","Aggregated gradients (layer 0 sample): [-0.36736983  1.2640204  -0.35747236  1.2983592   0.44309664]\n","Parameters before update (layer 0 sample): [0.14492396 0.01017746 0.15075907 0.05045084 0.03661226]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917747 0.15175906 0.04945085 0.03561228]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 707\n","Worker 0 gradients (layer 0 sample): [ 1.3679197 -1.182426  -0.130343  -6.134207  -1.1948702]\n","Worker 1 processing data batch 707\n","Worker 1 gradients (layer 0 sample): [ 0.15558746 -0.79346657  0.5418089   0.224267   -0.69584095]\n","Aggregated gradients (layer 0 sample): [ 0.76175356 -0.9879463   0.20573294 -2.95497    -0.9453556 ]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917747 0.15175906 0.04945085 0.03561228]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.01017746 0.15075909 0.05045084 0.03661227]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 708\n","Worker 0 gradients (layer 0 sample): [-0.4498855   0.48511988 -0.16696317  2.4415984   0.19497693]\n","Worker 1 processing data batch 708\n","Worker 1 gradients (layer 0 sample): [-0.38246605  0.9752244  -0.39886475  1.4537215   0.41928414]\n","Aggregated gradients (layer 0 sample): [-0.41617578  0.73017216 -0.28291395  1.94766     0.30713052]\n","Parameters before update (layer 0 sample): [0.14492396 0.01017746 0.15075909 0.05045084 0.03661227]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917747 0.15175907 0.04945085 0.03561229]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 709\n","Worker 0 gradients (layer 0 sample): [ 0.28146976 -0.93746346 -0.27044147 -1.1120085  -0.5605196 ]\n","Worker 1 processing data batch 709\n","Worker 1 gradients (layer 0 sample): [ 0.4563365  -0.5526409  -0.33380747 -2.0160732  -0.6115182 ]\n","Aggregated gradients (layer 0 sample): [ 0.36890313 -0.7450522  -0.30212447 -1.5640409  -0.5860189 ]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917747 0.15175907 0.04945085 0.03561229]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.01017746 0.15275906 0.05045084 0.03661227]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 710\n","Worker 0 gradients (layer 0 sample): [-0.9178998  -1.3194332   1.1655391   7.2485714  -0.02174047]\n","Worker 1 processing data batch 710\n","Worker 1 gradients (layer 0 sample): [-0.01902902  1.4061718   0.18424606 -0.17232776  0.45605296]\n","Aggregated gradients (layer 0 sample): [-0.4684644   0.04336929  0.6748926   3.5381217   0.21715625]\n","Parameters before update (layer 0 sample): [0.14492396 0.01017746 0.15275906 0.05045084 0.03661227]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917754 0.15175907 0.04945085 0.03561229]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 710, Losses: [1.7242507, 1.9556322]\n","Worker 0 processing data batch 711\n","Worker 0 gradients (layer 0 sample): [ 0.79733163 -0.20323446 -0.5404059  -5.097939   -0.65153164]\n","Worker 1 processing data batch 711\n","Worker 1 gradients (layer 0 sample): [ 0.97280246  1.2282476  -1.3402878  -7.1752195  -0.32573164]\n","Aggregated gradients (layer 0 sample): [ 0.88506705  0.5125066  -0.94034684 -6.1365795  -0.48863164]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917754 0.15175907 0.04945085 0.03561229]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.00817756 0.15275906 0.05045085 0.03661228]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 712\n","Worker 0 gradients (layer 0 sample): [-0.49110195 -1.400182    1.4973669   6.72935    -0.20321661]\n","Worker 1 processing data batch 712\n","Worker 1 gradients (layer 0 sample): [-0.7116891  -1.2937098   1.0012264   6.1546383  -0.05996624]\n","Aggregated gradients (layer 0 sample): [-0.60139555 -1.3469459   1.2492967   6.441994   -0.13159142]\n","Parameters before update (layer 0 sample): [0.14492396 0.00817756 0.15275906 0.05045085 0.03661228]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917755 0.15175907 0.04945085 0.03761225]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 713\n","Worker 0 gradients (layer 0 sample): [ 0.40329596  2.0808077  -0.97591853 -4.5407357   0.7145146 ]\n","Worker 1 processing data batch 713\n","Worker 1 gradients (layer 0 sample): [ 0.5709507   1.3942205  -0.9064349  -4.93629     0.59570843]\n","Aggregated gradients (layer 0 sample): [ 0.4871233   1.737514   -0.9411767  -4.738513    0.65511155]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917755 0.15175907 0.04945085 0.03761225]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.00817755 0.15275906 0.05045084 0.03661226]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 714\n","Worker 0 gradients (layer 0 sample): [-0.66565406 -1.1433537   1.4575998   7.6193314   0.34745866]\n","Worker 1 processing data batch 714\n","Worker 1 gradients (layer 0 sample): [-0.7310269  -1.3037772   1.8546425   8.430133    0.07399963]\n","Aggregated gradients (layer 0 sample): [-0.6983405  -1.2235655   1.6561211   8.024733    0.21072915]\n","Parameters before update (layer 0 sample): [0.14492396 0.00817755 0.15275906 0.05045084 0.03661226]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917755 0.15175907 0.04945085 0.03561228]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 715\n","Worker 0 gradients (layer 0 sample): [ 0.9245355   0.17988445 -0.9101534  -6.350306   -0.7662988 ]\n","Worker 1 processing data batch 715\n","Worker 1 gradients (layer 0 sample): [ 0.6580493   0.33335438 -0.60601455 -4.9192057  -0.42684233]\n","Aggregated gradients (layer 0 sample): [ 0.7912924   0.25661942 -0.75808394 -5.634756   -0.59657055]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917755 0.15175907 0.04945085 0.03561228]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.00817756 0.15275906 0.05045084 0.03661227]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 716\n","Worker 0 gradients (layer 0 sample): [-0.8270544  -0.82245225  1.0002942   6.498195    0.07095134]\n","Worker 1 processing data batch 716\n","Worker 1 gradients (layer 0 sample): [-1.0057231  -1.4055939   1.68015     9.205925    0.07691123]\n","Aggregated gradients (layer 0 sample): [-0.91638875 -1.1140231   1.3402221   7.8520603   0.07393129]\n","Parameters before update (layer 0 sample): [0.14492396 0.00817756 0.15275906 0.05045084 0.03661227]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917755 0.15175907 0.04945084 0.03561232]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 717\n","Worker 0 gradients (layer 0 sample): [  1.4713902    2.2820475   -1.6702528  -10.145592    -0.16901448]\n","Worker 1 processing data batch 717\n","Worker 1 gradients (layer 0 sample): [ 0.590024    1.242293   -0.9079417  -4.0288258   0.20525417]\n","Aggregated gradients (layer 0 sample): [ 1.0307071   1.7621703  -1.2890973  -7.0872087   0.01811984]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917755 0.15175907 0.04945084 0.03561232]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.00817756 0.15275906 0.05045084 0.0346125 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 718\n","Worker 0 gradients (layer 0 sample): [-0.42998478 -1.3188083   1.0561428   4.3842974  -0.26602912]\n","Worker 1 processing data batch 718\n","Worker 1 gradients (layer 0 sample): [-0.8601097  -1.9370024   1.1778063   7.8694963  -0.06012043]\n","Aggregated gradients (layer 0 sample): [-0.64504725 -1.6279054   1.1169746   6.126897   -0.16307478]\n","Parameters before update (layer 0 sample): [0.14492396 0.00817756 0.15275906 0.05045084 0.0346125 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917755 0.15175907 0.04945084 0.03561247]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 719\n","Worker 0 gradients (layer 0 sample): [ 0.14043638  2.1368191  -0.87915957 -2.611282    0.7700274 ]\n","Worker 1 processing data batch 719\n","Worker 1 gradients (layer 0 sample): [ 0.8111759  2.9394917 -1.1077617 -7.2491508  0.5908247]\n","Aggregated gradients (layer 0 sample): [ 0.47580612  2.5381556  -0.99346066 -4.9302163   0.68042606]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917755 0.15175907 0.04945084 0.03561247]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.00817756 0.15275906 0.05045084 0.03461248]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 720\n","Worker 0 gradients (layer 0 sample): [-0.15813096 -0.6650244   0.06654605  0.68978834 -0.21747053]\n","Worker 1 processing data batch 720\n","Worker 1 gradients (layer 0 sample): [-0.10341648 -0.7677171   0.9171896   3.0837715  -0.15723659]\n","Aggregated gradients (layer 0 sample): [-0.13077372 -0.71637076  0.49186784  1.8867799  -0.18735355]\n","Parameters before update (layer 0 sample): [0.14492396 0.00817756 0.15275906 0.05045084 0.03461248]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917755 0.15175907 0.04945084 0.03561246]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 720, Losses: [1.576744, 1.9743016]\n","Worker 0 processing data batch 721\n","Worker 0 gradients (layer 0 sample): [  1.8249509    3.3445926   -1.8042947  -12.210264    -0.20869142]\n","Worker 1 processing data batch 721\n","Worker 1 gradients (layer 0 sample): [ 0.6798178   1.3733063  -1.2558862  -6.630936    0.08216184]\n","Aggregated gradients (layer 0 sample): [ 1.2523844   2.3589494  -1.5300905  -9.4206     -0.06326479]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917755 0.15175907 0.04945084 0.03561246]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817756 0.15275906 0.05045084 0.0366124 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 722\n","Worker 0 gradients (layer 0 sample): [-0.29774925 -0.8617417   1.0242136   4.672068    0.05761039]\n","Worker 1 processing data batch 722\n","Worker 1 gradients (layer 0 sample): [-0.8684498  -2.2500923   1.9254693  10.084676   -0.18237473]\n","Aggregated gradients (layer 0 sample): [-0.58309954 -1.555917    1.4748414   7.378372   -0.06238217]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817756 0.15275906 0.05045084 0.0366124 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917755 0.15175907 0.04945084 0.03761235]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 723\n","Worker 0 gradients (layer 0 sample): [ 1.1794801   2.1421604  -1.2570832  -8.615724    0.27383935]\n","Worker 1 processing data batch 723\n","Worker 1 gradients (layer 0 sample): [ 0.45640647  1.0478649  -0.9311244  -3.957672    0.05026925]\n","Aggregated gradients (layer 0 sample): [ 0.8179433  1.5950127 -1.0941038 -6.286698   0.1620543]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917755 0.15175907 0.04945084 0.03761235]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817756 0.15275906 0.05045084 0.03661237]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 724\n","Worker 0 gradients (layer 0 sample): [-1.045449   -1.2591461   1.7186484  10.804644    0.33032307]\n","Worker 1 processing data batch 724\n","Worker 1 gradients (layer 0 sample): [-1.0669063  -0.66388935  1.9129316  11.145081    1.1695615 ]\n","Aggregated gradients (layer 0 sample): [-1.0561776 -0.9615177  1.8157899 10.974862   0.7499423]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817756 0.15275906 0.05045084 0.03661237]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917755 0.15175907 0.04945084 0.03561238]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 725\n","Worker 0 gradients (layer 0 sample): [ 0.6680685 -1.1415386 -0.2528409 -2.7678504 -1.051357 ]\n","Worker 1 processing data batch 725\n","Worker 1 gradients (layer 0 sample): [ 1.1002622  1.6614231 -1.3365347 -8.00274   -0.1949852]\n","Aggregated gradients (layer 0 sample): [ 0.88416535  0.25994223 -0.7946878  -5.385295   -0.6231711 ]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917755 0.15175907 0.04945084 0.03561238]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661237]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 726\n","Worker 0 gradients (layer 0 sample): [-0.7109721  -0.41040683  0.9917712   5.677613    0.507013  ]\n","Worker 1 processing data batch 726\n","Worker 1 gradients (layer 0 sample): [-1.1404648 -2.0076776  2.3745074 12.595591  -0.2852763]\n","Aggregated gradients (layer 0 sample): [-0.9257184  -1.2090422   1.6831393   9.136601    0.11086836]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661237]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.0356124 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 727\n","Worker 0 gradients (layer 0 sample): [ 0.7565446   0.5365802  -0.8357787  -5.159333   -0.41746318]\n","Worker 1 processing data batch 727\n","Worker 1 gradients (layer 0 sample): [ 0.89872235  0.73510766 -1.6356502  -7.605656   -0.76149696]\n","Aggregated gradients (layer 0 sample): [ 0.8276335   0.63584393 -1.2357144  -6.382495   -0.58948004]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.0356124 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661239]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 728\n","Worker 0 gradients (layer 0 sample): [-0.79843664 -0.5762434   1.4078183   8.305291    0.12594068]\n","Worker 1 processing data batch 728\n","Worker 1 gradients (layer 0 sample): [-1.0766529 -1.0480995  2.415254  12.401842   0.7770026]\n","Aggregated gradients (layer 0 sample): [-0.93754476 -0.81217146  1.9115362  10.353567    0.45147163]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661239]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.0356124 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 729\n","Worker 0 gradients (layer 0 sample): [ 1.0771753   0.0998853  -0.68504953 -5.905842   -0.49708146]\n","Worker 1 processing data batch 729\n","Worker 1 gradients (layer 0 sample): [ 0.29470682  0.92822105 -0.9180473  -2.89042     0.03336922]\n","Aggregated gradients (layer 0 sample): [ 0.68594104  0.51405317 -0.8015484  -4.398131   -0.23185612]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.0356124 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661238]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 730\n","Worker 0 gradients (layer 0 sample): [-0.37740773 -1.6828715   1.6663557   6.28628    -0.9707359 ]\n","Worker 1 processing data batch 730\n","Worker 1 gradients (layer 0 sample): [-1.1491771  -0.14891563  1.2059338   9.44441     1.4055396 ]\n","Aggregated gradients (layer 0 sample): [-0.76329243 -0.91589355  1.4361448   7.865345    0.21740186]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661238]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.0356124 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 730, Losses: [2.0614696, 1.9547503]\n","Worker 0 processing data batch 731\n","Worker 0 gradients (layer 0 sample): [ 0.6734376   0.06830615 -0.48133278 -5.037165   -0.70692647]\n","Worker 1 processing data batch 731\n","Worker 1 gradients (layer 0 sample): [ 0.85631186  1.0319492  -1.1441166  -6.0283375  -0.41283312]\n","Aggregated gradients (layer 0 sample): [ 0.7648747  0.5501276 -0.8127247 -5.532751  -0.5598798]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.0356124 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817758 0.15275906 0.05045083 0.03661239]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 732\n","Worker 0 gradients (layer 0 sample): [-0.7738162  -0.98139423  1.3797206   8.1601715   0.07611667]\n","Worker 1 processing data batch 732\n","Worker 1 gradients (layer 0 sample): [-0.37951636 -1.0745788   0.70217294  4.973072    0.22276731]\n","Aggregated gradients (layer 0 sample): [-0.5766663  -1.0279865   1.0409467   6.566622    0.14944199]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817758 0.15275906 0.05045083 0.03661239]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917757 0.15175907 0.04945084 0.03561242]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 733\n","Worker 0 gradients (layer 0 sample): [ 0.5292741   2.2431283  -0.7008639  -5.554607    0.39056993]\n","Worker 1 processing data batch 733\n","Worker 1 gradients (layer 0 sample): [ 0.6892264  1.9251055 -1.0294597 -5.9454136  0.2938879]\n","Aggregated gradients (layer 0 sample): [ 0.60925025  2.084117   -0.8651618  -5.7500105   0.34222892]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917757 0.15175907 0.04945084 0.03561242]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045083 0.03461244]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 734\n","Worker 0 gradients (layer 0 sample): [-0.5675561  -1.636165    1.6096567   6.8103094  -0.09298261]\n","Worker 1 processing data batch 734\n","Worker 1 gradients (layer 0 sample): [-0.47977468 -0.8383441   1.4129618   6.5465426  -0.11600035]\n","Aggregated gradients (layer 0 sample): [-0.52366537 -1.2372546   1.5113093   6.678426   -0.10449148]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045083 0.03461244]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.0356124 ]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 735\n","Worker 0 gradients (layer 0 sample): [ 1.0645776  -0.2792171  -0.59473467 -5.9056287  -0.8952746 ]\n","Worker 1 processing data batch 735\n","Worker 1 gradients (layer 0 sample): [ 0.98513377  2.481935   -1.4782882  -7.91123     0.40296948]\n","Aggregated gradients (layer 0 sample): [ 1.0248556   1.101359   -1.0365114  -6.908429   -0.24615255]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.0356124 ]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045083 0.03661238]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 736\n","Worker 0 gradients (layer 0 sample): [-0.72550493 -2.17494     1.5642452   8.523909   -0.5163848 ]\n","Worker 1 processing data batch 736\n","Worker 1 gradients (layer 0 sample): [-1.0787174  -0.98878586  1.9062219   9.19698     0.09045301]\n","Aggregated gradients (layer 0 sample): [-0.9021112  -1.5818629   1.7352335   8.860445   -0.21296588]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045083 0.03661238]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917757 0.15175907 0.04945084 0.03761236]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 737\n","Worker 0 gradients (layer 0 sample): [ 0.55567443  2.677459   -1.7610852  -6.791893    3.4082806 ]\n","Worker 1 processing data batch 737\n","Worker 1 gradients (layer 0 sample): [ 0.7689081  2.0159392 -1.4354746 -7.505354   1.124474 ]\n","Aggregated gradients (layer 0 sample): [ 0.6622913  2.3466992 -1.59828   -7.1486235  2.2663774]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917757 0.15175907 0.04945084 0.03761236]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045083 0.03661237]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 738\n","Worker 0 gradients (layer 0 sample): [-0.3256108  -0.79509014  1.103182    4.2926106  -0.43745667]\n","Worker 1 processing data batch 738\n","Worker 1 gradients (layer 0 sample): [-0.45254034 -1.7198274   1.2893379   4.8469157  -0.9709616 ]\n","Aggregated gradients (layer 0 sample): [-0.38907558 -1.2574588   1.19626     4.569763   -0.7042091 ]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045083 0.03661237]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917757 0.15175907 0.04945084 0.03761236]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 739\n","Worker 0 gradients (layer 0 sample): [ 1.0841063  2.8191137 -1.5465374 -6.976892   2.338131 ]\n","Worker 1 processing data batch 739\n","Worker 1 gradients (layer 0 sample): [ 0.5618973  2.0707018 -1.1432633 -5.7734528  1.364577 ]\n","Aggregated gradients (layer 0 sample): [ 0.8230018  2.4449077 -1.3449004 -6.3751726  1.851354 ]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917757 0.15175907 0.04945084 0.03761236]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661237]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 740\n","Worker 0 gradients (layer 0 sample): [-0.04247082 -1.8560085   1.5595183   4.3913045  -0.9451123 ]\n","Worker 1 processing data batch 740\n","Worker 1 gradients (layer 0 sample): [-0.602187   -1.1940929   0.8364479   5.611497   -0.33200455]\n","Aggregated gradients (layer 0 sample): [-0.3223289 -1.5250506  1.1979831  5.001401  -0.6385584]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661237]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917757 0.15175907 0.04945084 0.03761235]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 740, Losses: [1.9306679, 1.6996319]\n","Worker 0 processing data batch 741\n","Worker 0 gradients (layer 0 sample): [ 0.48707387  2.5062299  -1.6616039  -6.381317    2.003124  ]\n","Worker 1 processing data batch 741\n","Worker 1 gradients (layer 0 sample): [ 0.46208805  0.75986266 -0.69155383 -2.5614762   0.5316421 ]\n","Aggregated gradients (layer 0 sample): [ 0.47458094  1.6330463  -1.1765789  -4.4713964   1.2673831 ]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917757 0.15175907 0.04945084 0.03761235]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661236]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 742\n","Worker 0 gradients (layer 0 sample): [-0.6509923  -1.336073    1.1387645   6.3693037   0.18505102]\n","Worker 1 processing data batch 742\n","Worker 1 gradients (layer 0 sample): [-0.52682245 -1.1470283   0.8652984   5.960814   -0.47886148]\n","Aggregated gradients (layer 0 sample): [-0.58890736 -1.2415507   1.0020314   6.165059   -0.14690523]\n","Parameters before update (layer 0 sample): [0.14492394 0.00817757 0.15275906 0.05045084 0.03661236]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.03761233]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 743\n","Worker 0 gradients (layer 0 sample): [ 0.4246713  1.7620856 -0.9706071 -3.7694373  1.5702384]\n","Worker 1 processing data batch 743\n","Worker 1 gradients (layer 0 sample): [-0.25865716  0.7983604  -0.5520462   0.10180211  1.4296789 ]\n","Aggregated gradients (layer 0 sample): [ 0.08300707  1.280223   -0.7613267  -1.8338176   1.4999586 ]\n","Parameters before update (layer 0 sample): [0.14592393 0.00917756 0.15175907 0.04945084 0.03761233]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03661234]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 744\n","Worker 0 gradients (layer 0 sample): [-0.49254793 -2.3339427   1.7134705   8.010245   -0.9242346 ]\n","Worker 1 processing data batch 744\n","Worker 1 gradients (layer 0 sample): [-0.02501111 -1.9030688   1.3137643   4.0158873  -1.2566527 ]\n","Aggregated gradients (layer 0 sample): [-0.25877953 -2.1185057   1.5136174   6.0130663  -1.0904436 ]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03661234]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945084 0.03761233]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 745\n","Worker 0 gradients (layer 0 sample): [ 1.0450274  2.523766  -1.7845154 -8.863334   2.3291993]\n","Worker 1 processing data batch 745\n","Worker 1 gradients (layer 0 sample): [ 0.76796615 -0.41331404 -0.68415093 -5.235999   -1.2855852 ]\n","Aggregated gradients (layer 0 sample): [ 0.90649676  1.055226   -1.2343332  -7.0496664   0.5218071 ]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945084 0.03761233]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817758 0.15275906 0.05045083 0.03661235]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 746\n","Worker 0 gradients (layer 0 sample): [-0.73550004  0.99469393  0.6683121   5.6554394   1.9565973 ]\n","Worker 1 processing data batch 746\n","Worker 1 gradients (layer 0 sample): [-0.9264846 -1.8056546  1.8215718  9.921371  -0.2686575]\n","Aggregated gradients (layer 0 sample): [-0.83099234 -0.40548036  1.244942    7.7884054   0.84396994]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817758 0.15275906 0.05045083 0.03661235]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945084 0.03561236]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 747\n","Worker 0 gradients (layer 0 sample): [ 0.8368044   0.70474607 -0.83934844 -5.7431536  -0.5292588 ]\n","Worker 1 processing data batch 747\n","Worker 1 gradients (layer 0 sample): [ 0.772602   -0.10728306 -0.4536682  -3.5901337  -0.681911  ]\n","Aggregated gradients (layer 0 sample): [ 0.80470324  0.2987315  -0.64650834 -4.6666436  -0.60558486]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945084 0.03561236]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817758 0.15275906 0.05045083 0.03661234]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 748\n","Worker 0 gradients (layer 0 sample): [-0.3862734  -0.806876    0.71924347  4.4499397  -0.372886  ]\n","Worker 1 processing data batch 748\n","Worker 1 gradients (layer 0 sample): [-0.5245838 -0.4146276  0.5031067  3.8151276  0.0092439]\n","Aggregated gradients (layer 0 sample): [-0.4554286  -0.6107518   0.61117506  4.1325336  -0.18182105]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817758 0.15275906 0.05045083 0.03661234]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945084 0.03761232]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 749\n","Worker 0 gradients (layer 0 sample): [ 1.4512117   1.641629   -1.0343764  -8.6358795   0.21317211]\n","Worker 1 processing data batch 749\n","Worker 1 gradients (layer 0 sample): [  1.1675932   3.4074488  -1.7139659 -10.605228    1.811612 ]\n","Aggregated gradients (layer 0 sample): [ 1.3094025  2.524539  -1.3741711 -9.620554   1.012392 ]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945084 0.03761232]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03661233]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 750\n","Worker 0 gradients (layer 0 sample): [-1.3643084 -1.637898   2.300262  13.608442   0.611612 ]\n","Worker 1 processing data batch 750\n","Worker 1 gradients (layer 0 sample): [-0.6112221  -0.3812601   0.71804196  5.1399765   0.42654586]\n","Aggregated gradients (layer 0 sample): [-0.9877652  -1.0095791   1.5091519   9.374209    0.51907897]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03661233]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945084 0.03561234]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 750, Losses: [1.9461703, 1.7745492]\n","Worker 0 processing data batch 751\n","Worker 0 gradients (layer 0 sample): [ 0.52322704  1.7168424  -0.62411326 -4.505032    0.12420727]\n","Worker 1 processing data batch 751\n","Worker 1 gradients (layer 0 sample): [ 0.566008    0.5285293  -0.41011778 -3.8107684  -0.3997493 ]\n","Aggregated gradients (layer 0 sample): [ 0.54461753  1.1226859  -0.51711553 -4.1579003  -0.13777103]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945084 0.03561234]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03661231]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 752\n","Worker 0 gradients (layer 0 sample): [-0.7642088  -0.60709584  0.84236556  7.683736    0.18297979]\n","Worker 1 processing data batch 752\n","Worker 1 gradients (layer 0 sample): [-0.86013114 -2.175066    1.6191683   9.489776   -0.23189121]\n","Aggregated gradients (layer 0 sample): [-0.81216997 -1.3910809   1.2307669   8.586756   -0.02445571]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03661231]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03761217]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 753\n","Worker 0 gradients (layer 0 sample): [ 0.9123961   2.008278   -0.8046856  -6.106229    0.19618365]\n","Worker 1 processing data batch 753\n","Worker 1 gradients (layer 0 sample): [ 0.26852402  1.6673901  -0.5902794  -3.4286795   0.6531952 ]\n","Aggregated gradients (layer 0 sample): [ 0.59046006  1.837834   -0.69748247 -4.767454    0.4246894 ]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03761217]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661219]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 754\n","Worker 0 gradients (layer 0 sample): [-0.3535301 -1.0639119  1.0391287  4.65751   -0.2720009]\n","Worker 1 processing data batch 754\n","Worker 1 gradients (layer 0 sample): [ 0.10377026 -1.9784234   1.1017265   3.3970797  -1.4800955 ]\n","Aggregated gradients (layer 0 sample): [-0.12487993 -1.5211676   1.0704277   4.0272946  -0.8760482 ]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661219]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917756 0.15175907 0.04945083 0.03761218]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 755\n","Worker 0 gradients (layer 0 sample): [ 0.8276114  -0.04594597 -0.70114243 -5.1914954  -1.0495797 ]\n","Worker 1 processing data batch 755\n","Worker 1 gradients (layer 0 sample): [ 0.7382985  2.242805  -1.199562  -6.2871842  1.0995135]\n","Aggregated gradients (layer 0 sample): [ 0.78295493  1.0984296  -0.9503522  -5.73934     0.0249669 ]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917756 0.15175907 0.04945083 0.03761218]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492396 0.00817757 0.15275906 0.05045083 0.03661231]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 756\n","Worker 0 gradients (layer 0 sample): [-0.85723066 -1.048641    1.2718189   8.281127    0.52839124]\n","Worker 1 processing data batch 756\n","Worker 1 gradients (layer 0 sample): [-0.7982557  -1.0495712   1.6138351   8.546621   -0.32388172]\n","Aggregated gradients (layer 0 sample): [-0.8277432  -1.0491061   1.442827    8.413874    0.10225476]\n","Parameters before update (layer 0 sample): [0.14492396 0.00817757 0.15275906 0.05045083 0.03661231]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592394 0.00917756 0.15175907 0.04945083 0.03561235]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 757\n","Worker 0 gradients (layer 0 sample): [ 0.17797531  1.034557   -0.39142916 -0.89511514 -0.11778767]\n","Worker 1 processing data batch 757\n","Worker 1 gradients (layer 0 sample): [ 0.22284287  2.4598691  -0.8764101  -3.7544596   1.0365404 ]\n","Aggregated gradients (layer 0 sample): [ 0.20040908  1.7472131  -0.63391966 -2.3247874   0.45937636]\n","Parameters before update (layer 0 sample): [0.14592394 0.00917756 0.15175907 0.04945083 0.03561235]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03461236]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 758\n","Worker 0 gradients (layer 0 sample): [-0.23427987 -2.1837692   1.7830406   6.1673565  -0.6919277 ]\n","Worker 1 processing data batch 758\n","Worker 1 gradients (layer 0 sample): [-0.95448375 -1.7024584   1.5058354   9.08626     0.20629486]\n","Aggregated gradients (layer 0 sample): [-0.5943818 -1.9431138  1.644438   7.626808  -0.2428164]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03461236]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03561234]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 759\n","Worker 0 gradients (layer 0 sample): [ 0.38261354  1.3777868  -0.79746586 -3.4385974  -0.10342583]\n","Worker 1 processing data batch 759\n","Worker 1 gradients (layer 0 sample): [ 0.9243252   0.6981008  -0.44805998 -4.905507   -0.02181857]\n","Aggregated gradients (layer 0 sample): [ 0.6534694  1.0379438 -0.6227629 -4.1720524 -0.0626222]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03561234]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661228]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 760\n","Worker 0 gradients (layer 0 sample): [-0.4045737  -1.4565942   0.43633133  3.2990646  -0.15598479]\n","Worker 1 processing data batch 760\n","Worker 1 gradients (layer 0 sample): [-0.91608924 -1.000788    1.3906431   8.895958   -0.01709116]\n","Aggregated gradients (layer 0 sample): [-0.6603315  -1.2286911   0.9134872   6.0975113  -0.08653797]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661228]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03761224]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 760, Losses: [1.6348284, 2.1969347]\n","Worker 0 processing data batch 761\n","Worker 0 gradients (layer 0 sample): [ 1.098499    1.7770486  -1.1961203  -9.21851     0.14358024]\n","Worker 1 processing data batch 761\n","Worker 1 gradients (layer 0 sample): [-0.15271923  0.01781806  0.1650874   1.1193537   0.3723508 ]\n","Aggregated gradients (layer 0 sample): [ 0.47288984  0.89743334 -0.5155164  -4.049578    0.25796553]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03761224]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661226]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 762\n","Worker 0 gradients (layer 0 sample): [-0.46368906 -1.9350002   1.1333684   5.9375668  -1.120326  ]\n","Worker 1 processing data batch 762\n","Worker 1 gradients (layer 0 sample): [-0.68255574 -0.9378276   0.813626    5.088214    0.27167347]\n","Aggregated gradients (layer 0 sample): [-0.5731224  -1.4364139   0.97349715  5.5128903  -0.4243263 ]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661226]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03761225]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 763\n","Worker 0 gradients (layer 0 sample): [ 0.5300843  1.8733407 -1.3208    -5.1192713  1.260744 ]\n","Worker 1 processing data batch 763\n","Worker 1 gradients (layer 0 sample): [ 0.62422276  2.8121204  -1.0536416  -5.768629    2.0820322 ]\n","Aggregated gradients (layer 0 sample): [ 0.57715356  2.3427305  -1.1872208  -5.44395     1.6713881 ]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03761225]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661226]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 764\n","Worker 0 gradients (layer 0 sample): [-8.1937504e-01 -1.5814176e+00  1.5114114e+00  8.9713154e+00\n"," -3.5686642e-03]\n","Worker 1 processing data batch 764\n","Worker 1 gradients (layer 0 sample): [-0.4905694  -0.29858124  0.7946895   4.468283   -0.18674737]\n","Aggregated gradients (layer 0 sample): [-0.6549722  -0.9399994   1.1530504   6.719799   -0.09515802]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661226]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03761222]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 765\n","Worker 0 gradients (layer 0 sample): [ 1.0692145  3.4034796 -2.182486  -9.055601   2.0797288]\n","Worker 1 processing data batch 765\n","Worker 1 gradients (layer 0 sample): [ 0.8231365   1.4273212  -1.0839572  -7.005874    0.34058136]\n","Aggregated gradients (layer 0 sample): [ 0.94617546  2.4154005  -1.6332216  -8.030738    1.2101551 ]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03761222]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661223]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 766\n","Worker 0 gradients (layer 0 sample): [-0.14284909 -0.6682664   0.6578059   2.305891   -0.5751832 ]\n","Worker 1 processing data batch 766\n","Worker 1 gradients (layer 0 sample): [-0.7699659  -0.03146237  1.089365    6.786512    1.0332525 ]\n","Aggregated gradients (layer 0 sample): [-0.4564075  -0.3498644   0.87358546  4.5462017   0.22903463]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661223]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03561225]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 767\n","Worker 0 gradients (layer 0 sample): [ 1.0124861   0.82509273 -0.41940475 -6.3344927  -0.4581256 ]\n","Worker 1 processing data batch 767\n","Worker 1 gradients (layer 0 sample): [ 0.8588665   1.2406454  -0.6697295  -5.8570457  -0.15195432]\n","Aggregated gradients (layer 0 sample): [ 0.93567634  1.0328691  -0.5445671  -6.095769   -0.30503994]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03561225]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661223]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 768\n","Worker 0 gradients (layer 0 sample): [-0.7301646  -0.9939938   0.9831927   5.592908    0.07029504]\n","Worker 1 processing data batch 768\n","Worker 1 gradients (layer 0 sample): [-0.81126726 -0.84538436  1.1778364   8.57954     0.6161205 ]\n","Aggregated gradients (layer 0 sample): [-0.77071595 -0.91968906  1.0805146   7.086224    0.34320778]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661223]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03561225]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 769\n","Worker 0 gradients (layer 0 sample): [ 0.8736718  -0.01617202 -0.53923833 -5.192427   -0.632329  ]\n","Worker 1 processing data batch 769\n","Worker 1 gradients (layer 0 sample): [ 1.9472458   0.51733464 -0.61898357 -9.368823   -1.38017   ]\n","Aggregated gradients (layer 0 sample): [ 1.4104588   0.25058132 -0.579111   -7.2806253  -1.0062494 ]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945083 0.03561225]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661224]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 770\n","Worker 0 gradients (layer 0 sample): [-0.72582436  0.28920388  0.5689899   5.9912176   0.99930406]\n","Worker 1 processing data batch 770\n","Worker 1 gradients (layer 0 sample): [-0.62468284 -0.04279804  0.09773573  5.19455     1.1937159 ]\n","Aggregated gradients (layer 0 sample): [-0.6752536   0.12320292  0.33336282  5.592884    1.0965099 ]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661224]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00717761 0.15175907 0.04945083 0.03561225]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 770, Losses: [1.8680601, 1.8753165]\n","Worker 0 processing data batch 771\n","Worker 0 gradients (layer 0 sample): [ 1.8571922  -0.6865837  -0.67091775 -8.409496   -2.0848837 ]\n","Worker 1 processing data batch 771\n","Worker 1 gradients (layer 0 sample): [ 0.76319575  0.44583958 -0.3572861  -4.415325   -0.1954503 ]\n","Aggregated gradients (layer 0 sample): [ 1.310194   -0.12037206 -0.5141019  -6.4124107  -1.140167  ]\n","Parameters before update (layer 0 sample): [0.14592396 0.00717761 0.15175907 0.04945083 0.03561225]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661224]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 772\n","Worker 0 gradients (layer 0 sample): [-1.0425621  -1.1267843   1.4767072   8.681057    0.40052137]\n","Worker 1 processing data batch 772\n","Worker 1 gradients (layer 0 sample): [-1.267422   -1.5274205   1.5173836  10.940521    0.28647095]\n","Aggregated gradients (layer 0 sample): [-1.1549921  -1.3271024   1.4970454   9.810789    0.34349614]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03661224]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945083 0.03561226]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 773\n","Worker 0 gradients (layer 0 sample): [ 0.88241136  2.555031   -1.4876707  -6.958762    0.4859522 ]\n","Worker 1 processing data batch 773\n","Worker 1 gradients (layer 0 sample): [ 8.5699594e-01  9.8767340e-01 -9.6505207e-01 -6.4883881e+00\n"," -6.0085058e-03]\n","Aggregated gradients (layer 0 sample): [ 0.86970365  1.7713523  -1.2263614  -6.723575    0.23997185]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945083 0.03561226]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03461228]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 774\n","Worker 0 gradients (layer 0 sample): [-0.5663283 -2.9198132  1.9223509  8.529924  -0.8950267]\n","Worker 1 processing data batch 774\n","Worker 1 gradients (layer 0 sample): [-0.01495504 -1.6016927   1.0991328   1.8486505  -0.8984712 ]\n","Aggregated gradients (layer 0 sample): [-0.29064167 -2.260753    1.5107418   5.189287   -0.8967489 ]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045082 0.03461228]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945083 0.03561227]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 775\n","Worker 0 gradients (layer 0 sample): [ 0.7581681   2.7471383  -0.94057083 -6.518978    0.890301  ]\n","Worker 1 processing data batch 775\n","Worker 1 gradients (layer 0 sample): [ 1.1608452   1.2606354  -0.7450179  -8.995705   -0.49758795]\n","Aggregated gradients (layer 0 sample): [ 0.95950663  2.0038867  -0.84279436 -7.7573414   0.19635652]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945083 0.03561227]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03461229]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 776\n","Worker 0 gradients (layer 0 sample): [ 0.03564443 -1.7660451   0.9397808   2.6461082  -0.93584293]\n","Worker 1 processing data batch 776\n","Worker 1 gradients (layer 0 sample): [-0.78512394 -1.5400305   1.1097747   6.872068   -0.3207165 ]\n","Aggregated gradients (layer 0 sample): [-0.37473977 -1.6530378   1.0247778   4.759088   -0.6282797 ]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03461229]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945084 0.03561228]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 777\n","Worker 0 gradients (layer 0 sample): [ 0.96435654  3.252421   -1.405665   -8.269496    0.6524534 ]\n","Worker 1 processing data batch 777\n","Worker 1 gradients (layer 0 sample): [ 0.59649855  4.419981   -1.5659285  -6.7061787   1.5933759 ]\n","Aggregated gradients (layer 0 sample): [ 0.7804276  3.836201  -1.4857967 -7.4878373  1.1229147]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917757 0.15175907 0.04945084 0.03561228]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03461229]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 778\n","Worker 0 gradients (layer 0 sample): [-0.47530836 -1.2863588   1.4270446   6.2890224  -0.2495124 ]\n","Worker 1 processing data batch 778\n","Worker 1 gradients (layer 0 sample): [-0.11012305 -0.90982723  0.8280163   3.4930673  -0.35463342]\n","Aggregated gradients (layer 0 sample): [-0.2927157 -1.098093   1.1275305  4.8910446 -0.3020729]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045083 0.03461229]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945084 0.03561227]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 779\n","Worker 0 gradients (layer 0 sample): [ 1.6324241   1.2121706  -1.5164802  -9.927269   -0.49882993]\n","Worker 1 processing data batch 779\n","Worker 1 gradients (layer 0 sample): [ 0.8975009   1.1496642  -1.1336899  -6.3105717  -0.35377195]\n","Aggregated gradients (layer 0 sample): [ 1.2649624   1.1809174  -1.325085   -8.11892    -0.42630094]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945084 0.03561227]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045084 0.03661226]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Worker 0 processing data batch 780\n","Worker 0 gradients (layer 0 sample): [-1.3025894  -0.40208334  0.9165271   9.630682    1.640738  ]\n","Worker 1 processing data batch 780\n","Worker 1 gradients (layer 0 sample): [-0.88792115 -0.5494516   1.1179463   7.4571066   0.98632514]\n","Aggregated gradients (layer 0 sample): [-1.0952553  -0.47576746  1.0172367   8.543894    1.3135316 ]\n","Parameters before update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045084 0.03661226]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945084 0.03561227]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","Step 780, Losses: [1.9170194, 1.8038998]\n","Worker 0 processing data batch 781\n","Worker 0 gradients (layer 0 sample): [ 1.2206327   5.4805565  -0.79820204 -8.489744    0.99691737]\n","Worker 1 processing data batch 781\n","Worker 1 gradients (layer 0 sample): [ 0.1929431   1.7243853  -1.3251007  -5.251397    0.21559657]\n","Aggregated gradients (layer 0 sample): [ 0.7067879   3.6024709  -1.0616513  -6.8705707   0.60625696]\n","Parameters before update (layer 0 sample): [0.14592396 0.00917756 0.15175907 0.04945084 0.03561227]\n","Updated parameters on the Parameter Server.\n","Parameters after update (layer 0 sample): [0.14492397 0.00817757 0.15275906 0.05045084 0.03461228]\n","Worker 0 synchronized with updated parameters.\n","Worker 1 synchronized with updated parameters.\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.3052 - loss: 1.9244\n","Test Loss: 1.9260179996490479\n","Test Accuracy: 0.299699991941452\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","# Use MirroredStrategy for parallel training simulation\n","strategy = tf.distribute.MirroredStrategy()\n","\n","# Define the LeNet model\n","def create_model():\n","    return models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),  # Input layer\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Flatten(),                                        # Flatten\n","        layers.Dense(120, activation='relu'),                   # Dense layer\n","        layers.Dense(84, activation='relu'),                    # Dense layer\n","        layers.Dense(10, activation='softmax')                  # Output layer\n","    ])\n","\n","# Dataset preparation\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Create datasets\n","train_dataset, test_dataset = create_dataset()\n","\n","# Parallel training setup\n","with strategy.scope():\n","    model = create_model()\n","    model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(train_dataset, epochs=1)\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_acc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBAk0DoFS1PW","executionInfo":{"status":"ok","timestamp":1735440467723,"user_tz":-330,"elapsed":79414,"user":{"displayName":"Chandra Sekhar","userId":"00330720378866032186"}},"outputId":"c61453c6-5621-46bf-ebdd-e91c025211e4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 36ms/step - accuracy: 0.2773 - loss: 1.9937\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.4021 - loss: 1.6785\n","Test Loss: 1.688963532447815\n","Test Accuracy: 0.39489999413490295\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import os\n","\n","# Set up the TF_CONFIG for simulating three workers\n","os.environ[\"TF_CONFIG\"] = \"\"\"\n","{\n","    \"cluster\": {\n","        \"worker\": [\"localhost:12345\", \"localhost:12346\", \"localhost:12347\"]\n","    },\n","    \"task\": {\"type\": \"worker\", \"index\": 0}\n","}\n","\"\"\"\n","\n","# Initialize MultiWorkerMirroredStrategy\n","strategy = tf.distribute.MultiWorkerMirroredStrategy()\n","\n","# Log the number of replicas (workers)\n","num_workers = strategy.num_replicas_in_sync\n","print(f\"Number of workers (replicas): {num_workers}\")\n","\n","# Define the LeNet model\n","def create_model():\n","    return models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),  # Input layer\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Flatten(),                                        # Flatten\n","        layers.Dense(120, activation='relu'),                   # Dense layer\n","        layers.Dense(84, activation='relu'),                    # Dense layer\n","        layers.Dense(10, activation='softmax')                  # Output layer\n","    ])\n","\n","# Dataset preparation\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Create datasets\n","train_dataset, test_dataset = create_dataset()\n","\n","# Parallel training setup\n","with strategy.scope():\n","    model = create_model()\n","    model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","# Train the model\n","print(\"\\nStarting training...\")\n","model.fit(train_dataset, epochs=1)\n","\n","# Evaluate the model\n","print(\"\\nEvaluating the model...\")\n","test_loss, test_acc = model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_acc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"RqZp4FyNYcO4","executionInfo":{"status":"error","timestamp":1735442770016,"user_tz":-330,"elapsed":1135,"user":{"displayName":"Chandra Sekhar","userId":"00330720378866032186"}},"outputId":"e34f8a82-8c82-44ae-98a6-eb87f8a2ed31"},"execution_count":11,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Collective ops must be configured at program startup","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-e8c1f48b1882>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Initialize MultiWorkerMirroredStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiWorkerMirroredStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Log the number of replicas (workers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cluster_resolver, communication_options)\u001b[0m\n\u001b[1;32m    184\u001b[0m       \u001b[0mcommunication_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     super(CollectiveAllReduceStrategy, self).__init__(\n\u001b[0;32m--> 186\u001b[0;31m         CollectiveAllReduceExtended(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mcluster_resolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, container_strategy, cluster_resolver, communication_options, devices)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommunication_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collective_key_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collective_key_base\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster_resolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfer_fn_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeakKeyDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_enable_get_next_as_optional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m_initialize_strategy\u001b[0;34m(self, cluster_resolver, devices)\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_multi_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_resolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m_initialize_multi_worker\u001b[0;34m(self, cluster_resolver)\u001b[0m\n\u001b[1;32m    477\u001b[0m     if (ops.executing_eagerly_outside_functions() and\n\u001b[1;32m    478\u001b[0m         not getattr(self, \"_local_or_standalone_client_mode\", False)):\n\u001b[0;32m--> 479\u001b[0;31m       context.context().configure_collective_ops(\n\u001b[0m\u001b[1;32m    480\u001b[0m           collective_leader=multi_worker_util.collective_leader(\n\u001b[1;32m    481\u001b[0m               cluster_spec, task_type, task_id),\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mconfigure_collective_ops\u001b[0;34m(self, collective_leader, scoped_allocator_enabled_ops, use_nccl_communication, device_filters)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Collective ops must be configured at program startup\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collective_leader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollective_leader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Collective ops must be configured at program startup"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import numpy as np\n","import threading\n","\n","# Define the LeNet model\n","def create_model():\n","    return models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),  # Input layer\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Conv layer\n","        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n","        layers.Flatten(),                                        # Flatten\n","        layers.Dense(120, activation='relu'),                   # Dense layer\n","        layers.Dense(84, activation='relu'),                    # Dense layer\n","        layers.Dense(10, activation='softmax')                  # Output layer\n","    ])\n","\n","# Dataset preparation\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Worker class\n","class Worker:\n","    def __init__(self, worker_id, model, dataset, parameter_server):\n","        self.worker_id = worker_id\n","        self.model = model\n","        self.dataset = dataset\n","        self.parameter_server = parameter_server\n","        self.optimizer = tf.keras.optimizers.Adam()\n","\n","    def compute_and_send_gradients(self, features, labels):\n","        with tf.GradientTape() as tape:\n","            predictions = self.model(features, training=True)\n","            loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n","            loss = tf.reduce_mean(loss)\n","        gradients = tape.gradient(loss, self.model.trainable_variables)\n","        self.parameter_server.receive_gradients(self.worker_id, gradients)\n","        return loss\n","\n","    def run(self):\n","        for step, (features, labels) in enumerate(self.dataset):\n","            features = tf.convert_to_tensor(features)\n","            labels = tf.convert_to_tensor(labels)\n","            loss = self.compute_and_send_gradients(features, labels)\n","            print(f\"Worker {self.worker_id} - Step {step}, Loss: {loss.numpy()}\")\n","\n","# Parameter server\n","class ParameterServer:\n","    def __init__(self, model):\n","        self.model = model\n","        self.gradient_accumulator = [tf.zeros_like(var) for var in model.trainable_variables]\n","        self.lock = threading.Lock()\n","\n","    def receive_gradients(self, worker_id, gradients):\n","        with self.lock:\n","            for i, grad in enumerate(gradients):\n","                self.gradient_accumulator[i] += grad\n","            print(f\"Parameter Server - Received gradients from Worker {worker_id}\")\n","\n","    def apply_gradients(self):\n","        optimizer = tf.keras.optimizers.Adam()\n","        with self.lock:\n","            optimizer.apply_gradients(zip(self.gradient_accumulator, self.model.trainable_variables))\n","            self.gradient_accumulator = [tf.zeros_like(var) for var in self.model.trainable_variables]\n","            print(\"Parameter Server - Gradients applied and parameters updated\")\n","\n","# Parallel training setup\n","train_dataset, test_dataset = create_dataset()\n","global_model = create_model()\n","parameter_server = ParameterServer(global_model)\n","\n","# Split dataset for workers\n","worker_datasets = [train_dataset.shard(num_shards=3, index=i) for i in range(3)]\n","workers = [Worker(worker_id=i, model=create_model(), dataset=worker_datasets[i], parameter_server=parameter_server) for i in range(3)]\n","\n","# Run workers in parallel\n","threads = []\n","for worker in workers:\n","    thread = threading.Thread(target=worker.run)\n","    threads.append(thread)\n","    thread.start()\n","\n","# Wait for all workers to finish\n","for thread in threads:\n","    thread.join()\n","\n","# Apply accumulated gradients on the parameter server\n","parameter_server.apply_gradients()\n","\n","# Evaluate the global model\n","global_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","test_loss, test_acc = global_model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_acc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAgPd0YDdAC8","executionInfo":{"status":"ok","timestamp":1735465542883,"user_tz":-330,"elapsed":90136,"user":{"displayName":"Chandra Sekhar","userId":"00330720378866032186"}},"outputId":"e5c1ea6c-8750-42e7-8a71-3bb91e8e743d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 0, Loss: 2.304616928100586\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 0, Loss: 2.307723045349121\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 1, Loss: 2.3021416664123535\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 0, Loss: 2.3018152713775635\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 1, Loss: 2.3013782501220703\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 2, Loss: 2.2952466011047363\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 1, Loss: 2.3033993244171143\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 2, Loss: 2.303593158721924\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 3, Loss: 2.2999205589294434\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 2, Loss: 2.304145336151123\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 3, Loss: 2.303189277648926\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 4, Loss: 2.3035173416137695\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 3, Loss: 2.3027286529541016\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 4, Loss: 2.3036155700683594\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 5, Loss: 2.3065805435180664\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 5, Loss: 2.301689624786377\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 4, Loss: 2.2999167442321777\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 5, Loss: 2.3062095642089844\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 6, Loss: 2.30218243598938\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 6, Loss: 2.299783706665039\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 6, Loss: 2.2997875213623047\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 7, Loss: 2.302774429321289\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 7, Loss: 2.302576780319214\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 8, Loss: 2.3069310188293457\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 7, Loss: 2.302942991256714\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 8, Loss: 2.296124219894409\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 9, Loss: 2.3067593574523926\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 8, Loss: 2.303572416305542\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 9, Loss: 2.3065576553344727\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 10, Loss: 2.3032498359680176\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 9, Loss: 2.300476312637329\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 10, Loss: 2.306117057800293\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 11, Loss: 2.298827886581421\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 10, Loss: 2.2995657920837402\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 11, Loss: 2.3036415576934814\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 12, Loss: 2.2965903282165527\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 11, Loss: 2.3026843070983887\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 12, Loss: 2.3103833198547363\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 13, Loss: 2.300995111465454\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 12, Loss: 2.301987648010254\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 13, Loss: 2.3005127906799316\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 14, Loss: 2.2969164848327637\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 13, Loss: 2.301891803741455\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 14, Loss: 2.297851085662842\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 15, Loss: 2.3070931434631348\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 15, Loss: 2.299598217010498\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 14, Loss: 2.3033127784729004\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 16, Loss: 2.304446220397949\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 16, Loss: 2.292762279510498\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 15, Loss: 2.3046371936798096\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 17, Loss: 2.3099584579467773\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 16, Loss: 2.3010473251342773\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 17, Loss: 2.297208309173584\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 18, Loss: 2.307962656021118\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 17, Loss: 2.3021812438964844\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 18, Loss: 2.296966552734375\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 19, Loss: 2.304373264312744\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 18, Loss: 2.3040614128112793\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 19, Loss: 2.301023244857788\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 20, Loss: 2.298464298248291\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 20, Loss: 2.306253433227539\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 19, Loss: 2.3056132793426514\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 21, Loss: 2.305180549621582\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 21, Loss: 2.3118677139282227\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 20, Loss: 2.304173231124878\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 22, Loss: 2.307424545288086\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 22, Loss: 2.296257734298706\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 21, Loss: 2.29850435256958\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 23, Loss: 2.2972469329833984\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 23, Loss: 2.3063414096832275\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 22, Loss: 2.3042595386505127\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 24, Loss: 2.304065227508545\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 24, Loss: 2.3008837699890137\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 23, Loss: 2.3016202449798584\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 25, Loss: 2.303698778152466\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 25, Loss: 2.3061132431030273\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 24, Loss: 2.301689624786377\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 26, Loss: 2.293961524963379\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 26, Loss: 2.3049864768981934\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 27, Loss: 2.3038992881774902\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 25, Loss: 2.3021132946014404\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 27, Loss: 2.2898662090301514\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 28, Loss: 2.307199478149414\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 26, Loss: 2.304001569747925\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 28, Loss: 2.301356554031372\n","Parameter Server - Received gradients from Worker 0\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 29, Loss: 2.313570976257324\n","Worker 0 - Step 27, Loss: 2.3086488246917725\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 29, Loss: 2.295832633972168\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 30, Loss: 2.304175853729248\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 28, Loss: 2.298372983932495\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 30, Loss: 2.292611837387085\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 31, Loss: 2.300724983215332\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 29, Loss: 2.3054914474487305\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 31, Loss: 2.301809072494507\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 32, Loss: 2.2925734519958496\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 30, Loss: 2.306269645690918\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 32, Loss: 2.296018600463867\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 33, Loss: 2.3098111152648926\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 31, Loss: 2.300320625305176\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 33, Loss: 2.310638427734375\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 34, Loss: 2.306307792663574\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 32, Loss: 2.3072195053100586\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 34, Loss: 2.2989120483398438\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 35, Loss: 2.309129476547241\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 33, Loss: 2.3013601303100586\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 35, Loss: 2.3085954189300537\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 36, Loss: 2.2982611656188965\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 34, Loss: 2.3048765659332275\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 36, Loss: 2.3070244789123535\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 35, Loss: 2.304417133331299\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 37, Loss: 2.3017706871032715\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 37, Loss: 2.301884412765503\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 38, Loss: 2.3069112300872803\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 36, Loss: 2.305142879486084\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 38, Loss: 2.29491925239563\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 37, Loss: 2.3021278381347656\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 39, Loss: 2.3012561798095703\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 39, Loss: 2.3038713932037354\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 38, Loss: 2.3019511699676514\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 40, Loss: 2.298518419265747\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 40, Loss: 2.3026652336120605\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 39, Loss: 2.304931640625\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 41, Loss: 2.301875114440918\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 41, Loss: 2.309669017791748\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 42, Loss: 2.3087100982666016\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 40, Loss: 2.3017945289611816\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 42, Loss: 2.309732437133789\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 41, Loss: 2.3034238815307617\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 43, Loss: 2.304584503173828\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 43, Loss: 2.3001415729522705\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 44, Loss: 2.295588970184326\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 42, Loss: 2.304814100265503\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 44, Loss: 2.3120431900024414\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 43, Loss: 2.3034586906433105\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 45, Loss: 2.301563262939453\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 45, Loss: 2.3090403079986572\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 44, Loss: 2.3023159503936768\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 46, Loss: 2.3101646900177\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 46, Loss: 2.306769371032715\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 45, Loss: 2.3032898902893066\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 47, Loss: 2.30126953125\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 47, Loss: 2.2964744567871094\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 48, Loss: 2.305555582046509\n","Parameter Server - Received gradients from Worker 0\n","Parameter Server - Received gradients from Worker 1Worker 0 - Step 46, Loss: 2.2976534366607666\n","\n","Worker 1 - Step 48, Loss: 2.3018276691436768\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 49, Loss: 2.3010149002075195\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 47, Loss: 2.304248809814453\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 49, Loss: 2.3011527061462402\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 50, Loss: 2.3054213523864746\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 48, Loss: 2.301792621612549\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 50, Loss: 2.3054752349853516\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 51, Loss: 2.303393840789795\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 49, Loss: 2.3025612831115723\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 52, Loss: 2.301807165145874\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 51, Loss: 2.3056628704071045\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 50, Loss: 2.3021833896636963\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 52, Loss: 2.304584503173828\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 53, Loss: 2.302844524383545\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 51, Loss: 2.3018198013305664\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 53, Loss: 2.3048248291015625\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 54, Loss: 2.294883966445923\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 52, Loss: 2.300809860229492\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 54, Loss: 2.2970919609069824\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 55, Loss: 2.2990643978118896\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 53, Loss: 2.3051204681396484\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 55, Loss: 2.2995195388793945\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 56, Loss: 2.299809455871582\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 54, Loss: 2.3009791374206543\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 56, Loss: 2.304220199584961\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 57, Loss: 2.301142692565918\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 55, Loss: 2.3022689819335938\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 57, Loss: 2.3071129322052\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 58, Loss: 2.3025922775268555\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 56, Loss: 2.305063247680664\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 58, Loss: 2.3001818656921387\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 57, Loss: 2.3034005165100098\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 59, Loss: 2.31474232673645\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 59, Loss: 2.307826519012451\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 58, Loss: 2.3018131256103516\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 60, Loss: 2.295468330383301\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 60, Loss: 2.3025832176208496\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 61, Loss: 2.293323278427124\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 59, Loss: 2.3048248291015625\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 61, Loss: 2.3083038330078125\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 62, Loss: 2.299013376235962\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 60, Loss: 2.3021435737609863\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 62, Loss: 2.3036274909973145\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 61, Loss: 2.3060302734375\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 63, Loss: 2.3067526817321777\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 63, Loss: 2.301365375518799\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 64, Loss: 2.302157402038574\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 62, Loss: 2.3037636280059814\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 64, Loss: 2.311110019683838\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 63, Loss: 2.301391124725342\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 65, Loss: 2.3067221641540527\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 65, Loss: 2.29813289642334\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 66, Loss: 2.301710844039917\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 64, Loss: 2.3011727333068848\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 66, Loss: 2.3096470832824707\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 67, Loss: 2.299638271331787\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 65, Loss: 2.297675371170044\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 67, Loss: 2.3085999488830566\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 68, Loss: 2.304757595062256\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 66, Loss: 2.3025803565979004\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 68, Loss: 2.3039045333862305\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 69, Loss: 2.302741050720215\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 67, Loss: 2.305586576461792\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 69, Loss: 2.3043346405029297\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 70, Loss: 2.3042407035827637\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 68, Loss: 2.302398204803467\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 71, Loss: 2.2990658283233643\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 70, Loss: 2.294811964035034\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 69, Loss: 2.3055500984191895\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 72, Loss: 2.308076858520508\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 71, Loss: 2.297334671020508\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 70, Loss: 2.3044662475585938\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 73, Loss: 2.3002991676330566\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 72, Loss: 2.3047773838043213\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 71, Loss: 2.3015074729919434\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 74, Loss: 2.2987513542175293\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 73, Loss: 2.295151710510254\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 72, Loss: 2.298537015914917\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 74, Loss: 2.306013584136963\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 75, Loss: 2.3035898208618164\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 73, Loss: 2.3009390830993652\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 75, Loss: 2.3053934574127197\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 76, Loss: 2.3006081581115723\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 74, Loss: 2.303959846496582\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 76, Loss: 2.3004255294799805\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 75, Loss: 2.305657386779785\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 77, Loss: 2.3030824661254883\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 77, Loss: 2.307318687438965\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 78, Loss: 2.3026785850524902\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 76, Loss: 2.2994394302368164\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 78, Loss: 2.30279541015625\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 79, Loss: 2.3085708618164062\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 79, Loss: 2.3052117824554443\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 77, Loss: 2.3016281127929688\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 80, Loss: 2.30377197265625\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 80, Loss: 2.294673204421997\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 78, Loss: 2.3015060424804688\n","Parameter Server - Received gradients from Worker 1\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 81, Loss: 2.3095827102661133\n","Worker 1 - Step 81, Loss: 2.2988839149475098\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 79, Loss: 2.3030953407287598\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 82, Loss: 2.3033738136291504\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 82, Loss: 2.300697088241577\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 80, Loss: 2.307300567626953\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 83, Loss: 2.2997758388519287\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 83, Loss: 2.2989256381988525\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 81, Loss: 2.3023200035095215\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 84, Loss: 2.303774356842041\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 84, Loss: 2.2947070598602295\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 82, Loss: 2.301560401916504\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 85, Loss: 2.304039239883423\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 85, Loss: 2.297077178955078\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 83, Loss: 2.301630973815918\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 86, Loss: 2.2955236434936523\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 86, Loss: 2.300520658493042\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 84, Loss: 2.300109624862671\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 87, Loss: 2.294905662536621\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 87, Loss: 2.2978103160858154\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 85, Loss: 2.3036630153656006\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 88, Loss: 2.296961545944214\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 88, Loss: 2.298762798309326Parameter Server - Received gradients from Worker 0\n","\n","Worker 0 - Step 86, Loss: 2.301909923553467\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 89, Loss: 2.2985918521881104\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 89, Loss: 2.3080897331237793\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 87, Loss: 2.299236297607422\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 90, Loss: 2.305152416229248\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 88, Loss: 2.3031814098358154\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 90, Loss: 2.3027846813201904\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 91, Loss: 2.300182342529297\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 89, Loss: 2.3004798889160156\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 91, Loss: 2.301797866821289\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 92, Loss: 2.302037239074707\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 90, Loss: 2.3013365268707275\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 92, Loss: 2.294032573699951\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 93, Loss: 2.301828384399414\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 91, Loss: 2.3027477264404297\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 94, Loss: 2.29738712310791\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 93, Loss: 2.3098273277282715\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 92, Loss: 2.304037094116211\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 95, Loss: 2.3032007217407227\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 94, Loss: 2.294424057006836\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 93, Loss: 2.304137706756592\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 96, Loss: 2.2974252700805664\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 94, Loss: 2.3017520904541016\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 95, Loss: 2.305359363555908\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 95, Loss: 2.301570415496826\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 97, Loss: 2.295257091522217\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 96, Loss: 2.3027024269104004\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 96, Loss: 2.3032655715942383\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 98, Loss: 2.2960152626037598\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 97, Loss: 2.306563377380371\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 97, Loss: 2.300189971923828\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 99, Loss: 2.3019094467163086\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 98, Loss: 2.3025102615356445\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 98, Loss: 2.302232265472412\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 100, Loss: 2.306974411010742\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 99, Loss: 2.3014767169952393\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 101, Loss: 2.294556140899658\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 99, Loss: 2.3036978244781494\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 100, Loss: 2.307620048522949\n","Parameter Server - Received gradients from Worker 0\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 102, Loss: 2.3010077476501465\n","Worker 0 - Step 100, Loss: 2.3028523921966553\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 101, Loss: 2.309576988220215\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 103, Loss: 2.3019983768463135\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 101, Loss: 2.301422119140625\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 102, Loss: 2.304117202758789\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 104, Loss: 2.29726505279541\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 103, Loss: 2.308624029159546Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 102, Loss: 2.3055331707000732\n","\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 105, Loss: 2.2914843559265137\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 104, Loss: 2.2997093200683594\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 103, Loss: 2.304488182067871\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 106, Loss: 2.299165725708008\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 104, Loss: 2.303215742111206\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 105, Loss: 2.302659511566162\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 106, Loss: 2.3081789016723633\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 107, Loss: 2.3034963607788086\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 105, Loss: 2.3012094497680664\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 107, Loss: 2.3040690422058105\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 108, Loss: 2.3019800186157227\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 106, Loss: 2.3034274578094482\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 108, Loss: 2.298430919647217\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 107, Loss: 2.3070836067199707\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 109, Loss: 2.303166151046753\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 109, Loss: 2.311115026473999\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 108, Loss: 2.3037352561950684\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 110, Loss: 2.298905372619629\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 110, Loss: 2.2943716049194336\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 109, Loss: 2.3022732734680176\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 111, Loss: 2.301137924194336\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 111, Loss: 2.307534694671631\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 110, Loss: 2.3010683059692383\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 112, Loss: 2.29555606842041\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 112, Loss: 2.3061861991882324\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 111, Loss: 2.3027572631835938\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 113, Loss: 2.2977230548858643\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 113, Loss: 2.303079605102539\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 112, Loss: 2.3027267456054688\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 114, Loss: 2.2987897396087646\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 114, Loss: 2.3038926124572754\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 113, Loss: 2.301835536956787\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 115, Loss: 2.305131673812866\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 115, Loss: 2.307976245880127\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 114, Loss: 2.3039627075195312\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 116, Loss: 2.301827907562256\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 115, Loss: 2.3016819953918457\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 116, Loss: 2.297366142272949\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 117, Loss: 2.3044252395629883\n","Parameter Server - Received gradients from Worker 0\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 117, Loss: 2.294802188873291\n","Worker 0 - Step 116, Loss: 2.3028628826141357\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 118, Loss: 2.302615165710449\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 118, Loss: 2.3020684719085693\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 117, Loss: 2.299544334411621\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 119, Loss: 2.299495220184326\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 119, Loss: 2.300175189971924\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 118, Loss: 2.301496982574463\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 120, Loss: 2.3176281452178955\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 120, Loss: 2.3013675212860107\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 119, Loss: 2.305634021759033\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 121, Loss: 2.298236608505249\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 121, Loss: 2.300234317779541\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 120, Loss: 2.304129123687744\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 122, Loss: 2.308370590209961\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 122, Loss: 2.3062078952789307\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 121, Loss: 2.300786256790161\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 123, Loss: 2.308098554611206\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 123, Loss: 2.308231830596924\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 122, Loss: 2.300389289855957\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 124, Loss: 2.313483953475952\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 124, Loss: 2.293534755706787\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 123, Loss: 2.303182601928711\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 125, Loss: 2.3038954734802246\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 125, Loss: 2.3061654567718506\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 124, Loss: 2.3060128688812256\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 126, Loss: 2.2956244945526123\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 126, Loss: 2.298262119293213\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 125, Loss: 2.302353620529175\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 127, Loss: 2.3028564453125\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 127, Loss: 2.29683256149292\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 126, Loss: 2.303741931915283\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 128, Loss: 2.3004822731018066\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 128, Loss: 2.3073365688323975\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 129, Loss: 2.3157641887664795\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 127, Loss: 2.3026671409606934\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 129, Loss: 2.306236505508423\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 128, Loss: 2.304513931274414\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 130, Loss: 2.3026962280273438\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 130, Loss: 2.2952017784118652\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 129, Loss: 2.304154872894287\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 131, Loss: 2.3052620887756348\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 131, Loss: 2.3047237396240234\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 132, Loss: 2.3096847534179688\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 130, Loss: 2.2975242137908936\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 132, Loss: 2.3045592308044434\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 133, Loss: 2.3040730953216553\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 131, Loss: 2.303727388381958\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 133, Loss: 2.295445680618286\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 134, Loss: 2.294405937194824\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 132, Loss: 2.298694133758545\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 134, Loss: 2.2993569374084473\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 135, Loss: 2.3090267181396484\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 135, Loss: 2.298088788986206\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 133, Loss: 2.3019180297851562\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 136, Loss: 2.300544500350952\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 136, Loss: 2.312455892562866\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 134, Loss: 2.3019673824310303\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 137, Loss: 2.3099162578582764\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 138, Loss: 2.310143232345581\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 137, Loss: 2.305204391479492\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 135, Loss: 2.3028860092163086\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 136, Loss: 2.3008952140808105\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 139, Loss: 2.3069257736206055\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 138, Loss: 2.3036444187164307\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 140, Loss: 2.294879913330078\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 139, Loss: 2.3029048442840576\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 137, Loss: 2.3003430366516113\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 141, Loss: 2.3065481185913086\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 140, Loss: 2.30942440032959\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 138, Loss: 2.305422067642212\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 142, Loss: 2.297149181365967\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 141, Loss: 2.301558017730713\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 139, Loss: 2.304008960723877\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 143, Loss: 2.3027091026306152\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 142, Loss: 2.300689220428467\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 140, Loss: 2.305851459503174\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 144, Loss: 2.308493137359619\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 143, Loss: 2.2976880073547363\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 141, Loss: 2.30446195602417\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 145, Loss: 2.3105826377868652\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 144, Loss: 2.296358108520508\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 146, Loss: 2.2988734245300293\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 142, Loss: 2.300978183746338Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 145, Loss: 2.3023135662078857\n","\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 146, Loss: 2.300485134124756\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 147, Loss: 2.302590847015381\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 143, Loss: 2.302748203277588\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 148, Loss: 2.298977851867676\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 147, Loss: 2.3037495613098145\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 144, Loss: 2.301299571990967\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 149, Loss: 2.308929920196533\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 148, Loss: 2.3135385513305664\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 145, Loss: 2.3003950119018555\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 150, Loss: 2.2974143028259277\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 149, Loss: 2.306271553039551\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 146, Loss: 2.2999393939971924\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 150, Loss: 2.305320978164673\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 151, Loss: 2.3046767711639404\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 147, Loss: 2.3024487495422363\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 151, Loss: 2.304563045501709\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 152, Loss: 2.3003182411193848\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 148, Loss: 2.3012828826904297\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 153, Loss: 2.3047726154327393\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 152, Loss: 2.3004984855651855\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 149, Loss: 2.3003365993499756\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 153, Loss: 2.3028175830841064\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 154, Loss: 2.3005783557891846\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 150, Loss: 2.3026061058044434\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 154, Loss: 2.3054378032684326\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 155, Loss: 2.301060676574707\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 151, Loss: 2.29970383644104\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 156, Loss: 2.3050537109375\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 155, Loss: 2.305837631225586\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 152, Loss: 2.303730010986328\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 156, Loss: 2.2942209243774414\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 157, Loss: 2.3087010383605957\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 153, Loss: 2.3015055656433105\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 158, Loss: 2.302064895629883\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 157, Loss: 2.301234245300293\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 154, Loss: 2.3015971183776855\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 158, Loss: 2.3027708530426025\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 159, Loss: 2.3060693740844727\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 155, Loss: 2.3010387420654297\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 159, Loss: 2.3039751052856445\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 160, Loss: 2.2983641624450684\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 156, Loss: 2.304906129837036\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 160, Loss: 2.298912763595581\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 161, Loss: 2.3087856769561768\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 157, Loss: 2.300750732421875\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 161, Loss: 2.300816774368286\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 162, Loss: 2.2936973571777344\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 158, Loss: 2.301776885986328\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 162, Loss: 2.2981128692626953\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 163, Loss: 2.3090739250183105\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 159, Loss: 2.299168586730957\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 163, Loss: 2.3081891536712646\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 164, Loss: 2.3030567169189453\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 160, Loss: 2.304584503173828\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 164, Loss: 2.2978780269622803\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 165, Loss: 2.298710346221924\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 161, Loss: 2.3012423515319824\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 165, Loss: 2.300173759460449\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 166, Loss: 2.300718069076538\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 162, Loss: 2.2985477447509766\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 166, Loss: 2.309978723526001\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 167, Loss: 2.3015971183776855\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 163, Loss: 2.2989370822906494\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 168, Loss: 2.2965316772460938\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 167, Loss: 2.3062520027160645\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 164, Loss: 2.303943157196045\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 169, Loss: 2.301083564758301\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 168, Loss: 2.3051369190216064\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 165, Loss: 2.3027303218841553\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 169, Loss: 2.3007664680480957\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 170, Loss: 2.307962417602539\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 166, Loss: 2.3026363849639893\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 171, Loss: 2.308957815170288\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 170, Loss: 2.299931526184082\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 167, Loss: 2.3025388717651367\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 168, Loss: 2.302495241165161\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 172, Loss: 2.29712176322937\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 171, Loss: 2.297482967376709\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 169, Loss: 2.2985808849334717\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 173, Loss: 2.299215793609619\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 172, Loss: 2.2982394695281982\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 174, Loss: 2.3115222454071045\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 173, Loss: 2.2966995239257812\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 170, Loss: 2.2992186546325684\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 175, Loss: 2.3066906929016113\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 174, Loss: 2.2994351387023926\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 171, Loss: 2.3029537200927734\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 176, Loss: 2.2954599857330322\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 175, Loss: 2.306419610977173\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 172, Loss: 2.304163932800293\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 176, Loss: 2.3061089515686035\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 173, Loss: 2.303612232208252\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 177, Loss: 2.3085553646087646\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 177, Loss: 2.3027257919311523\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 174, Loss: 2.3064358234405518\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 178, Loss: 2.2954654693603516\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 178, Loss: 2.308335781097412\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 175, Loss: 2.3022117614746094\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 179, Loss: 2.304879665374756\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 179, Loss: 2.3081936836242676\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 176, Loss: 2.305551528930664\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 180, Loss: 2.3013453483581543\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 180, Loss: 2.3011350631713867\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 177, Loss: 2.301969528198242\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 181, Loss: 2.306718349456787\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 181, Loss: 2.3109121322631836\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 178, Loss: 2.3014163970947266\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 182, Loss: 2.3041529655456543\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 182, Loss: 2.296842575073242\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 179, Loss: 2.3035497665405273\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 183, Loss: 2.305164337158203\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 183, Loss: 2.3044533729553223\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 180, Loss: 2.3030104637145996\n","Parameter Server - Received gradients from Worker 1\n","Parameter Server - Received gradients from Worker 2\n","Worker 1 - Step 184, Loss: 2.3095197677612305Worker 2 - Step 184, Loss: 2.3060266971588135\n","\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 181, Loss: 2.3018105030059814\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 185, Loss: 2.2988195419311523\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 185, Loss: 2.3045825958251953\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 182, Loss: 2.3001255989074707\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 186, Loss: 2.296870708465576\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 186, Loss: 2.310908794403076\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 183, Loss: 2.3026766777038574\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 187, Loss: 2.301187038421631\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 187, Loss: 2.3061881065368652\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 184, Loss: 2.302795171737671\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 188, Loss: 2.302006244659424\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 188, Loss: 2.3087899684906006\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 185, Loss: 2.2975950241088867\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 189, Loss: 2.303976535797119\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 186, Loss: 2.297637462615967\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 189, Loss: 2.3018081188201904\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 190, Loss: 2.2938928604125977\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 190, Loss: 2.301621437072754\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 187, Loss: 2.3028361797332764\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 191, Loss: 2.2902588844299316\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 191, Loss: 2.3049392700195312\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 188, Loss: 2.3043060302734375\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 192, Loss: 2.2960658073425293\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 189, Loss: 2.302182674407959\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 192, Loss: 2.3033387660980225\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 193, Loss: 2.299307346343994\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 190, Loss: 2.305866241455078\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 193, Loss: 2.298776865005493\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 194, Loss: 2.3001303672790527\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 191, Loss: 2.3024234771728516\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 194, Loss: 2.300719738006592\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 192, Loss: 2.304004669189453\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 195, Loss: 2.2975282669067383\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 195, Loss: 2.305497169494629\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 193, Loss: 2.2991092205047607\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 196, Loss: 2.306750774383545\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 196, Loss: 2.2916131019592285\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 194, Loss: 2.3038110733032227\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 197, Loss: 2.3026652336120605\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 195, Loss: 2.301729440689087\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 197, Loss: 2.2976386547088623\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 198, Loss: 2.3103785514831543\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 196, Loss: 2.3057007789611816\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 198, Loss: 2.3021793365478516\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 199, Loss: 2.297391891479492\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 199, Loss: 2.292900800704956\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 197, Loss: 2.3013103008270264\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 200, Loss: 2.2993924617767334\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 200, Loss: 2.2984910011291504\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 198, Loss: 2.29978084564209\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 201, Loss: 2.298762798309326\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 201, Loss: 2.3066487312316895\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 199, Loss: 2.3039846420288086\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 202, Loss: 2.311264991760254\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 202, Loss: 2.299449920654297\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 200, Loss: 2.3001761436462402\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 203, Loss: 2.303673505783081\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 201, Loss: 2.301508665084839\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 203, Loss: 2.3001956939697266\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 202, Loss: 2.306429862976074\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 204, Loss: 2.2974295616149902\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 204, Loss: 2.302919387817383\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 203, Loss: 2.3010687828063965\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 205, Loss: 2.300658702850342\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 205, Loss: 2.301730155944824\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 206, Loss: 2.299701690673828\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 204, Loss: 2.308274745941162\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 206, Loss: 2.2963333129882812\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 207, Loss: 2.2989959716796875\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 205, Loss: 2.3026363849639893\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 207, Loss: 2.3087716102600098\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 208, Loss: 2.306283950805664\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 206, Loss: 2.30055832862854\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 208, Loss: 2.2986416816711426\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 209, Loss: 2.2965335845947266\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 209, Loss: 2.3038763999938965\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 207, Loss: 2.3014190196990967\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 208, Loss: 2.3053808212280273\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 210, Loss: 2.307096242904663\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 210, Loss: 2.295835018157959\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 209, Loss: 2.305879592895508\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 211, Loss: 2.299436569213867\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 211, Loss: 2.3008875846862793\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 212, Loss: 2.2978649139404297\n","Parameter Server - Received gradients from Worker 0\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 212, Loss: 2.301632881164551\n","Worker 0 - Step 210, Loss: 2.3069796562194824\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 211, Loss: 2.2998433113098145\n","Parameter Server - Received gradients from Worker 2\n","Parameter Server - Received gradients from Worker 1\n","Worker 2 - Step 213, Loss: 2.305480480194092\n","Worker 1 - Step 213, Loss: 2.307474374771118\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 214, Loss: 2.305467128753662\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 212, Loss: 2.3027329444885254\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 214, Loss: 2.3077192306518555\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 215, Loss: 2.3008430004119873\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 213, Loss: 2.3036322593688965\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 215, Loss: 2.3000540733337402\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 216, Loss: 2.3012897968292236\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 214, Loss: 2.301727294921875\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 216, Loss: 2.3037760257720947\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 217, Loss: 2.2975213527679443\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 215, Loss: 2.303212881088257\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 217, Loss: 2.2995822429656982\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 216, Loss: 2.304600238800049\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 218, Loss: 2.3054990768432617\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 218, Loss: 2.311002492904663\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 217, Loss: 2.3031415939331055\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 219, Loss: 2.305893898010254\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 219, Loss: 2.301236152648926\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 218, Loss: 2.299903154373169\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 220, Loss: 2.3076343536376953\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 220, Loss: 2.3109846115112305\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 219, Loss: 2.299461841583252\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 221, Loss: 2.304429531097412\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 221, Loss: 2.3095173835754395\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 220, Loss: 2.306692600250244\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 222, Loss: 2.3004672527313232\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 222, Loss: 2.308229923248291\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 221, Loss: 2.3006677627563477\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 223, Loss: 2.297577142715454\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 223, Loss: 2.301090717315674\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 222, Loss: 2.305248737335205\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 224, Loss: 2.298339366912842\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 224, Loss: 2.303460121154785\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 225, Loss: 2.302610397338867\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 223, Loss: 2.301396608352661\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 225, Loss: 2.310317277908325\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 226, Loss: 2.3032760620117188\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 224, Loss: 2.302034378051758\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 226, Loss: 2.299687385559082\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 227, Loss: 2.3023815155029297\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 225, Loss: 2.2995269298553467\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 227, Loss: 2.297110080718994\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 228, Loss: 2.3051252365112305\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 228, Loss: 2.3019299507141113\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 226, Loss: 2.299159288406372\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 229, Loss: 2.309603214263916\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 229, Loss: 2.3023715019226074\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 227, Loss: 2.3048133850097656\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 230, Loss: 2.3019886016845703\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 230, Loss: 2.310058832168579\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 228, Loss: 2.302694320678711\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 231, Loss: 2.3020849227905273\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 229, Loss: 2.2993640899658203\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 232, Loss: 2.30526065826416\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 231, Loss: 2.29950213432312\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 233, Loss: 2.3025007247924805\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 230, Loss: 2.299553394317627\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 232, Loss: 2.304838180541992\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 231, Loss: 2.3049933910369873\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 234, Loss: 2.3017072677612305\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 233, Loss: 2.2964534759521484\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 232, Loss: 2.305433988571167\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 235, Loss: 2.297942638397217\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 234, Loss: 2.294890880584717\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 236, Loss: 2.3038816452026367\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 233, Loss: 2.300588607788086\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 235, Loss: 2.3057804107666016\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 237, Loss: 2.3069357872009277\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 234, Loss: 2.3048744201660156\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 236, Loss: 2.3057761192321777\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 238, Loss: 2.30631947517395\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 235, Loss: 2.3009097576141357\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 237, Loss: 2.3037939071655273\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 239, Loss: 2.306072950363159\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 236, Loss: 2.3024942874908447\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 238, Loss: 2.3021016120910645\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 240, Loss: 2.3024892807006836\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 237, Loss: 2.3019587993621826\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 239, Loss: 2.298077344894409\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 241, Loss: 2.29941463470459\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 238, Loss: 2.298746109008789\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 240, Loss: 2.3052191734313965\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 242, Loss: 2.2909607887268066\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 239, Loss: 2.3042550086975098\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 241, Loss: 2.3032164573669434\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 243, Loss: 2.2998416423797607\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 240, Loss: 2.310544729232788\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 242, Loss: 2.3101742267608643\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 244, Loss: 2.3018527030944824\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 241, Loss: 2.297853946685791\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 243, Loss: 2.3013808727264404\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 245, Loss: 2.2981674671173096\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 242, Loss: 2.3029775619506836\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 244, Loss: 2.2942733764648438\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 246, Loss: 2.293168067932129\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 245, Loss: 2.304823160171509\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 243, Loss: 2.297504425048828\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 244, Loss: 2.3027427196502686\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 246, Loss: 2.3065567016601562\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 247, Loss: 2.297868251800537\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 245, Loss: 2.3016464710235596\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 248, Loss: 2.3005025386810303\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 247, Loss: 2.3068923950195312\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 246, Loss: 2.303253412246704\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 249, Loss: 2.2980828285217285\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 248, Loss: 2.2998383045196533\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 249, Loss: 2.304075002670288\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 247, Loss: 2.3017544746398926\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 250, Loss: 2.3090481758117676\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 250, Loss: 2.3015732765197754\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 248, Loss: 2.305785655975342\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 251, Loss: 2.300288200378418\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 251, Loss: 2.3058247566223145\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 249, Loss: 2.3058242797851562\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 252, Loss: 2.3067145347595215\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 252, Loss: 2.309892177581787\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 250, Loss: 2.3000144958496094\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 253, Loss: 2.2957162857055664\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 253, Loss: 2.3071610927581787\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 251, Loss: 2.3025145530700684\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 254, Loss: 2.299318313598633\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 254, Loss: 2.303400993347168\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 252, Loss: 2.3041532039642334\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 255, Loss: 2.303555965423584\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 255, Loss: 2.3059511184692383\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 253, Loss: 2.303593873977661\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 256, Loss: 2.297727584838867\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 256, Loss: 2.2908778190612793\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 254, Loss: 2.303393840789795\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 257, Loss: 2.3088040351867676\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 257, Loss: 2.3045084476470947\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 255, Loss: 2.306209087371826\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 258, Loss: 2.3004422187805176\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 258, Loss: 2.310925006866455\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 256, Loss: 2.3006253242492676\n","Parameter Server - Received gradients from Worker 2\n","Worker 2 - Step 259, Loss: 2.3058226108551025\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 257, Loss: 2.302849292755127\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 259, Loss: 2.299971580505371\n","Parameter Server - Received gradients from Worker 1\n","Worker 1 - Step 260, Loss: 2.2995429039001465\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 258, Loss: 2.3019819259643555\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 259, Loss: 2.3051540851593018\n","Parameter Server - Received gradients from Worker 0\n","Worker 0 - Step 260, Loss: 2.3062260150909424\n","Parameter Server - Gradients applied and parameters updated\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.0916 - loss: 2.3010\n","Test Loss: 2.300501823425293\n","Test Accuracy: 0.093299999833107\n"]}]},{"cell_type":"markdown","source":["Worker 0 gets the first shard.\n","Worker 1 gets the second shard.\n","Worker 2 gets the third shard."],"metadata":{"id":"GjpRWerFxpG8"}},{"cell_type":"code","source":[],"metadata":{"id":"xhwY5ex_xp7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import numpy as np\n","\n","# Define the model\n","def create_model():\n","    return models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),  # Input layer\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Worker 0 updates this\n","        layers.AveragePooling2D(pool_size=(2, 2)),                # Worker 0 skips this (no weights)\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Worker 1 updates this\n","        layers.AveragePooling2D(pool_size=(2, 2)),                # Worker 1 skips this (no weights)\n","        layers.Flatten(),                                         # Worker 1 skips this (no weights)\n","        layers.Dense(120, activation='relu'),                    # Worker 1 updates this\n","        layers.Dense(84, activation='relu'),                     # Worker 1 updates this\n","        layers.Dense(10, activation='softmax')                   # Worker 1 updates this\n","    ])\n","\n","# Dataset preparation\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Custom Worker Class\n","class Worker:\n","    def __init__(self, worker_id, model, layers_to_update, dataset):\n","        self.worker_id = worker_id\n","        self.model = model\n","        self.layers_to_update = layers_to_update  # Indices of layers this worker updates\n","        self.dataset = dataset\n","        self.optimizer = tf.keras.optimizers.Adam()\n","\n","    def compute_and_update(self, features, labels):\n","        with tf.GradientTape(persistent=True) as tape:\n","            predictions = self.model(features, training=True)\n","            loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n","            loss = tf.reduce_mean(loss)\n","\n","        # Compute gradients only for the assigned layers with trainable weights\n","        gradients = []\n","        variables = []\n","        for i in self.layers_to_update:\n","            if i < len(self.model.layers) and self.model.layers[i].trainable_weights:\n","                gradients.extend(tape.gradient(loss, self.model.layers[i].trainable_weights))\n","                variables.extend(self.model.layers[i].trainable_weights)\n","\n","        # Apply gradients if there are any\n","        if gradients and variables:\n","            self.optimizer.apply_gradients(zip(gradients, variables))\n","            print(f\"Worker {self.worker_id} updated layers: {self.layers_to_update}\")\n","        else:\n","            print(f\"Worker {self.worker_id} found no trainable variables to update.\")\n","\n","        del tape\n","        return loss\n","\n","    def run(self):\n","        for step, (features, labels) in enumerate(self.dataset):\n","            features = tf.convert_to_tensor(features)\n","            labels = tf.convert_to_tensor(labels)\n","            loss = self.compute_and_update(features, labels)\n","            print(f\"Worker {self.worker_id} - Step {step}, Loss: {loss.numpy()}\")\n","\n","# Create datasets\n","train_dataset, test_dataset = create_dataset()\n","\n","# Verify model structure\n","global_model = create_model()\n","global_model.summary()\n","\n","# Assign layers to workers based on the model summary\n","worker_0_layers = [0]  # Conv2D(6) for Worker 0\n","worker_1_layers = [2, 5, 6, 7]  # Conv2D(16) and Dense layers for Worker 1\n","\n","# Split dataset for workers\n","worker_0_dataset = train_dataset.shard(num_shards=2, index=0)\n","worker_1_dataset = train_dataset.shard(num_shards=2, index=1)\n","\n","# Create workers\n","worker_0 = Worker(worker_id=0, model=global_model, layers_to_update=worker_0_layers, dataset=worker_0_dataset)\n","worker_1 = Worker(worker_id=1, model=global_model, layers_to_update=worker_1_layers, dataset=worker_1_dataset)\n","\n","# Run workers\n","worker_0.run()\n","worker_1.run()\n","\n","# Evaluate the global model\n","global_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","test_loss, test_acc = global_model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_acc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vkJF48y3myco","executionInfo":{"status":"ok","timestamp":1735466491070,"user_tz":-330,"elapsed":122566,"user":{"displayName":"Chandra Sekhar","userId":"00330720378866032186"}},"outputId":"380aed03-b805-454c-aa8d-08ea7c2f43cc"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_10\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)           │             \u001b[38;5;34m156\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_20                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │           \u001b[38;5;34m2,416\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_21                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)                 │          \u001b[38;5;34m48,120\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)                  │          \u001b[38;5;34m10,164\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m850\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_20                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_21                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Worker 0 updated layers: [0]\n","Worker 0 - Step 0, Loss: 2.3132429122924805\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 1, Loss: 2.3208091259002686\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 2, Loss: 2.3052821159362793\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 3, Loss: 2.3051953315734863\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 4, Loss: 2.311065435409546\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 5, Loss: 2.3174688816070557\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 6, Loss: 2.297121047973633\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 7, Loss: 2.2987008094787598\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 8, Loss: 2.303971529006958\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 9, Loss: 2.311938524246216\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 10, Loss: 2.3091397285461426\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 11, Loss: 2.2956953048706055\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 12, Loss: 2.29311466217041\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 13, Loss: 2.3084816932678223\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 14, Loss: 2.3017497062683105\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 15, Loss: 2.296990394592285\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 16, Loss: 2.309321165084839\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 17, Loss: 2.3053970336914062\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 18, Loss: 2.293076515197754\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 19, Loss: 2.3043928146362305\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 20, Loss: 2.309767246246338\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 21, Loss: 2.3008198738098145\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 22, Loss: 2.3028903007507324\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 23, Loss: 2.3055260181427\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 24, Loss: 2.3085484504699707\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 25, Loss: 2.300302505493164\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 26, Loss: 2.306337833404541\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 27, Loss: 2.3027637004852295\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 28, Loss: 2.304511070251465\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 29, Loss: 2.304896593093872\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 30, Loss: 2.2953953742980957\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 31, Loss: 2.304171562194824\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 32, Loss: 2.3053226470947266\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 33, Loss: 2.3106236457824707\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 34, Loss: 2.303833484649658\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 35, Loss: 2.303966522216797\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 36, Loss: 2.306638717651367\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 37, Loss: 2.3025970458984375\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 38, Loss: 2.304006576538086\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 39, Loss: 2.2986252307891846\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 40, Loss: 2.2997303009033203\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 41, Loss: 2.308825731277466\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 42, Loss: 2.3072152137756348\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 43, Loss: 2.3063297271728516\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 44, Loss: 2.3043060302734375\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 45, Loss: 2.3067684173583984\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 46, Loss: 2.3017852306365967\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 47, Loss: 2.3048510551452637\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 48, Loss: 2.3069984912872314\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 49, Loss: 2.311528205871582\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 50, Loss: 2.2976813316345215\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 51, Loss: 2.302572727203369\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 52, Loss: 2.3002119064331055\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 53, Loss: 2.2988173961639404\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 54, Loss: 2.299443244934082\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 55, Loss: 2.3016481399536133\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 56, Loss: 2.30336594581604\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 57, Loss: 2.3000457286834717\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 58, Loss: 2.3024520874023438\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 59, Loss: 2.300880193710327\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 60, Loss: 2.3062472343444824\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 61, Loss: 2.3012166023254395\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 62, Loss: 2.303234577178955\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 63, Loss: 2.3035337924957275\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 64, Loss: 2.3018524646759033\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 65, Loss: 2.3023037910461426\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 66, Loss: 2.3010852336883545\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 67, Loss: 2.299551248550415\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 68, Loss: 2.304047107696533\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 69, Loss: 2.3011085987091064\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 70, Loss: 2.2994375228881836\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 71, Loss: 2.300692081451416\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 72, Loss: 2.303683280944824\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 73, Loss: 2.2989048957824707\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 74, Loss: 2.3009305000305176\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 75, Loss: 2.302403211593628\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 76, Loss: 2.298910617828369\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 77, Loss: 2.2999820709228516\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 78, Loss: 2.3009414672851562\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 79, Loss: 2.3008906841278076\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 80, Loss: 2.2999987602233887\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 81, Loss: 2.3101654052734375\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 82, Loss: 2.3080406188964844\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 83, Loss: 2.300476551055908\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 84, Loss: 2.3000776767730713\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 85, Loss: 2.3011398315429688\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 86, Loss: 2.300215482711792\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 87, Loss: 2.3060436248779297\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 88, Loss: 2.302333116531372\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 89, Loss: 2.301839590072632\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 90, Loss: 2.300516128540039\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 91, Loss: 2.298860788345337\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 92, Loss: 2.3024895191192627\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 93, Loss: 2.3097734451293945\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 94, Loss: 2.2983884811401367\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 95, Loss: 2.30133056640625\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 96, Loss: 2.301658868789673\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 97, Loss: 2.301513433456421\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 98, Loss: 2.3006913661956787\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 99, Loss: 2.3041293621063232\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 100, Loss: 2.301938056945801\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 101, Loss: 2.308347225189209\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 102, Loss: 2.3004703521728516\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 103, Loss: 2.3067307472229004\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 104, Loss: 2.296830654144287\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 105, Loss: 2.3051674365997314\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 106, Loss: 2.2993416786193848\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 107, Loss: 2.297427177429199\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 108, Loss: 2.303014039993286\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 109, Loss: 2.30326771736145\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 110, Loss: 2.3039145469665527\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 111, Loss: 2.3090767860412598\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 112, Loss: 2.303689479827881\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 113, Loss: 2.301551342010498\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 114, Loss: 2.3027052879333496\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 115, Loss: 2.3025364875793457\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 116, Loss: 2.3068742752075195\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 117, Loss: 2.2994680404663086\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 118, Loss: 2.2973859310150146\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 119, Loss: 2.309440851211548\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 120, Loss: 2.3086347579956055\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 121, Loss: 2.3082218170166016\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 122, Loss: 2.310392379760742\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 123, Loss: 2.301642417907715\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 124, Loss: 2.2999463081359863\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 125, Loss: 2.3015058040618896\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 126, Loss: 2.303643226623535\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 127, Loss: 2.3037033081054688\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 128, Loss: 2.300537109375\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 129, Loss: 2.304553508758545\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 130, Loss: 2.304098606109619\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 131, Loss: 2.30293607711792\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 132, Loss: 2.300675630569458\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 133, Loss: 2.303933620452881\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 134, Loss: 2.304516315460205\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 135, Loss: 2.304389476776123\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 136, Loss: 2.303269386291504\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 137, Loss: 2.300480365753174\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 138, Loss: 2.301828622817993\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 139, Loss: 2.30118989944458\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 140, Loss: 2.3009133338928223\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 141, Loss: 2.3024415969848633\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 142, Loss: 2.3009281158447266\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 143, Loss: 2.302725076675415\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 144, Loss: 2.300685405731201\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 145, Loss: 2.299859046936035\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 146, Loss: 2.302429676055908\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 147, Loss: 2.3008322715759277\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 148, Loss: 2.3008344173431396\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 149, Loss: 2.301819324493408\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 150, Loss: 2.298412322998047\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 151, Loss: 2.299514055252075\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 152, Loss: 2.301445960998535\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 153, Loss: 2.304168939590454\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 154, Loss: 2.2962441444396973\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 155, Loss: 2.3047022819519043\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 156, Loss: 2.3090484142303467\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 157, Loss: 2.304036855697632\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 158, Loss: 2.3098034858703613\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 159, Loss: 2.2949225902557373\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 160, Loss: 2.3009231090545654\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 161, Loss: 2.301771640777588\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 162, Loss: 2.299356460571289\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 163, Loss: 2.2967488765716553\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 164, Loss: 2.2983031272888184\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 165, Loss: 2.3028135299682617\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 166, Loss: 2.2976677417755127\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 167, Loss: 2.295301914215088\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 168, Loss: 2.301600217819214\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 169, Loss: 2.3074822425842285\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 170, Loss: 2.3019871711730957\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 171, Loss: 2.299506902694702\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 172, Loss: 2.3026123046875\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 173, Loss: 2.3003740310668945\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 174, Loss: 2.297863245010376\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 175, Loss: 2.297483444213867\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 176, Loss: 2.304105043411255\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 177, Loss: 2.2961061000823975\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 178, Loss: 2.302398681640625\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 179, Loss: 2.2995102405548096\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 180, Loss: 2.301483154296875\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 181, Loss: 2.301023006439209\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 182, Loss: 2.307115077972412\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 183, Loss: 2.3002820014953613\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 184, Loss: 2.305312156677246\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 185, Loss: 2.3049261569976807\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 186, Loss: 2.3055927753448486\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 187, Loss: 2.295603036880493\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 188, Loss: 2.2956900596618652\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 189, Loss: 2.3011062145233154\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 190, Loss: 2.306391477584839\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 191, Loss: 2.2944793701171875\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 192, Loss: 2.297037363052368\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 193, Loss: 2.3001184463500977\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 194, Loss: 2.2943224906921387\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 195, Loss: 2.3040876388549805\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 196, Loss: 2.307405710220337\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 197, Loss: 2.3055062294006348\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 198, Loss: 2.3028807640075684\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 199, Loss: 2.3016786575317383\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 200, Loss: 2.297675132751465\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 201, Loss: 2.3001608848571777\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 202, Loss: 2.300884962081909\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 203, Loss: 2.3088600635528564\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 204, Loss: 2.309121608734131\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 205, Loss: 2.304409980773926\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 206, Loss: 2.3009893894195557\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 207, Loss: 2.299751043319702\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 208, Loss: 2.2963826656341553\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 209, Loss: 2.295897960662842\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 210, Loss: 2.29399037361145\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 211, Loss: 2.304023265838623\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 212, Loss: 2.303025245666504\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 213, Loss: 2.2953882217407227\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 214, Loss: 2.2959418296813965\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 215, Loss: 2.2988693714141846\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 216, Loss: 2.3021109104156494\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 217, Loss: 2.3024234771728516\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 218, Loss: 2.299896001815796\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 219, Loss: 2.3106656074523926\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 220, Loss: 2.3082022666931152\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 221, Loss: 2.304516315460205\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 222, Loss: 2.3028836250305176\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 223, Loss: 2.30015230178833\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 224, Loss: 2.295144557952881\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 225, Loss: 2.301758289337158\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 226, Loss: 2.300201416015625\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 227, Loss: 2.3056678771972656\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 228, Loss: 2.3003125190734863\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 229, Loss: 2.303187370300293\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 230, Loss: 2.3029699325561523\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 231, Loss: 2.305539608001709\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 232, Loss: 2.2951724529266357\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 233, Loss: 2.2945151329040527\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 234, Loss: 2.305901527404785\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 235, Loss: 2.3017005920410156\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 236, Loss: 2.3006083965301514\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 237, Loss: 2.301961660385132\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 238, Loss: 2.3020949363708496\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 239, Loss: 2.302114248275757\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 240, Loss: 2.3034281730651855\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 241, Loss: 2.2982821464538574\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 242, Loss: 2.3047068119049072\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 243, Loss: 2.297884941101074\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 244, Loss: 2.3112130165100098\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 245, Loss: 2.3040637969970703\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 246, Loss: 2.3027515411376953\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 247, Loss: 2.297412157058716\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 248, Loss: 2.3083550930023193\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 249, Loss: 2.2992758750915527\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 250, Loss: 2.30419659614563\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 251, Loss: 2.311520576477051\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 252, Loss: 2.3030858039855957\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 253, Loss: 2.2992773056030273\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 254, Loss: 2.3069005012512207\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 255, Loss: 2.298804521560669\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 256, Loss: 2.3073432445526123\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 257, Loss: 2.2976303100585938\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 258, Loss: 2.299537181854248\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 259, Loss: 2.295900821685791\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 260, Loss: 2.3048136234283447\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 261, Loss: 2.298161268234253\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 262, Loss: 2.3047657012939453\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 263, Loss: 2.3025131225585938\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 264, Loss: 2.303375720977783\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 265, Loss: 2.305572509765625\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 266, Loss: 2.2934112548828125\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 267, Loss: 2.296538829803467\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 268, Loss: 2.304257869720459\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 269, Loss: 2.300872325897217\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 270, Loss: 2.2983756065368652\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 271, Loss: 2.3041419982910156\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 272, Loss: 2.3004753589630127\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 273, Loss: 2.2929208278656006\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 274, Loss: 2.2981929779052734\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 275, Loss: 2.2933640480041504\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 276, Loss: 2.2975151538848877\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 277, Loss: 2.301853656768799\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 278, Loss: 2.298311233520508\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 279, Loss: 2.3007025718688965\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 280, Loss: 2.296689510345459\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 281, Loss: 2.3052916526794434\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 282, Loss: 2.309652328491211\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 283, Loss: 2.299234628677368\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 284, Loss: 2.3024165630340576\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 285, Loss: 2.3005459308624268\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 286, Loss: 2.297025442123413\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 287, Loss: 2.301095962524414\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 288, Loss: 2.29335355758667\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 289, Loss: 2.295938491821289\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 290, Loss: 2.300466537475586\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 291, Loss: 2.310441493988037\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 292, Loss: 2.3004448413848877\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 293, Loss: 2.300313711166382\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 294, Loss: 2.300480365753174\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 295, Loss: 2.3002233505249023\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 296, Loss: 2.3077592849731445\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 297, Loss: 2.2930335998535156\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 298, Loss: 2.3033814430236816\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 299, Loss: 2.301222801208496\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 300, Loss: 2.3039679527282715\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 301, Loss: 2.3100314140319824\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 302, Loss: 2.298625946044922\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 303, Loss: 2.3029847145080566\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 304, Loss: 2.3096518516540527\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 305, Loss: 2.302579879760742\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 306, Loss: 2.3052306175231934\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 307, Loss: 2.305274248123169\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 308, Loss: 2.3050694465637207\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 309, Loss: 2.302089214324951\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 310, Loss: 2.2997400760650635\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 311, Loss: 2.302183151245117\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 312, Loss: 2.2992665767669678\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 313, Loss: 2.299489736557007\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 314, Loss: 2.294217586517334\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 315, Loss: 2.300905227661133\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 316, Loss: 2.293883800506592\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 317, Loss: 2.2950572967529297\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 318, Loss: 2.299403190612793\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 319, Loss: 2.3034181594848633\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 320, Loss: 2.302959442138672\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 321, Loss: 2.294041156768799\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 322, Loss: 2.3038644790649414\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 323, Loss: 2.3087821006774902\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 324, Loss: 2.3042044639587402\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 325, Loss: 2.3064327239990234\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 326, Loss: 2.299701690673828\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 327, Loss: 2.303399085998535\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 328, Loss: 2.2983334064483643\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 329, Loss: 2.30232834815979\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 330, Loss: 2.300323009490967\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 331, Loss: 2.302417039871216\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 332, Loss: 2.2983944416046143\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 333, Loss: 2.297441244125366\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 334, Loss: 2.297636032104492\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 335, Loss: 2.2984182834625244\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 336, Loss: 2.291597843170166\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 337, Loss: 2.308762311935425\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 338, Loss: 2.3040242195129395\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 339, Loss: 2.2998039722442627\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 340, Loss: 2.294755697250366\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 341, Loss: 2.3020987510681152\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 342, Loss: 2.30326247215271\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 343, Loss: 2.301884651184082\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 344, Loss: 2.3055858612060547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 345, Loss: 2.3052151203155518\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 346, Loss: 2.3021774291992188\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 347, Loss: 2.3020992279052734\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 348, Loss: 2.3097331523895264\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 349, Loss: 2.292788028717041\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 350, Loss: 2.2961373329162598\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 351, Loss: 2.2974183559417725\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 352, Loss: 2.3003804683685303\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 353, Loss: 2.299058437347412\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 354, Loss: 2.297422170639038\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 355, Loss: 2.301044225692749\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 356, Loss: 2.2946064472198486\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 357, Loss: 2.3013696670532227\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 358, Loss: 2.2968573570251465\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 359, Loss: 2.3058106899261475\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 360, Loss: 2.2895026206970215\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 361, Loss: 2.305828094482422\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 362, Loss: 2.3071346282958984\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 363, Loss: 2.295149803161621\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 364, Loss: 2.3020756244659424\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 365, Loss: 2.2953078746795654\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 366, Loss: 2.29866623878479\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 367, Loss: 2.307321548461914\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 368, Loss: 2.302971839904785\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 369, Loss: 2.29518985748291\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 370, Loss: 2.301619291305542\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 371, Loss: 2.296961545944214\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 372, Loss: 2.300243377685547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 373, Loss: 2.2983298301696777\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 374, Loss: 2.2958946228027344\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 375, Loss: 2.3015947341918945\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 376, Loss: 2.2985520362854004\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 377, Loss: 2.2948598861694336\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 378, Loss: 2.301046371459961\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 379, Loss: 2.3001708984375\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 380, Loss: 2.299180269241333\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 381, Loss: 2.306713581085205\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 382, Loss: 2.300719738006592\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 383, Loss: 2.306342124938965\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 384, Loss: 2.298849582672119\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 385, Loss: 2.298495054244995\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 386, Loss: 2.3015947341918945\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 387, Loss: 2.2953007221221924\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 388, Loss: 2.3077993392944336\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 389, Loss: 2.311509609222412\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 390, Loss: 2.2984204292297363\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 0, Loss: 2.3051066398620605\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 1, Loss: 2.300591468811035\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 2, Loss: 2.3087806701660156\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 3, Loss: 2.2971203327178955\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 4, Loss: 2.287259578704834\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 5, Loss: 2.290750026702881\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 6, Loss: 2.2864065170288086\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 7, Loss: 2.2945146560668945\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 8, Loss: 2.2778539657592773\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 9, Loss: 2.27679443359375\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 10, Loss: 2.2771337032318115\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 11, Loss: 2.2714462280273438\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 12, Loss: 2.2774124145507812\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 13, Loss: 2.312002182006836\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 14, Loss: 2.2747879028320312\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 15, Loss: 2.24770450592041\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 16, Loss: 2.250366687774658\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 17, Loss: 2.278992176055908\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 18, Loss: 2.316237449645996\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 19, Loss: 2.2556960582733154\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 20, Loss: 2.2762131690979004\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 21, Loss: 2.270817518234253\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 22, Loss: 2.2167768478393555\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 23, Loss: 2.2837750911712646\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 24, Loss: 2.2603964805603027\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 25, Loss: 2.238365888595581\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 26, Loss: 2.2581350803375244\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 27, Loss: 2.2546193599700928\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 28, Loss: 2.221860408782959\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 29, Loss: 2.2284321784973145\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 30, Loss: 2.2306010723114014\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 31, Loss: 2.2219090461730957\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 32, Loss: 2.2271952629089355\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 33, Loss: 2.193556308746338\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 34, Loss: 2.1213254928588867\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 35, Loss: 2.233553171157837\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 36, Loss: 2.278197765350342\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 37, Loss: 2.247880458831787\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 38, Loss: 2.23728084564209\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 39, Loss: 2.1938581466674805\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 40, Loss: 2.254917621612549\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 41, Loss: 2.109043598175049\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 42, Loss: 2.1092896461486816\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 43, Loss: 2.1657886505126953\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 44, Loss: 2.1743385791778564\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 45, Loss: 2.1042208671569824\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 46, Loss: 2.1594035625457764\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 47, Loss: 2.237640380859375\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 48, Loss: 2.212493896484375\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 49, Loss: 2.152606725692749\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 50, Loss: 2.137559413909912\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 51, Loss: 2.1239840984344482\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 52, Loss: 2.1242105960845947\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 53, Loss: 2.139179229736328\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 54, Loss: 2.0874266624450684\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 55, Loss: 2.2510430812835693\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 56, Loss: 2.102674961090088\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 57, Loss: 2.167375087738037\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 58, Loss: 2.2115910053253174\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 59, Loss: 2.0997655391693115\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 60, Loss: 2.098768711090088\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 61, Loss: 2.190187454223633\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 62, Loss: 2.124955177307129\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 63, Loss: 2.1394896507263184\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 64, Loss: 2.0940914154052734\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 65, Loss: 2.108603000640869\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 66, Loss: 2.1599533557891846\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 67, Loss: 2.25161075592041\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 68, Loss: 2.103363037109375\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 69, Loss: 2.127336025238037\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 70, Loss: 2.035953998565674\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 71, Loss: 2.086242914199829\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 72, Loss: 2.118936061859131\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 73, Loss: 2.082857608795166\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 74, Loss: 2.1402039527893066\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 75, Loss: 1.9998807907104492\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 76, Loss: 2.0848398208618164\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 77, Loss: 2.0447027683258057\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 78, Loss: 2.1865320205688477\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 79, Loss: 2.137505054473877\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 80, Loss: 2.258708953857422\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 81, Loss: 2.0448670387268066\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 82, Loss: 2.0238075256347656\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 83, Loss: 2.073028564453125\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 84, Loss: 2.081543207168579\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 85, Loss: 2.06563663482666\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 86, Loss: 2.089778423309326\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 87, Loss: 2.2332940101623535\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 88, Loss: 2.061939239501953\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 89, Loss: 2.000843048095703\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 90, Loss: 2.0234527587890625\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 91, Loss: 2.105471611022949\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 92, Loss: 2.0066728591918945\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 93, Loss: 2.092937469482422\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 94, Loss: 1.9976508617401123\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 95, Loss: 2.2242226600646973\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 96, Loss: 2.0436718463897705\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 97, Loss: 2.0508766174316406\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 98, Loss: 2.106144428253174\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 99, Loss: 2.0345957279205322\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 100, Loss: 2.005094051361084\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 101, Loss: 2.1141481399536133\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 102, Loss: 2.10595440864563\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 103, Loss: 1.9860568046569824\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 104, Loss: 1.979081153869629\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 105, Loss: 1.931457281112671\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 106, Loss: 2.113219738006592\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 107, Loss: 1.97767174243927\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 108, Loss: 2.0176613330841064\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 109, Loss: 2.0155529975891113\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 110, Loss: 2.1274166107177734\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 111, Loss: 2.1076626777648926\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 112, Loss: 2.0517356395721436\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 113, Loss: 1.9438807964324951\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 114, Loss: 2.032895565032959\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 115, Loss: 1.9023728370666504\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 116, Loss: 1.94945228099823\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 117, Loss: 2.0454931259155273\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 118, Loss: 1.9241176843643188\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 119, Loss: 2.015932083129883\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 120, Loss: 1.8773682117462158\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 121, Loss: 2.036719799041748\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 122, Loss: 1.7792432308197021\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 123, Loss: 1.988762617111206\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 124, Loss: 1.9735777378082275\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 125, Loss: 2.0797128677368164\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 126, Loss: 2.1558523178100586\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 127, Loss: 1.984919548034668\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 128, Loss: 2.10139799118042\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 129, Loss: 2.027536392211914\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 130, Loss: 2.0185980796813965\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 131, Loss: 2.015007495880127\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 132, Loss: 2.203913688659668\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 133, Loss: 2.098015308380127\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 134, Loss: 1.9811967611312866\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 135, Loss: 2.037259101867676\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 136, Loss: 1.998640775680542\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 137, Loss: 2.0016255378723145\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 138, Loss: 2.0223286151885986\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 139, Loss: 2.234720230102539\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 140, Loss: 1.9353642463684082\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 141, Loss: 1.9702072143554688\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 142, Loss: 1.862576961517334\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 143, Loss: 2.0129847526550293\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 144, Loss: 2.0851354598999023\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 145, Loss: 1.840265154838562\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 146, Loss: 1.9511475563049316\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 147, Loss: 2.0393056869506836\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 148, Loss: 1.868335485458374\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 149, Loss: 2.036714553833008\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 150, Loss: 1.98563551902771\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 151, Loss: 1.9015769958496094\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 152, Loss: 2.006411075592041\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 153, Loss: 1.8162670135498047\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 154, Loss: 2.0814194679260254\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 155, Loss: 2.057755470275879\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 156, Loss: 2.1554150581359863\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 157, Loss: 1.8859262466430664\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 158, Loss: 1.9055821895599365\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 159, Loss: 2.0749118328094482\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 160, Loss: 2.067575216293335\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 161, Loss: 2.065721035003662\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 162, Loss: 1.97292160987854\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 163, Loss: 2.0881662368774414\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 164, Loss: 1.9731086492538452\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 165, Loss: 1.9155473709106445\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 166, Loss: 1.9852659702301025\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 167, Loss: 2.00911283493042\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 168, Loss: 2.03434681892395\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 169, Loss: 1.9950511455535889\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 170, Loss: 1.9723917245864868\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 171, Loss: 1.9795632362365723\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 172, Loss: 2.0114858150482178\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 173, Loss: 1.8391045331954956\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 174, Loss: 1.9324218034744263\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 175, Loss: 1.9221055507659912\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 176, Loss: 1.9672218561172485\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 177, Loss: 1.9544540643692017\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 178, Loss: 1.9955072402954102\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 179, Loss: 1.8658324480056763\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 180, Loss: 1.9567420482635498\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 181, Loss: 1.8856730461120605\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 182, Loss: 2.0294151306152344\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 183, Loss: 2.0590755939483643\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 184, Loss: 2.090803623199463\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 185, Loss: 1.925383448600769\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 186, Loss: 1.8277689218521118\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 187, Loss: 2.0407540798187256\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 188, Loss: 2.095767021179199\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 189, Loss: 2.0678963661193848\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 190, Loss: 1.9005221128463745\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 191, Loss: 2.0143847465515137\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 192, Loss: 1.9366979598999023\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 193, Loss: 2.022688865661621\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 194, Loss: 1.864161729812622\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 195, Loss: 1.9748859405517578\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 196, Loss: 1.9031367301940918\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 197, Loss: 1.9219801425933838\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 198, Loss: 1.9406309127807617\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 199, Loss: 1.8804807662963867\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 200, Loss: 1.9414905309677124\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 201, Loss: 1.9652413129806519\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 202, Loss: 1.9079031944274902\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 203, Loss: 2.1696581840515137\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 204, Loss: 1.879620909690857\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 205, Loss: 1.7918601036071777\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 206, Loss: 1.9860150814056396\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 207, Loss: 1.9312973022460938\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 208, Loss: 2.0496535301208496\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 209, Loss: 1.9816968441009521\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 210, Loss: 1.8898699283599854\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 211, Loss: 1.9913897514343262\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 212, Loss: 2.0571892261505127\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 213, Loss: 1.8782048225402832\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 214, Loss: 1.99402916431427\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 215, Loss: 2.0142641067504883\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 216, Loss: 2.0107574462890625\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 217, Loss: 1.7866097688674927\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 218, Loss: 1.9794368743896484\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 219, Loss: 1.8464324474334717\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 220, Loss: 1.9789953231811523\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 221, Loss: 1.9273134469985962\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 222, Loss: 1.7822351455688477\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 223, Loss: 1.8516677618026733\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 224, Loss: 1.9089547395706177\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 225, Loss: 1.8692007064819336\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 226, Loss: 1.9959241151809692\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 227, Loss: 1.894305944442749\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 228, Loss: 1.879989504814148\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 229, Loss: 1.9638009071350098\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 230, Loss: 2.018691062927246\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 231, Loss: 2.095311164855957\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 232, Loss: 1.9464695453643799\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 233, Loss: 2.0092527866363525\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 234, Loss: 1.7688672542572021\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 235, Loss: 1.7934736013412476\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 236, Loss: 1.93277108669281\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 237, Loss: 1.9124990701675415\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 238, Loss: 1.858208179473877\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 239, Loss: 1.8862195014953613\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 240, Loss: 1.927750825881958\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 241, Loss: 2.0456442832946777\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 242, Loss: 2.0126237869262695\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 243, Loss: 1.8334221839904785\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 244, Loss: 2.010655403137207\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 245, Loss: 1.9375016689300537\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 246, Loss: 1.9305143356323242\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 247, Loss: 1.9634449481964111\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 248, Loss: 1.9686044454574585\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 249, Loss: 1.8341009616851807\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 250, Loss: 1.8196566104888916\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 251, Loss: 1.9368648529052734\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 252, Loss: 1.9515564441680908\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 253, Loss: 1.9205660820007324\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 254, Loss: 1.9486660957336426\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 255, Loss: 1.7279807329177856\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 256, Loss: 1.6536180973052979\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 257, Loss: 1.9389452934265137\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 258, Loss: 1.7992748022079468\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 259, Loss: 2.079883575439453\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 260, Loss: 1.924830675125122\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 261, Loss: 1.915576696395874\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 262, Loss: 1.7745671272277832\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 263, Loss: 1.8951517343521118\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 264, Loss: 1.8629193305969238\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 265, Loss: 1.9158413410186768\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 266, Loss: 1.806781530380249\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 267, Loss: 1.8994730710983276\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 268, Loss: 1.959313154220581\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 269, Loss: 1.9807894229888916\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 270, Loss: 1.7109007835388184\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 271, Loss: 1.7658259868621826\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 272, Loss: 1.768773078918457\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 273, Loss: 1.9116830825805664\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 274, Loss: 2.135718584060669\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 275, Loss: 1.859933614730835\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 276, Loss: 1.8947513103485107\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 277, Loss: 1.9388158321380615\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 278, Loss: 1.8797577619552612\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 279, Loss: 1.8861671686172485\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 280, Loss: 1.7166309356689453\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 281, Loss: 1.871077299118042\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 282, Loss: 2.0063600540161133\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 283, Loss: 2.0400471687316895\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 284, Loss: 1.9330312013626099\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 285, Loss: 1.929274320602417\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 286, Loss: 1.8818535804748535\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 287, Loss: 2.0012598037719727\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 288, Loss: 1.8367366790771484\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 289, Loss: 2.0442943572998047\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 290, Loss: 1.9058847427368164\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 291, Loss: 1.8070944547653198\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 292, Loss: 1.7841622829437256\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 293, Loss: 1.977911353111267\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 294, Loss: 1.9118123054504395\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 295, Loss: 1.8699942827224731\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 296, Loss: 1.8662896156311035\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 297, Loss: 1.7915292978286743\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 298, Loss: 1.9487203359603882\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 299, Loss: 1.9432291984558105\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 300, Loss: 1.9557727575302124\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 301, Loss: 2.0083441734313965\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 302, Loss: 1.819691777229309\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 303, Loss: 1.869079351425171\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 304, Loss: 1.8110146522521973\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 305, Loss: 1.8622820377349854\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 306, Loss: 1.9622433185577393\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 307, Loss: 1.902584433555603\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 308, Loss: 1.883741855621338\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 309, Loss: 1.690102219581604\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 310, Loss: 1.8434324264526367\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 311, Loss: 1.8809559345245361\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 312, Loss: 1.8785933256149292\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 313, Loss: 1.8180162906646729\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 314, Loss: 1.995314121246338\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 315, Loss: 1.9758431911468506\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 316, Loss: 1.7614127397537231\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 317, Loss: 1.8175592422485352\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 318, Loss: 1.9320876598358154\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 319, Loss: 1.8604432344436646\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 320, Loss: 1.6188684701919556\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 321, Loss: 1.922826886177063\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 322, Loss: 1.9987467527389526\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 323, Loss: 2.0233941078186035\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 324, Loss: 1.9975147247314453\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 325, Loss: 1.984724998474121\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 326, Loss: 1.7843457460403442\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 327, Loss: 1.798470139503479\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 328, Loss: 1.9421645402908325\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 329, Loss: 1.833377480506897\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 330, Loss: 1.8737891912460327\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 331, Loss: 1.7394142150878906\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 332, Loss: 1.8392215967178345\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 333, Loss: 2.009791612625122\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 334, Loss: 1.899773120880127\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 335, Loss: 1.7360337972640991\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 336, Loss: 1.8213250637054443\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 337, Loss: 1.9560058116912842\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 338, Loss: 1.8788892030715942\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 339, Loss: 1.86631178855896\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 340, Loss: 1.9447609186172485\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 341, Loss: 1.8044284582138062\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 342, Loss: 1.9189229011535645\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 343, Loss: 1.8766168355941772\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 344, Loss: 1.9291586875915527\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 345, Loss: 1.9630717039108276\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 346, Loss: 1.8061907291412354\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 347, Loss: 1.9877774715423584\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 348, Loss: 1.8282314538955688\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 349, Loss: 1.8211207389831543\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 350, Loss: 1.9992635250091553\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 351, Loss: 1.8206250667572021\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 352, Loss: 1.8549306392669678\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 353, Loss: 1.817232608795166\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 354, Loss: 1.7037020921707153\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 355, Loss: 1.6986184120178223\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 356, Loss: 1.7112345695495605\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 357, Loss: 1.8094377517700195\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 358, Loss: 1.8721314668655396\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 359, Loss: 1.8166335821151733\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 360, Loss: 1.7595341205596924\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 361, Loss: 1.6516401767730713\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 362, Loss: 1.8163135051727295\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 363, Loss: 1.897315502166748\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 364, Loss: 1.8334269523620605\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 365, Loss: 1.778746247291565\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 366, Loss: 1.7752909660339355\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 367, Loss: 1.7375178337097168\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 368, Loss: 1.9417285919189453\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 369, Loss: 1.909150242805481\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 370, Loss: 1.757077693939209\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 371, Loss: 1.6875828504562378\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 372, Loss: 2.155348062515259\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 373, Loss: 2.019326686859131\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 374, Loss: 1.63456130027771\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 375, Loss: 1.977027416229248\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 376, Loss: 1.8259772062301636\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 377, Loss: 1.9762698411941528\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 378, Loss: 1.7268331050872803\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 379, Loss: 1.841557502746582\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 380, Loss: 1.8723883628845215\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 381, Loss: 1.9193618297576904\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 382, Loss: 1.8360869884490967\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 383, Loss: 1.7846029996871948\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 384, Loss: 1.7183294296264648\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 385, Loss: 1.9409019947052002\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 386, Loss: 1.8937969207763672\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 387, Loss: 1.7146587371826172\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 388, Loss: 1.6560728549957275\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 389, Loss: 1.9324148893356323\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 390, Loss: 1.6199586391448975\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3283 - loss: 1.8307\n","Test Loss: 1.8358044624328613\n","Test Accuracy: 0.3248000144958496\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import numpy as np\n","import csv\n","import threading\n","\n","# Define the model\n","def create_model():\n","    return models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),  # Input layer\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Worker 0 updates this\n","        layers.AveragePooling2D(pool_size=(2, 2)),                # Worker 0 skips this (no weights)\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Worker 1 updates this\n","        layers.AveragePooling2D(pool_size=(2, 2)),                # Worker 1 skips this (no weights)\n","        layers.Flatten(),                                         # Worker 1 skips this (no weights)\n","        layers.Dense(120, activation='relu'),                    # Worker 1 updates this\n","        layers.Dense(84, activation='relu'),                     # Worker 1 updates this\n","        layers.Dense(10, activation='softmax')                   # Worker 1 updates this\n","    ])\n","\n","# Dataset preparation\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Save weights to CSV\n","def save_weights_to_csv(worker_id, layer_weights, filename):\n","    with open(filename, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","        for layer, weights in layer_weights.items():\n","            writer.writerow([f\"Worker {worker_id}\", layer, *weights.flatten()[:10]])  # Save first 10 weights\n","\n","# Custom Worker Class\n","class Worker:\n","    def __init__(self, worker_id, model, layers_to_update, dataset, csv_file):\n","        self.worker_id = worker_id\n","        self.model = model\n","        self.layers_to_update = layers_to_update  # Indices of layers this worker updates\n","        self.dataset = dataset\n","        self.csv_file = csv_file\n","        self.optimizer = tf.keras.optimizers.Adam()\n","\n","    def compute_and_update(self, features, labels):\n","        with tf.GradientTape(persistent=True) as tape:\n","            predictions = self.model(features, training=True)\n","            loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n","            loss = tf.reduce_mean(loss)\n","\n","        # Compute gradients only for the assigned layers with trainable weights\n","        gradients = []\n","        variables = []\n","        layer_weights = {}\n","        for i in self.layers_to_update:\n","            if i < len(self.model.layers) and self.model.layers[i].trainable_weights:\n","                gradients.extend(tape.gradient(loss, self.model.layers[i].trainable_weights))\n","                variables.extend(self.model.layers[i].trainable_weights)\n","                # Log weights\n","                layer_weights[f\"Layer {i}\"] = self.model.layers[i].trainable_weights[0].numpy()\n","\n","        # Apply gradients if there are any\n","        if gradients and variables:\n","            self.optimizer.apply_gradients(zip(gradients, variables))\n","            save_weights_to_csv(self.worker_id, layer_weights, self.csv_file)\n","            print(f\"Worker {self.worker_id} updated layers: {self.layers_to_update}\")\n","        else:\n","            print(f\"Worker {self.worker_id} found no trainable variables to update.\")\n","\n","        del tape\n","        return loss\n","\n","    def run(self):\n","        for step, (features, labels) in enumerate(self.dataset):\n","            features = tf.convert_to_tensor(features)\n","            labels = tf.convert_to_tensor(labels)\n","            loss = self.compute_and_update(features, labels)\n","            print(f\"Worker {self.worker_id} - Step {step}, Loss: {loss.numpy()}\")\n","\n","# Create datasets\n","train_dataset, test_dataset = create_dataset()\n","\n","# Verify model structure\n","global_model = create_model()\n","global_model.summary()\n","\n","# Assign layers to workers based on the model summary\n","worker_0_layers = [0]  # Conv2D(6) for Worker 0\n","worker_1_layers = [2, 5, 6, 7]  # Conv2D(16) and Dense layers for Worker 1\n","\n","# Split dataset for workers\n","worker_0_dataset = train_dataset.shard(num_shards=2, index=0)\n","worker_1_dataset = train_dataset.shard(num_shards=2, index=1)\n","\n","# Create workers with CSV file names\n","worker_0 = Worker(worker_id=0, model=global_model, layers_to_update=worker_0_layers, dataset=worker_0_dataset, csv_file='worker_0_weights.csv')\n","worker_1 = Worker(worker_id=1, model=global_model, layers_to_update=worker_1_layers, dataset=worker_1_dataset, csv_file='worker_1_weights.csv')\n","\n","# Parallel worker execution using threading\n","def run_worker(worker):\n","    worker.run()\n","\n","# Create threads for workers\n","worker_0_thread = threading.Thread(target=run_worker, args=(worker_0,))\n","worker_1_thread = threading.Thread(target=run_worker, args=(worker_1,))\n","\n","# Start threads\n","worker_0_thread.start()\n","worker_1_thread.start()\n","\n","# Wait for both workers to finish\n","worker_0_thread.join()\n","worker_1_thread.join()\n","\n","# Evaluate the global model\n","global_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","test_loss, test_acc = global_model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_acc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DbUTTDUJvnYC","executionInfo":{"status":"ok","timestamp":1735447886108,"user_tz":-330,"elapsed":120974,"user":{"displayName":"Chandra Sekhar","userId":"00330720378866032186"}},"outputId":"4a5263c3-4664-4a3c-cf8c-818797207860"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_23\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_23\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)           │             \u001b[38;5;34m156\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_46                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │           \u001b[38;5;34m2,416\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_47                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_23 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_69 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)                 │          \u001b[38;5;34m48,120\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_70 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)                  │          \u001b[38;5;34m10,164\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m850\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_46                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_47                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Worker 0 updated layers: [0]\n","Worker 0 - Step 0, Loss: 2.3010637760162354\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 1, Loss: 2.3063416481018066\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 2, Loss: 2.305616855621338\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 0, Loss: 2.306950092315674\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 3, Loss: 2.301142692565918\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 4, Loss: 2.300665855407715\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 5, Loss: 2.2978196144104004\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 1, Loss: 2.2992303371429443\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 6, Loss: 2.304971694946289\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 7, Loss: 2.302351951599121\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 8, Loss: 2.3071470260620117\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 2, Loss: 2.3068220615386963\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 9, Loss: 2.3035683631896973\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 10, Loss: 2.298595905303955\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 11, Loss: 2.294771432876587\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 3, Loss: 2.3066720962524414\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 12, Loss: 2.296144723892212\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 13, Loss: 2.3035106658935547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 14, Loss: 2.302572250366211\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 4, Loss: 2.307743787765503\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 15, Loss: 2.301833152770996\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 16, Loss: 2.306882858276367\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 17, Loss: 2.2966742515563965\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 18, Loss: 2.2949342727661133\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 5, Loss: 2.305626630783081\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 19, Loss: 2.2953126430511475\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 20, Loss: 2.2974205017089844\n","Worker 1 updated layers: [2, 5, 6, 7]Worker 0 updated layers: [0]\n","Worker 1 - Step 6, Loss: 2.2928450107574463\n","\n","Worker 0 - Step 21, Loss: 2.303262948989868\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 22, Loss: 2.2976889610290527\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 23, Loss: 2.2923994064331055\n","Worker 1 updated layers: [2, 5, 6, 7]Worker 0 updated layers: [0]\n","Worker 1 - Step 7, Loss: 2.290194511413574\n","\n","Worker 0 - Step 24, Loss: 2.3029866218566895\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 25, Loss: 2.2996182441711426\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 26, Loss: 2.2967886924743652\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 27, Loss: 2.296387195587158\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 8, Loss: 2.3015809059143066\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 28, Loss: 2.3015918731689453\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 29, Loss: 2.299882650375366\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 9, Loss: 2.2874913215637207\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 30, Loss: 2.2987899780273438\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 31, Loss: 2.3014161586761475\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 32, Loss: 2.283663749694824\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 10, Loss: 2.2919647693634033\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 33, Loss: 2.289700984954834\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 34, Loss: 2.291381597518921\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 35, Loss: 2.2961621284484863\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 11, Loss: 2.2981181144714355\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 36, Loss: 2.296980381011963\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 37, Loss: 2.2829275131225586\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 38, Loss: 2.306877613067627\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 12, Loss: 2.287094831466675\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 39, Loss: 2.2794909477233887\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 40, Loss: 2.285858154296875\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 41, Loss: 2.29569935798645\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 13, Loss: 2.287614107131958\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 42, Loss: 2.2782933712005615\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 43, Loss: 2.2680399417877197\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 44, Loss: 2.272005081176758\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 14, Loss: 2.276926040649414\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 45, Loss: 2.298698902130127\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 46, Loss: 2.279597043991089\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 47, Loss: 2.312649965286255\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 15, Loss: 2.310361862182617\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 48, Loss: 2.26922607421875\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 49, Loss: 2.2735583782196045\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 50, Loss: 2.2657904624938965\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 16, Loss: 2.29331111907959\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 51, Loss: 2.263667106628418\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 52, Loss: 2.2758007049560547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 53, Loss: 2.274984836578369\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 17, Loss: 2.3113317489624023\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 54, Loss: 2.294381856918335\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 55, Loss: 2.2751309871673584\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 56, Loss: 2.2266077995300293\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 57, Loss: 2.2534525394439697\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 18, Loss: 2.2600598335266113\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 58, Loss: 2.2715530395507812\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 59, Loss: 2.284482479095459\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 60, Loss: 2.2727856636047363\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 19, Loss: 2.2322473526000977\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 61, Loss: 2.22861909866333\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 62, Loss: 2.275629997253418\n","Worker 0 updated layers: [0]\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 0 - Step 63, Loss: 2.2384095191955566\n","Worker 1 - Step 20, Loss: 2.23311185836792\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 64, Loss: 2.2378108501434326\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 65, Loss: 2.2591300010681152\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 21, Loss: 2.295440196990967\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 66, Loss: 2.2729713916778564\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 67, Loss: 2.2965574264526367\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 68, Loss: 2.2178850173950195\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 69, Loss: 2.270981788635254\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 22, Loss: 2.24627685546875\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 70, Loss: 2.285642623901367\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 71, Loss: 2.2501487731933594\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 72, Loss: 2.234981060028076\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 23, Loss: 2.2934939861297607\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 73, Loss: 2.27778959274292\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 74, Loss: 2.270897388458252\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 75, Loss: 2.25296950340271\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 24, Loss: 2.217249870300293\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 76, Loss: 2.2544710636138916\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 77, Loss: 2.2219135761260986\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 25, Loss: 2.276771306991577\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 78, Loss: 2.245293617248535\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 79, Loss: 2.2747793197631836\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 80, Loss: 2.2656657695770264\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 26, Loss: 2.2612035274505615\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 81, Loss: 2.2617883682250977\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 82, Loss: 2.236813545227051\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 83, Loss: 2.223428964614868\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 84, Loss: 2.2028415203094482\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 27, Loss: 2.2236056327819824\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 85, Loss: 2.2033023834228516\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 86, Loss: 2.2267818450927734\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 87, Loss: 2.2159299850463867\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 28, Loss: 2.268589973449707\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 88, Loss: 2.183835983276367\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 89, Loss: 2.214184045791626\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 90, Loss: 2.259542465209961\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 29, Loss: 2.140842914581299\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 91, Loss: 2.2678351402282715\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 92, Loss: 2.2459630966186523\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 93, Loss: 2.2755062580108643\n","Worker 0 updated layers: [0]Worker 1 updated layers: [2, 5, 6, 7]\n","\n","Worker 0 - Step 94, Loss: 2.2580502033233643\n","Worker 1 - Step 30, Loss: 2.2095236778259277\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 95, Loss: 2.1922926902770996\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 96, Loss: 2.1729507446289062\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 97, Loss: 2.261154890060425\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 31, Loss: 2.137242317199707\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 98, Loss: 2.2884647846221924\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 99, Loss: 2.1504251956939697\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 100, Loss: 2.2669763565063477\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 32, Loss: 2.2186710834503174\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 101, Loss: 2.1895008087158203\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 102, Loss: 2.247943878173828\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 103, Loss: 2.282539129257202\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 33, Loss: 2.2238965034484863\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 104, Loss: 2.177424192428589\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 105, Loss: 2.1865129470825195\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 106, Loss: 2.2176899909973145\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 34, Loss: 2.2529635429382324\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 107, Loss: 2.195521831512451\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 108, Loss: 2.232811212539673\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 109, Loss: 2.1728742122650146\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 35, Loss: 2.1727564334869385\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 110, Loss: 2.2484476566314697\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 111, Loss: 2.1596240997314453\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 112, Loss: 2.2524070739746094\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 36, Loss: 2.1235382556915283\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 113, Loss: 2.171316623687744\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 114, Loss: 2.2110490798950195\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 115, Loss: 2.2156643867492676\n","Worker 0 updated layers: [0]Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 37, Loss: 2.303758382797241\n","Worker 0 - Step 116, Loss: 2.1668150424957275\n","\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 117, Loss: 2.204054355621338\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 118, Loss: 2.1093966960906982\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 119, Loss: 2.1781370639801025\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 38, Loss: 2.1221365928649902\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 120, Loss: 2.268291473388672\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 121, Loss: 2.2146239280700684\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 39, Loss: 2.211662769317627\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 122, Loss: 2.172226905822754\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 123, Loss: 2.1402676105499268\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 124, Loss: 2.187657594680786\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 40, Loss: 2.091369867324829\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 125, Loss: 2.165252208709717\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 126, Loss: 2.128117084503174\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 41, Loss: 2.1405892372131348\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 127, Loss: 2.1005606651306152\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 128, Loss: 2.189459800720215\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 129, Loss: 2.263585329055786\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 42, Loss: 2.206108570098877\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 130, Loss: 2.1943163871765137\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 131, Loss: 2.2017953395843506\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 132, Loss: 2.1757943630218506\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 43, Loss: 2.20124888420105\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 133, Loss: 2.0995278358459473\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 134, Loss: 2.1811015605926514\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 135, Loss: 2.1396007537841797\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 44, Loss: 2.0967440605163574\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 136, Loss: 2.2017602920532227\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 137, Loss: 2.1780738830566406\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 138, Loss: 2.117739677429199\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 45, Loss: 2.1787266731262207\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 139, Loss: 2.133493185043335\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 140, Loss: 2.059938907623291\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 141, Loss: 2.184171199798584\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 46, Loss: 2.1642065048217773\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 142, Loss: 2.115786075592041\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 143, Loss: 2.1072728633880615\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 144, Loss: 2.161728620529175\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 47, Loss: 2.08964204788208\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 145, Loss: 2.1401023864746094\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 146, Loss: 2.232666492462158\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 147, Loss: 2.1249518394470215\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 148, Loss: 2.0993010997772217\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 48, Loss: 2.142444372177124\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 149, Loss: 2.025186061859131\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 150, Loss: 2.080737590789795\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 49, Loss: 2.057673454284668\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 151, Loss: 2.1291098594665527\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 152, Loss: 2.102616310119629\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 153, Loss: 1.9925388097763062\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 50, Loss: 2.0063087940216064\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 154, Loss: 2.138054370880127\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 155, Loss: 2.214622735977173\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 156, Loss: 2.093404769897461\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 51, Loss: 2.160064935684204\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 157, Loss: 2.197270393371582\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 158, Loss: 2.103724241256714\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 159, Loss: 1.9998934268951416\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 52, Loss: 2.226222515106201\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 160, Loss: 2.0960638523101807\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 161, Loss: 2.1665539741516113\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 162, Loss: 2.2422475814819336\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 53, Loss: 2.088604688644409\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 163, Loss: 2.162343978881836\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 164, Loss: 2.1497304439544678\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 165, Loss: 2.09541392326355\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 54, Loss: 2.1838021278381348\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 166, Loss: 2.15529203414917\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 167, Loss: 2.1308529376983643\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 168, Loss: 2.096320152282715\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 55, Loss: 2.1703500747680664\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 169, Loss: 2.05940318107605\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 170, Loss: 1.9303854703903198\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 171, Loss: 2.0826637744903564\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 56, Loss: 2.073042869567871\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 172, Loss: 2.1313252449035645\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 173, Loss: 2.1431467533111572\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 174, Loss: 2.0924243927001953\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 57, Loss: 2.128502368927002\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 175, Loss: 2.0765180587768555\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 176, Loss: 2.174206256866455\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 177, Loss: 2.1280057430267334\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 178, Loss: 2.109665870666504\n","Worker 1 - Step 58, Loss: 2.0762202739715576\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 179, Loss: 2.069584369659424\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 180, Loss: 2.1248505115509033\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 59, Loss: 2.054593563079834\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 181, Loss: 2.1414594650268555\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 182, Loss: 2.1784257888793945\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 183, Loss: 2.0655298233032227\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 60, Loss: 2.2190840244293213\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 184, Loss: 2.0759775638580322\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 185, Loss: 1.9900703430175781\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 186, Loss: 2.1771817207336426\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 61, Loss: 2.0449459552764893\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 187, Loss: 2.216798782348633\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 188, Loss: 2.0853466987609863\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 189, Loss: 2.1344549655914307\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 62, Loss: 2.0949645042419434\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 190, Loss: 2.1393375396728516\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 191, Loss: 2.069202423095703\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 192, Loss: 1.9666788578033447\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 63, Loss: 2.0979504585266113\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 193, Loss: 2.120145559310913\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 194, Loss: 2.1409833431243896\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 195, Loss: 1.9663832187652588\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 196, Loss: 2.1526002883911133\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 64, Loss: 2.228325366973877\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 197, Loss: 2.1728224754333496\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 198, Loss: 2.066654682159424\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 199, Loss: 2.1812753677368164\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 65, Loss: 2.078748941421509\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 200, Loss: 2.0638961791992188\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 201, Loss: 2.0128655433654785\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 202, Loss: 2.0921621322631836\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 66, Loss: 2.1389176845550537\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 203, Loss: 2.166409969329834\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 204, Loss: 2.353304862976074\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 205, Loss: 1.975127935409546\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 67, Loss: 2.0603184700012207\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 206, Loss: 2.0962071418762207\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 207, Loss: 2.048074722290039\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 208, Loss: 2.0573182106018066\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 68, Loss: 2.1333260536193848\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 209, Loss: 2.024620294570923\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 210, Loss: 2.115849494934082\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 211, Loss: 1.9896297454833984\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 212, Loss: 2.1825575828552246\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 69, Loss: 2.2267656326293945\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 213, Loss: 2.0773704051971436\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 214, Loss: 2.112882137298584\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 215, Loss: 2.118227005004883\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 70, Loss: 2.0694074630737305\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 216, Loss: 2.077080488204956\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 217, Loss: 2.1022796630859375\n","Worker 1 updated layers: [2, 5, 6, 7]Worker 0 updated layers: [0]\n","Worker 0 - Step 218, Loss: 2.0623507499694824\n","Worker 1 - Step 71, Loss: 1.9548540115356445\n","\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 219, Loss: 2.0709939002990723\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 220, Loss: 2.0940563678741455\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 221, Loss: 2.0499086380004883\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 72, Loss: 1.9381484985351562\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 222, Loss: 2.0740256309509277\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 223, Loss: 2.101065158843994\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 224, Loss: 2.0437870025634766\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 73, Loss: 1.9565558433532715\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 225, Loss: 2.19214129447937\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 226, Loss: 2.2359464168548584\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 227, Loss: 2.1510534286499023\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 74, Loss: 2.0594968795776367\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 228, Loss: 2.0340795516967773\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 229, Loss: 2.0537896156311035\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 230, Loss: 1.9978140592575073\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 75, Loss: 2.025778293609619\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 231, Loss: 2.0590460300445557\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 232, Loss: 2.1241159439086914\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 233, Loss: 2.215053081512451\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 76, Loss: 2.216458320617676\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 234, Loss: 2.044617176055908\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 235, Loss: 2.0716238021850586\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 236, Loss: 2.0689537525177\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 77, Loss: 2.1576337814331055\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 237, Loss: 2.0526204109191895\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 238, Loss: 1.996213436126709\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 239, Loss: 2.2652218341827393\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 78, Loss: 2.35788631439209\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 240, Loss: 2.1399190425872803\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 241, Loss: 2.1490793228149414\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 242, Loss: 2.077521324157715\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 79, Loss: 2.1164004802703857\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 243, Loss: 2.165095567703247\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 244, Loss: 2.0533270835876465\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 245, Loss: 2.1026108264923096\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 246, Loss: 2.0682435035705566\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 80, Loss: 2.035207748413086\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 247, Loss: 2.0082616806030273\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 248, Loss: 2.1210129261016846\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 249, Loss: 2.220876932144165\n","Worker 1 - Step 81, Loss: 1.9290602207183838\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 250, Loss: 1.997981071472168\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 251, Loss: 1.972285509109497\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 82, Loss: 2.0571722984313965\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 252, Loss: 1.948974847793579\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 253, Loss: 2.116811513900757\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 254, Loss: 2.0876152515411377\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 83, Loss: 1.98354971408844\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 255, Loss: 2.0304012298583984\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 256, Loss: 2.1174135208129883\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 257, Loss: 2.1101179122924805\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 84, Loss: 2.1120314598083496\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 258, Loss: 2.10007643699646\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 259, Loss: 2.0942087173461914\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 260, Loss: 2.1039669513702393\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 85, Loss: 2.1668319702148438\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 261, Loss: 2.081641674041748\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 262, Loss: 2.0340702533721924\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 263, Loss: 2.1170430183410645\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 86, Loss: 2.0598692893981934\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 264, Loss: 2.193010091781616\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 265, Loss: 2.027973175048828\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 266, Loss: 2.1415066719055176\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 87, Loss: 2.0556492805480957\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 267, Loss: 2.204253911972046\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 268, Loss: 2.100328207015991\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 269, Loss: 1.964914083480835\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 88, Loss: 1.948394775390625\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 270, Loss: 1.9557435512542725\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 271, Loss: 2.101417064666748\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 272, Loss: 1.9466999769210815\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 273, Loss: 1.9917576313018799\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 89, Loss: 2.0946755409240723\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 274, Loss: 2.046719551086426\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 275, Loss: 2.1059811115264893\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 276, Loss: 2.036294460296631\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 90, Loss: 1.9133470058441162\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 277, Loss: 2.0794529914855957\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 278, Loss: 2.035513162612915\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 279, Loss: 1.9862879514694214\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 91, Loss: 2.235863208770752\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 280, Loss: 2.0753135681152344\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 281, Loss: 2.004706382751465\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 282, Loss: 2.1199753284454346\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 92, Loss: 1.9584026336669922\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 283, Loss: 2.1042354106903076\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 284, Loss: 2.0356035232543945\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 285, Loss: 2.1601109504699707\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 93, Loss: 2.0391268730163574\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 286, Loss: 2.1201794147491455\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 287, Loss: 2.076784372329712\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 288, Loss: 2.078601121902466\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 94, Loss: 1.9815176725387573\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 289, Loss: 1.9717707633972168\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 290, Loss: 1.9339542388916016\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 291, Loss: 2.0081963539123535\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 292, Loss: 1.8805673122406006\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 95, Loss: 1.959209680557251\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 293, Loss: 2.1144943237304688\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 294, Loss: 2.085965156555176\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 295, Loss: 1.9765205383300781\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 96, Loss: 2.0073366165161133\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 296, Loss: 2.165990114212036\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 297, Loss: 1.9743931293487549\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 97, Loss: 2.013845443725586\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 298, Loss: 1.9737228155136108\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 299, Loss: 2.189138412475586\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 300, Loss: 2.139526844024658\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 98, Loss: 2.1576180458068848\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 301, Loss: 1.9648001194000244\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 302, Loss: 2.0360398292541504\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 303, Loss: 2.080254316329956\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 99, Loss: 2.159855842590332\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 304, Loss: 2.080451726913452\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 305, Loss: 2.1662306785583496\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 306, Loss: 2.07936429977417\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 100, Loss: 2.164947986602783\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 307, Loss: 1.9686479568481445\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 308, Loss: 2.1023499965667725\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 309, Loss: 1.964871883392334\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 101, Loss: 1.992255687713623\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 310, Loss: 1.9598360061645508\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 311, Loss: 2.0094897747039795\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 312, Loss: 2.068110466003418\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 102, Loss: 2.045896291732788\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 313, Loss: 2.034693717956543\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 314, Loss: 1.958583116531372\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 315, Loss: 2.087124824523926\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 103, Loss: 2.091887950897217\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 316, Loss: 2.0644421577453613\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 317, Loss: 1.9150818586349487\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 318, Loss: 1.9626951217651367\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 104, Loss: 2.015866756439209\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 319, Loss: 2.17348575592041\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 320, Loss: 2.0496339797973633\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 321, Loss: 2.1288986206054688\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 105, Loss: 2.0344154834747314\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 322, Loss: 2.049133777618408\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 323, Loss: 2.042015790939331\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 324, Loss: 1.8581054210662842\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 106, Loss: 1.95475435256958\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 325, Loss: 2.116424560546875\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 326, Loss: 2.149134397506714\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 327, Loss: 1.9022841453552246\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 107, Loss: 2.002382755279541\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 328, Loss: 1.9381616115570068\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 329, Loss: 2.0020971298217773\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 330, Loss: 2.020847797393799\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 108, Loss: 1.9689829349517822\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 331, Loss: 1.9751157760620117\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 332, Loss: 1.9807255268096924\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 333, Loss: 1.9377357959747314\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 109, Loss: 1.9964268207550049\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 334, Loss: 1.923567771911621\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 335, Loss: 2.1186532974243164\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 336, Loss: 2.063715934753418\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 110, Loss: 1.9802522659301758\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 337, Loss: 2.0314040184020996\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 338, Loss: 1.9627833366394043\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 339, Loss: 1.8545421361923218\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 111, Loss: 1.969355583190918\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 340, Loss: 2.0571463108062744\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 341, Loss: 2.0239710807800293\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 342, Loss: 2.0243496894836426\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 112, Loss: 2.1066031455993652\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 343, Loss: 2.0077788829803467\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 344, Loss: 1.9928433895111084\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 345, Loss: 2.029320240020752\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 113, Loss: 1.9403413534164429\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 346, Loss: 2.0120420455932617\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 347, Loss: 1.9952785968780518\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 348, Loss: 2.1051137447357178\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 114, Loss: 1.9366904497146606\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 349, Loss: 2.0611319541931152\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 350, Loss: 1.9908679723739624\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 351, Loss: 2.020768880844116\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 115, Loss: 1.9182072877883911\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 352, Loss: 2.0910162925720215\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 353, Loss: 2.1317858695983887\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 354, Loss: 1.9280571937561035\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 355, Loss: 1.9716739654541016\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 116, Loss: 1.995760440826416\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 356, Loss: 1.9009990692138672\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 357, Loss: 1.9608509540557861\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 358, Loss: 2.0647835731506348\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 117, Loss: 1.9522936344146729\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 359, Loss: 2.200119733810425\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 360, Loss: 1.926721215248108\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 361, Loss: 2.042391300201416\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 118, Loss: 2.0740737915039062\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 362, Loss: 2.0224030017852783\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 363, Loss: 1.9693094491958618\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 364, Loss: 2.092988967895508\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 365, Loss: 2.050377607345581\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 119, Loss: 1.9951400756835938\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 366, Loss: 2.1886849403381348\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 367, Loss: 2.0468125343322754\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 120, Loss: 2.1089515686035156\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 368, Loss: 2.0228352546691895\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 369, Loss: 2.0871567726135254\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 370, Loss: 2.236896514892578\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 121, Loss: 2.05275297164917\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 371, Loss: 2.055729866027832\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 372, Loss: 1.9324909448623657\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 373, Loss: 2.112799644470215\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 122, Loss: 1.8059484958648682\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 374, Loss: 2.0234463214874268\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 375, Loss: 2.281386375427246\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 376, Loss: 1.9918025732040405\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 123, Loss: 1.890416145324707\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 377, Loss: 1.8625906705856323\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 378, Loss: 2.0134458541870117\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 379, Loss: 1.9495145082473755\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 124, Loss: 1.9019421339035034\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 380, Loss: 2.062559127807617\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 381, Loss: 1.9835693836212158\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 382, Loss: 1.9995001554489136\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 125, Loss: 1.904638648033142\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 383, Loss: 2.2060585021972656\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 384, Loss: 1.870295524597168\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 385, Loss: 1.9546829462051392\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 126, Loss: 2.1307177543640137\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 386, Loss: 2.001486301422119\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 387, Loss: 2.2486824989318848\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 388, Loss: 1.9410860538482666\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 127, Loss: 2.0553555488586426\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 389, Loss: 1.9067847728729248\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 390, Loss: 2.082718849182129\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 128, Loss: 1.9818544387817383\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 129, Loss: 1.8618532419204712\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 130, Loss: 2.0233516693115234\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 131, Loss: 1.9970484972000122\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 132, Loss: 2.072756767272949\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 133, Loss: 1.8781132698059082\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 134, Loss: 1.7862759828567505\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 135, Loss: 2.045259475708008\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 136, Loss: 1.8385288715362549\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 137, Loss: 1.9261935949325562\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 138, Loss: 1.911023736000061\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 139, Loss: 2.1314001083374023\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 140, Loss: 1.9648704528808594\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 141, Loss: 1.8441661596298218\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 142, Loss: 1.7891931533813477\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 143, Loss: 1.9678328037261963\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 144, Loss: 1.8047080039978027\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 145, Loss: 1.9271495342254639\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 146, Loss: 2.0276846885681152\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 147, Loss: 1.8960273265838623\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 148, Loss: 1.931567907333374\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 149, Loss: 1.8894654512405396\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 150, Loss: 2.0034332275390625\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 151, Loss: 1.9237236976623535\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 152, Loss: 2.087188243865967\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 153, Loss: 2.0921130180358887\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 154, Loss: 1.9366648197174072\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 155, Loss: 1.8564739227294922\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 156, Loss: 2.000530481338501\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 157, Loss: 2.004241466522217\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 158, Loss: 1.8548239469528198\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 159, Loss: 1.9105045795440674\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 160, Loss: 2.0508203506469727\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 161, Loss: 1.8818293809890747\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 162, Loss: 1.7055082321166992\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 163, Loss: 1.829838752746582\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 164, Loss: 1.7414469718933105\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 165, Loss: 1.9755817651748657\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 166, Loss: 2.0470454692840576\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 167, Loss: 1.928847074508667\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 168, Loss: 1.9643522500991821\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 169, Loss: 2.073057174682617\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 170, Loss: 1.8747457265853882\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 171, Loss: 1.9735714197158813\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 172, Loss: 1.9717395305633545\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 173, Loss: 2.0338590145111084\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 174, Loss: 1.8084297180175781\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 175, Loss: 1.9495751857757568\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 176, Loss: 1.8924660682678223\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 177, Loss: 1.9718961715698242\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 178, Loss: 1.86589515209198\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 179, Loss: 1.8396308422088623\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 180, Loss: 1.9548101425170898\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 181, Loss: 1.9251151084899902\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 182, Loss: 1.9879517555236816\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 183, Loss: 1.913527011871338\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 184, Loss: 1.89531409740448\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 185, Loss: 1.8345706462860107\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 186, Loss: 2.014115333557129\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 187, Loss: 1.9516406059265137\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 188, Loss: 2.0131499767303467\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 189, Loss: 2.0271434783935547\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 190, Loss: 1.9134347438812256\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 191, Loss: 1.953949213027954\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 192, Loss: 1.807523488998413\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 193, Loss: 1.8300526142120361\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 194, Loss: 1.8952453136444092\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 195, Loss: 1.8299267292022705\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 196, Loss: 1.8032010793685913\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 197, Loss: 1.9182902574539185\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 198, Loss: 1.9329012632369995\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 199, Loss: 1.837177038192749\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 200, Loss: 1.90633225440979\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 201, Loss: 2.0844240188598633\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 202, Loss: 1.9301271438598633\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 203, Loss: 1.5689067840576172\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 204, Loss: 1.9627101421356201\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 205, Loss: 1.8696043491363525\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 206, Loss: 1.7016417980194092\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 207, Loss: 1.7782665491104126\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 208, Loss: 2.0031909942626953\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 209, Loss: 1.9799813032150269\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 210, Loss: 1.9774837493896484\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 211, Loss: 1.8273098468780518\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 212, Loss: 1.7597218751907349\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 213, Loss: 1.7805376052856445\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 214, Loss: 2.010450601577759\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 215, Loss: 1.936689853668213\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 216, Loss: 1.7837127447128296\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 217, Loss: 1.8253819942474365\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 218, Loss: 1.922309398651123\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 219, Loss: 2.012482166290283\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 220, Loss: 1.9116270542144775\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 221, Loss: 1.7631151676177979\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 222, Loss: 1.930166244506836\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 223, Loss: 1.9354043006896973\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 224, Loss: 1.9824094772338867\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 225, Loss: 1.8498700857162476\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 226, Loss: 1.98119056224823\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 227, Loss: 1.9728261232376099\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 228, Loss: 1.8389036655426025\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 229, Loss: 1.9627412557601929\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 230, Loss: 1.9202038049697876\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 231, Loss: 1.9726366996765137\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 232, Loss: 1.7817943096160889\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 233, Loss: 1.7232054471969604\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 234, Loss: 2.0645947456359863\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 235, Loss: 1.9773085117340088\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 236, Loss: 1.7960379123687744\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 237, Loss: 1.7408387660980225\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 238, Loss: 1.72760009765625\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 239, Loss: 1.8974885940551758\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 240, Loss: 1.8969159126281738\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 241, Loss: 1.9230612516403198\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 242, Loss: 1.809702754020691\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 243, Loss: 1.7491629123687744\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 244, Loss: 2.0351366996765137\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 245, Loss: 1.9725899696350098\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 246, Loss: 1.8898098468780518\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 247, Loss: 1.8291810750961304\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 248, Loss: 2.0089757442474365\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 249, Loss: 1.7222195863723755\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 250, Loss: 1.933873176574707\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 251, Loss: 1.8446141481399536\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 252, Loss: 1.8392380475997925\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 253, Loss: 1.8426076173782349\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 254, Loss: 1.8004560470581055\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 255, Loss: 1.6934828758239746\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 256, Loss: 1.9453065395355225\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 257, Loss: 1.8260022401809692\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 258, Loss: 1.8033447265625\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 259, Loss: 1.9100875854492188\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 260, Loss: 1.8229272365570068\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 261, Loss: 1.6971611976623535\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 262, Loss: 1.8114306926727295\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 263, Loss: 1.7147178649902344\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 264, Loss: 1.9425849914550781\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 265, Loss: 1.819530725479126\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 266, Loss: 1.7503427267074585\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 267, Loss: 1.9197615385055542\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 268, Loss: 1.8756053447723389\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 269, Loss: 1.7586545944213867\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 270, Loss: 1.7576310634613037\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 271, Loss: 1.8190807104110718\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 272, Loss: 1.8320406675338745\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 273, Loss: 1.7947337627410889\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 274, Loss: 1.7154109477996826\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 275, Loss: 1.9771966934204102\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 276, Loss: 1.7273750305175781\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 277, Loss: 1.7991480827331543\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 278, Loss: 1.8583585023880005\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 279, Loss: 1.8958412408828735\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 280, Loss: 1.8193947076797485\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 281, Loss: 1.8974847793579102\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 282, Loss: 1.7319016456604004\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 283, Loss: 1.7316656112670898\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 284, Loss: 1.820367693901062\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 285, Loss: 1.9324053525924683\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 286, Loss: 1.7130047082901\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 287, Loss: 1.7336490154266357\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 288, Loss: 1.6615365743637085\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 289, Loss: 1.8385887145996094\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 290, Loss: 1.792680025100708\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 291, Loss: 1.844853401184082\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 292, Loss: 1.820683240890503\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 293, Loss: 1.8054518699645996\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 294, Loss: 1.9445457458496094\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 295, Loss: 1.9175238609313965\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 296, Loss: 1.7127578258514404\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 297, Loss: 1.8094266653060913\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 298, Loss: 2.057464122772217\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 299, Loss: 1.6770222187042236\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 300, Loss: 1.8778059482574463\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 301, Loss: 1.7755463123321533\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 302, Loss: 1.7893856763839722\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 303, Loss: 1.713441014289856\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 304, Loss: 2.050107479095459\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 305, Loss: 1.8397444486618042\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 306, Loss: 2.0007448196411133\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 307, Loss: 1.6650097370147705\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 308, Loss: 1.9887937307357788\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 309, Loss: 1.6667355298995972\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 310, Loss: 1.6589508056640625\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 311, Loss: 1.8412749767303467\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 312, Loss: 1.8537558317184448\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 313, Loss: 1.7336223125457764\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 314, Loss: 1.8043733835220337\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 315, Loss: 1.7361280918121338\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 316, Loss: 1.9030609130859375\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 317, Loss: 1.8279929161071777\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 318, Loss: 1.6153913736343384\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 319, Loss: 1.882386326789856\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 320, Loss: 1.819963812828064\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 321, Loss: 1.724703311920166\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 322, Loss: 1.8265435695648193\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 323, Loss: 1.7980670928955078\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 324, Loss: 1.7406494617462158\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 325, Loss: 1.9607113599777222\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 326, Loss: 1.7943527698516846\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 327, Loss: 1.7321608066558838\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 328, Loss: 1.8145338296890259\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 329, Loss: 1.926032543182373\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 330, Loss: 1.8192639350891113\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 331, Loss: 1.704188585281372\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 332, Loss: 1.7346618175506592\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 333, Loss: 1.7264395952224731\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 334, Loss: 1.582045316696167\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 335, Loss: 1.8375627994537354\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 336, Loss: 1.6381003856658936\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 337, Loss: 1.8009259700775146\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 338, Loss: 1.7658467292785645\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 339, Loss: 1.8098266124725342\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 340, Loss: 1.8232934474945068\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 341, Loss: 1.8128008842468262\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 342, Loss: 1.6555466651916504\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 343, Loss: 1.7876205444335938\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 344, Loss: 1.6128668785095215\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 345, Loss: 1.472009301185608\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 346, Loss: 1.805248737335205\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 347, Loss: 1.6386384963989258\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 348, Loss: 1.8622912168502808\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 349, Loss: 1.6570923328399658\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 350, Loss: 1.7649807929992676\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 351, Loss: 1.748590350151062\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 352, Loss: 1.7725756168365479\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 353, Loss: 1.6020898818969727\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 354, Loss: 1.5969125032424927\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 355, Loss: 1.7755892276763916\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 356, Loss: 1.7898974418640137\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 357, Loss: 1.77895188331604\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 358, Loss: 1.7132277488708496\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 359, Loss: 1.748800277709961\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 360, Loss: 1.9572094678878784\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 361, Loss: 1.9100711345672607\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 362, Loss: 1.5136802196502686\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 363, Loss: 1.6948792934417725\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 364, Loss: 1.5978158712387085\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 365, Loss: 1.78635835647583\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 366, Loss: 1.723116159439087\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 367, Loss: 1.707468867301941\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 368, Loss: 1.7041946649551392\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 369, Loss: 1.8712222576141357\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 370, Loss: 1.7937185764312744\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 371, Loss: 1.8283123970031738\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 372, Loss: 1.8023545742034912\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 373, Loss: 1.635006070137024\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 374, Loss: 1.5838481187820435\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 375, Loss: 1.7120094299316406\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 376, Loss: 1.7462663650512695\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 377, Loss: 1.628252387046814\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 378, Loss: 1.76934814453125\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 379, Loss: 1.6555218696594238\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 380, Loss: 1.8002095222473145\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 381, Loss: 1.6519348621368408\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 382, Loss: 1.6713840961456299\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 383, Loss: 1.6943206787109375\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 384, Loss: 1.877864122390747\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 385, Loss: 1.617729902267456\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 386, Loss: 1.6108415126800537\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 387, Loss: 1.9236764907836914\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 388, Loss: 1.7346789836883545\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 389, Loss: 1.8340182304382324\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 390, Loss: 1.4382257461547852\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.3600 - loss: 1.7559\n","Test Loss: 1.7625141143798828\n","Test Accuracy: 0.3553999960422516\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import numpy as np\n","import csv\n","import threading\n","\n","# Define the model\n","def create_model():\n","    return models.Sequential([\n","        layers.Input(shape=(32, 32, 1)),  # Input layer for grayscale images\n","        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Worker 0 updates this\n","        layers.AveragePooling2D(pool_size=(2, 2)),                # No trainable parameters\n","        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Worker 1 updates this\n","        layers.AveragePooling2D(pool_size=(2, 2)),                # No trainable parameters\n","        layers.Flatten(),                                         # No trainable parameters\n","        layers.Dense(120, activation='relu'),                    # Worker 1 updates this\n","        layers.Dense(84, activation='relu'),                     # Worker 1 updates this\n","        layers.Dense(10, activation='softmax')                   # Worker 1 updates this\n","    ])\n","\n","# Dataset preparation\n","def create_dataset():\n","    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","    # Resize images and convert to grayscale\n","    x_train = tf.image.resize(x_train, (32, 32))\n","    x_test = tf.image.resize(x_test, (32, 32))\n","    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0  # Normalize\n","    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0    # Normalize\n","\n","    # Create TensorFlow datasets\n","    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n","    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n","    return train_dataset, test_dataset\n","\n","# Assign random subsets to workers\n","def create_random_subsets(dataset, num_workers):\n","    \"\"\"\n","    Split the dataset into random subsets for each worker.\n","    - Each worker gets approximately 1/num_workers of the dataset.\n","    \"\"\"\n","    shuffled_dataset = dataset.shuffle(buffer_size=5000)\n","    subsets = []\n","    for i in range(num_workers):\n","        subsets.append(shuffled_dataset.shard(num_shards=num_workers, index=i))\n","    return subsets\n","\n","# Save weights to CSV\n","def save_weights_to_csv(worker_id, layer_weights, filename):\n","    with open(filename, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","        for layer, weights in layer_weights.items():\n","            writer.writerow([f\"Worker {worker_id}\", layer, *weights.flatten()[:10]])  # Save first 10 weights\n","\n","# Custom Worker Class\n","class Worker:\n","    \"\"\"\n","    Represents a worker that:\n","    - Processes a random subset of the dataset.\n","    - Updates assigned layers of the model.\n","    - Logs updated weights to a CSV file.\n","    \"\"\"\n","    def __init__(self, worker_id, model, layers_to_update, dataset, csv_file):\n","        self.worker_id = worker_id\n","        self.model = model\n","        self.layers_to_update = layers_to_update  # Indices of layers this worker updates\n","        self.dataset = dataset\n","        self.csv_file = csv_file\n","        self.optimizer = tf.keras.optimizers.Adam()  # Optimizer for this worker\n","\n","    def compute_and_update(self, features, labels):\n","        \"\"\"\n","        Compute gradients for assigned layers and update their weights.\n","        \"\"\"\n","        with tf.GradientTape(persistent=True) as tape:\n","            predictions = self.model(features, training=True)\n","            loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n","            loss = tf.reduce_mean(loss)\n","\n","        # Compute gradients for the assigned layers only\n","        gradients = []\n","        variables = []\n","        layer_weights = {}\n","        for i in self.layers_to_update:\n","            if i < len(self.model.layers) and self.model.layers[i].trainable_weights:\n","                gradients.extend(tape.gradient(loss, self.model.layers[i].trainable_weights))\n","                variables.extend(self.model.layers[i].trainable_weights)\n","                # Save current weights for logging\n","                layer_weights[f\"Layer {i}\"] = self.model.layers[i].trainable_weights[0].numpy()\n","\n","        # Apply gradients and log updated weights\n","        if gradients and variables:\n","            self.optimizer.apply_gradients(zip(gradients, variables))\n","            save_weights_to_csv(self.worker_id, layer_weights, self.csv_file)\n","            print(f\"Worker {self.worker_id} updated layers: {self.layers_to_update}\")\n","        else:\n","            print(f\"Worker {self.worker_id} found no trainable variables to update.\")\n","\n","        del tape\n","        return loss\n","\n","    def run(self):\n","        \"\"\"\n","        Process the dataset shard assigned to this worker.\n","        \"\"\"\n","        for step, (features, labels) in enumerate(self.dataset):\n","            features = tf.convert_to_tensor(features)\n","            labels = tf.convert_to_tensor(labels)\n","            loss = self.compute_and_update(features, labels)\n","            print(f\"Worker {self.worker_id} - Step {step}, Loss: {loss.numpy()}\")\n","\n","# Create datasets\n","train_dataset, test_dataset = create_dataset()\n","\n","# Verify model structure\n","global_model = create_model()\n","global_model.summary()\n","\n","# Assign layers to workers based on the model summary\n","worker_0_layers = [0]  # Conv2D(6) for Worker 0\n","worker_1_layers = [2, 5, 6, 7]  # Conv2D(16) and Dense layers for Worker 1\n","\n","# Create random subsets for workers\n","num_workers = 2\n","worker_datasets = create_random_subsets(train_dataset, num_workers)\n","\n","# Create workers with random datasets\n","worker_0 = Worker(worker_id=0, model=global_model, layers_to_update=worker_0_layers, dataset=worker_datasets[0], csv_file='worker_0_weights.csv')\n","worker_1 = Worker(worker_id=1, model=global_model, layers_to_update=worker_1_layers, dataset=worker_datasets[1], csv_file='worker_1_weights.csv')\n","\n","# Parallel worker execution using threading\n","def run_worker(worker):\n","    \"\"\"\n","    Helper function to run a worker in a thread.\n","    \"\"\"\n","    worker.run()\n","\n","# Create threads for workers\n","worker_0_thread = threading.Thread(target=run_worker, args=(worker_0,))\n","worker_1_thread = threading.Thread(target=run_worker, args=(worker_1,))\n","\n","# Start threads\n","worker_0_thread.start()\n","worker_1_thread.start()\n","\n","# Wait for both workers to finish\n","worker_0_thread.join()\n","worker_1_thread.join()\n","\n","# Evaluate the global model\n","global_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","test_loss, test_acc = global_model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_acc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6l8ZuOQ8R61C","executionInfo":{"status":"ok","timestamp":1735456892938,"user_tz":-330,"elapsed":175426,"user":{"displayName":"Chandra Sekhar","userId":"00330720378866032186"}},"outputId":"7825ae6d-b5b3-43d6-fad6-fa94dec63b87"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)           │             \u001b[38;5;34m156\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d (\u001b[38;5;33mAveragePooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │           \u001b[38;5;34m2,416\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_1                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)                 │          \u001b[38;5;34m48,120\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)                  │          \u001b[38;5;34m10,164\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m850\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ average_pooling2d_1                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Worker 0 updated layers: [0]\n","Worker 0 - Step 0, Loss: 2.303966999053955\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 1, Loss: 2.2964797019958496\n","Worker 0 updated layers: [0]Worker 1 updated layers: [2, 5, 6, 7]\n","\n","Worker 0 - Step 2, Loss: 2.307445764541626\n","Worker 1 - Step 0, Loss: 2.30586838722229\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 3, Loss: 2.311882257461548\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 4, Loss: 2.299121856689453\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 1, Loss: 2.3009235858917236\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 5, Loss: 2.2981696128845215\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 6, Loss: 2.310103416442871\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 7, Loss: 2.307710647583008\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 2, Loss: 2.304830551147461\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 8, Loss: 2.3085670471191406\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 9, Loss: 2.2985544204711914\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 10, Loss: 2.2924444675445557\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 3, Loss: 2.304739475250244\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 11, Loss: 2.298771381378174\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 12, Loss: 2.297914743423462\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 4, Loss: 2.3045458793640137\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 13, Loss: 2.3050708770751953\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 14, Loss: 2.2935006618499756\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 15, Loss: 2.3003299236297607\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 5, Loss: 2.303673267364502\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 16, Loss: 2.304821491241455\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 17, Loss: 2.296015739440918\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 18, Loss: 2.3081531524658203\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 6, Loss: 2.3018524646759033\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 19, Loss: 2.291292667388916\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 20, Loss: 2.307811737060547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 21, Loss: 2.305912494659424\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 7, Loss: 2.3012380599975586\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 22, Loss: 2.301546812057495\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 23, Loss: 2.3036258220672607\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 24, Loss: 2.3055105209350586\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 25, Loss: 2.293271541595459\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 8, Loss: 2.3081679344177246\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 26, Loss: 2.289726972579956\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 27, Loss: 2.3026649951934814\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 0 updated layers: [0]\n","Worker 1 - Step 9, Loss: 2.300031900405884Worker 0 - Step 28, Loss: 2.2957069873809814\n","\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 29, Loss: 2.289100170135498\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 30, Loss: 2.302142381668091\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 31, Loss: 2.302476406097412\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 10, Loss: 2.2978532314300537\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 32, Loss: 2.3027093410491943\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 33, Loss: 2.300915002822876\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 11, Loss: 2.2949538230895996\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 34, Loss: 2.300325393676758\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 35, Loss: 2.3000376224517822\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 36, Loss: 2.297952175140381\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 12, Loss: 2.307300090789795\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 37, Loss: 2.293201446533203\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 38, Loss: 2.2915196418762207\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 39, Loss: 2.3006012439727783\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 40, Loss: 2.301328659057617\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 13, Loss: 2.2964210510253906\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 41, Loss: 2.2818918228149414\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 42, Loss: 2.286241054534912\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 14, Loss: 2.3009471893310547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 43, Loss: 2.297156810760498\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 44, Loss: 2.296410322189331\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 45, Loss: 2.295693874359131\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 46, Loss: 2.295151472091675Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 15, Loss: 2.295266628265381\n","\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 47, Loss: 2.2930381298065186\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 48, Loss: 2.298415422439575\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 49, Loss: 2.2878830432891846\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 16, Loss: 2.2866225242614746\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 50, Loss: 2.286242723464966\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 51, Loss: 2.288943290710449\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 52, Loss: 2.2929019927978516\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 17, Loss: 2.2849130630493164\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 53, Loss: 2.295786142349243\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 54, Loss: 2.2818360328674316\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 55, Loss: 2.2955048084259033\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 18, Loss: 2.275959014892578\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 56, Loss: 2.2959821224212646\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 57, Loss: 2.2843079566955566\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 58, Loss: 2.297515869140625\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 19, Loss: 2.3010058403015137\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 59, Loss: 2.2962937355041504\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 60, Loss: 2.2952985763549805\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 61, Loss: 2.289046049118042\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 20, Loss: 2.290595769882202\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 62, Loss: 2.274247884750366\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 63, Loss: 2.312645435333252\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 64, Loss: 2.2807884216308594\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 21, Loss: 2.286943197250366\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 65, Loss: 2.2955098152160645\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 66, Loss: 2.274392604827881\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 67, Loss: 2.2762222290039062\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 22, Loss: 2.2807445526123047\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 68, Loss: 2.2723774909973145\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 69, Loss: 2.288680076599121\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 70, Loss: 2.2987656593322754\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 23, Loss: 2.2882819175720215\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 71, Loss: 2.265031337738037\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 72, Loss: 2.2674691677093506\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 73, Loss: 2.244720697402954\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 24, Loss: 2.2541136741638184\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 74, Loss: 2.2754154205322266\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 75, Loss: 2.2573728561401367\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 76, Loss: 2.2584190368652344\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 25, Loss: 2.262573003768921\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 77, Loss: 2.239269733428955\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 78, Loss: 2.2431962490081787\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 79, Loss: 2.2564680576324463\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 80, Loss: 2.2497894763946533\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 26, Loss: 2.269216537475586\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 81, Loss: 2.2670767307281494\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 82, Loss: 2.252859592437744\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 83, Loss: 2.257986068725586\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 27, Loss: 2.258897304534912\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 84, Loss: 2.2582035064697266\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 85, Loss: 2.2911198139190674\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 86, Loss: 2.271902561187744\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 28, Loss: 2.219636917114258\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 87, Loss: 2.2522997856140137\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 88, Loss: 2.2503018379211426\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 89, Loss: 2.2879951000213623\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 29, Loss: 2.245720863342285\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 90, Loss: 2.241257667541504\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 91, Loss: 2.227536678314209\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 92, Loss: 2.2480974197387695\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 30, Loss: 2.2054800987243652\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 93, Loss: 2.21907901763916\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 94, Loss: 2.227799892425537\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 31, Loss: 2.188141345977783\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 95, Loss: 2.2844882011413574\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 96, Loss: 2.2035019397735596\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 97, Loss: 2.216291666030884\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 32, Loss: 2.259526252746582\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 98, Loss: 2.1829833984375\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 99, Loss: 2.1718764305114746\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 100, Loss: 2.187333583831787\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 33, Loss: 2.2682433128356934\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 101, Loss: 2.212409734725952\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 102, Loss: 2.24613618850708\n","Worker 1 updated layers: [2, 5, 6, 7]Worker 0 updated layers: [0]\n","\n","Worker 0 - Step 103, Loss: 2.173989772796631Worker 1 - Step 34, Loss: 2.110957145690918\n","\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 104, Loss: 2.232377529144287\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 105, Loss: 2.1849637031555176\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 35, Loss: 2.1769237518310547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 106, Loss: 2.1654305458068848\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 107, Loss: 2.282784938812256\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 108, Loss: 2.1584010124206543\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 36, Loss: 2.278535842895508\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 109, Loss: 2.1765828132629395\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 110, Loss: 2.2557108402252197\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 111, Loss: 2.2424840927124023\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 112, Loss: 2.168059825897217\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 37, Loss: 2.2111730575561523\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 113, Loss: 2.1764323711395264\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 114, Loss: 2.1900582313537598\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 38, Loss: 2.1565420627593994\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 115, Loss: 2.2078704833984375\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 116, Loss: 2.1767141819000244\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 117, Loss: 2.1386284828186035\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 39, Loss: 2.247577667236328\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 118, Loss: 2.1932075023651123\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 119, Loss: 2.215146780014038\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 40, Loss: 2.2318053245544434\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 120, Loss: 2.128718137741089\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 121, Loss: 2.199571132659912\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 122, Loss: 2.182870864868164\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 41, Loss: 2.164301872253418\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 123, Loss: 2.1518988609313965\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 124, Loss: 2.129484176635742\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 125, Loss: 2.1594040393829346\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 42, Loss: 2.158501148223877\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 126, Loss: 2.1070404052734375\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 127, Loss: 2.1454243659973145\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 128, Loss: 2.2005181312561035\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 43, Loss: 2.2064261436462402\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 129, Loss: 2.2366604804992676\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 130, Loss: 2.1378235816955566\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 131, Loss: 2.1898300647735596\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 44, Loss: 2.0583009719848633\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 132, Loss: 2.032865524291992\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 133, Loss: 2.176173210144043\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 134, Loss: 2.2176513671875\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 45, Loss: 2.1946749687194824\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 135, Loss: 2.1803598403930664\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 136, Loss: 2.127163887023926\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 137, Loss: 2.198284149169922\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 46, Loss: 2.177304983139038\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 138, Loss: 2.022707462310791\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 139, Loss: 2.2351272106170654\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 140, Loss: 2.219153642654419\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 47, Loss: 2.1086034774780273\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 141, Loss: 2.103756904602051\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 142, Loss: 2.1494028568267822\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 48, Loss: 2.1195030212402344\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 143, Loss: 2.276592969894409\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 144, Loss: 2.24893856048584\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 145, Loss: 2.079664468765259\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 49, Loss: 2.007638692855835\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 146, Loss: 2.0597546100616455\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 147, Loss: 2.1395716667175293\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 148, Loss: 2.0412471294403076\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 50, Loss: 2.144430160522461\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 149, Loss: 2.232297420501709\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 150, Loss: 2.07820200920105\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 151, Loss: 2.189828872680664\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 51, Loss: 2.1611857414245605\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 152, Loss: 2.2459380626678467\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 153, Loss: 2.0795364379882812\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 154, Loss: 2.161990165710449\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 52, Loss: 2.180553436279297\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 155, Loss: 2.1877877712249756\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 156, Loss: 2.043276786804199\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 157, Loss: 2.141251564025879\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 53, Loss: 2.120967388153076\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 158, Loss: 2.199549913406372\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 159, Loss: 2.1145968437194824\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 160, Loss: 2.005838394165039\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 161, Loss: 1.9952807426452637\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 54, Loss: 2.094944477081299\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 162, Loss: 2.1666665077209473\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 163, Loss: 2.0686111450195312\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 55, Loss: 2.1690940856933594\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 164, Loss: 2.163652181625366\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 165, Loss: 2.1603527069091797\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 166, Loss: 2.183811664581299\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 56, Loss: 2.144664764404297\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 167, Loss: 2.1292834281921387\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 168, Loss: 2.068455934524536\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 169, Loss: 2.0358712673187256\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 57, Loss: 2.011094093322754\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 170, Loss: 2.1448562145233154\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 171, Loss: 2.0727062225341797\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 172, Loss: 2.029130220413208\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 58, Loss: 2.088014602661133\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 173, Loss: 2.07285737991333\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 174, Loss: 2.022836208343506\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 175, Loss: 2.0448451042175293\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 176, Loss: 2.138500690460205\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 59, Loss: 2.1008405685424805\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 177, Loss: 2.1486990451812744\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 178, Loss: 2.104447364807129\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 179, Loss: 2.0818378925323486\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 60, Loss: 2.0840964317321777\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 180, Loss: 2.158212900161743\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 181, Loss: 2.0329837799072266\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 182, Loss: 2.0950725078582764\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 61, Loss: 2.08625864982605\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 183, Loss: 2.173997402191162\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 184, Loss: 2.0481014251708984\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 185, Loss: 2.0154452323913574\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 62, Loss: 2.1252169609069824\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 186, Loss: 2.197040557861328\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 187, Loss: 2.0506081581115723\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 188, Loss: 1.9924906492233276\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 63, Loss: 2.0782246589660645\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 189, Loss: 1.9248921871185303\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 190, Loss: 1.9921760559082031\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 191, Loss: 2.0436205863952637\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 64, Loss: 2.051530122756958\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 192, Loss: 2.0915966033935547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 193, Loss: 2.038585662841797\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 194, Loss: 2.0712685585021973\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 65, Loss: 2.120211601257324\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 195, Loss: 2.184730052947998\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 196, Loss: 2.0415003299713135\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 197, Loss: 2.087930679321289\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 66, Loss: 1.994706153869629\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 198, Loss: 2.0403685569763184\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 199, Loss: 2.1704354286193848\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 200, Loss: 2.0302786827087402\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 67, Loss: 2.0791497230529785\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 201, Loss: 2.155583143234253\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 202, Loss: 2.1410040855407715\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 203, Loss: 2.1435482501983643\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 68, Loss: 2.064619541168213\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 204, Loss: 1.9196537733078003\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 205, Loss: 2.0906565189361572\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 206, Loss: 2.1859898567199707\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 69, Loss: 2.136282444000244\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 207, Loss: 2.15403413772583\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 208, Loss: 2.1972832679748535\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 209, Loss: 2.056530475616455\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 70, Loss: 2.0652904510498047\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 210, Loss: 2.040531635284424\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 211, Loss: 1.9846763610839844\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 212, Loss: 2.0271167755126953\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 71, Loss: 2.062166690826416\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 213, Loss: 2.268634796142578\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 214, Loss: 2.059884548187256\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 72, Loss: 1.9809550046920776\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 215, Loss: 1.899395227432251\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 216, Loss: 1.8397877216339111\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 217, Loss: 2.221482276916504\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 73, Loss: 2.1457834243774414\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 218, Loss: 1.9412329196929932\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 219, Loss: 2.0058164596557617\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 220, Loss: 2.130401134490967\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 74, Loss: 2.0874733924865723\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 221, Loss: 2.086428165435791\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 222, Loss: 2.0676848888397217\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 223, Loss: 1.9696455001831055\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 75, Loss: 2.0862724781036377\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 224, Loss: 2.007272243499756\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 225, Loss: 2.1139492988586426\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 226, Loss: 2.028093099594116\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 76, Loss: 1.9738049507141113\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 227, Loss: 2.0756657123565674\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 228, Loss: 2.0811262130737305\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 229, Loss: 1.8134686946868896\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 230, Loss: 1.9833180904388428\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 77, Loss: 2.1008076667785645\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 231, Loss: 2.090853691101074\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 232, Loss: 2.0136337280273438\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 233, Loss: 1.9564003944396973\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 78, Loss: 1.9984724521636963\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 234, Loss: 2.0341084003448486\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 235, Loss: 1.9600248336791992\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 236, Loss: 2.120807647705078\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 79, Loss: 2.0591020584106445\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 237, Loss: 2.0631473064422607\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 238, Loss: 2.082024574279785\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 239, Loss: 2.000077247619629\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 80, Loss: 1.916013240814209\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 240, Loss: 1.9468481540679932\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 241, Loss: 2.0406460762023926\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 242, Loss: 2.122300863265991\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 81, Loss: 2.0177526473999023\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 243, Loss: 1.9801595211029053\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 244, Loss: 1.9358652830123901\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 245, Loss: 2.123713493347168\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 82, Loss: 2.0528111457824707\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 246, Loss: 2.0991034507751465\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 247, Loss: 2.077359676361084\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 248, Loss: 1.8167972564697266\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 83, Loss: 2.0619730949401855\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 249, Loss: 1.9912056922912598\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 250, Loss: 2.079648017883301\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 251, Loss: 2.0627551078796387\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 84, Loss: 2.0525224208831787\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 252, Loss: 1.9696449041366577\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 253, Loss: 1.944379448890686\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 254, Loss: 2.0102791786193848\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 85, Loss: 1.9937463998794556\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 255, Loss: 1.9584543704986572\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 256, Loss: 2.1475234031677246\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 257, Loss: 2.020862102508545\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 86, Loss: 1.870361089706421\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 258, Loss: 1.9700167179107666\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 259, Loss: 1.973261833190918\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 260, Loss: 2.0581321716308594\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 261, Loss: 1.967997431755066\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 87, Loss: 1.9479434490203857\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 262, Loss: 1.885678768157959\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 263, Loss: 1.9887280464172363\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 264, Loss: 2.001305103302002\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 88, Loss: 1.9679083824157715\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 265, Loss: 1.912536859512329\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 266, Loss: 1.9618732929229736\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 267, Loss: 2.088435649871826\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 89, Loss: 1.948754072189331\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 268, Loss: 2.026487350463867\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 269, Loss: 2.066373348236084\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 270, Loss: 1.9652512073516846\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 271, Loss: 2.0453884601593018\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 90, Loss: 1.9401934146881104\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 272, Loss: 2.0119733810424805\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 273, Loss: 1.9874985218048096\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 91, Loss: 2.0270204544067383\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 274, Loss: 2.0344595909118652\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 275, Loss: 1.9956035614013672\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 276, Loss: 2.0322625637054443\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 92, Loss: 1.9359462261199951\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 277, Loss: 1.9486420154571533\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 278, Loss: 1.9882464408874512\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 279, Loss: 2.088761806488037\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 93, Loss: 2.0740880966186523\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 280, Loss: 1.9393235445022583\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 281, Loss: 1.6786487102508545\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 282, Loss: 2.0650365352630615\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 94, Loss: 2.098909378051758\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 283, Loss: 2.107509136199951\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 284, Loss: 2.0766096115112305\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 285, Loss: 1.8128364086151123\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 95, Loss: 2.0037386417388916\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 286, Loss: 1.9929641485214233\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 287, Loss: 2.07535457611084\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 288, Loss: 1.8799489736557007\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 96, Loss: 1.9872117042541504\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 289, Loss: 1.8287322521209717\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 290, Loss: 2.0155465602874756\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 291, Loss: 2.115537166595459\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 97, Loss: 1.9680900573730469\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 292, Loss: 1.9216489791870117\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 293, Loss: 1.9348421096801758\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 294, Loss: 2.0102896690368652\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 98, Loss: 2.046945571899414\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 295, Loss: 2.0099034309387207\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 296, Loss: 2.0169734954833984\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 297, Loss: 1.9968050718307495\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 99, Loss: 2.0338211059570312\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 298, Loss: 2.0172557830810547\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 299, Loss: 1.9978240728378296\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 0 updated layers: [0]\n","Worker 1 - Step 100, Loss: 2.1589221954345703\n","Worker 0 - Step 300, Loss: 1.9669809341430664\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 301, Loss: 1.9296817779541016\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 302, Loss: 1.8512024879455566\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 101, Loss: 1.9750021696090698\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 303, Loss: 1.8862429857254028\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 304, Loss: 2.063875675201416\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 305, Loss: 2.107351541519165\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 102, Loss: 1.9729278087615967\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 306, Loss: 1.848191499710083\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 307, Loss: 1.9299955368041992\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 308, Loss: 2.075571060180664\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 103, Loss: 2.0236239433288574\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 309, Loss: 1.8316493034362793\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 310, Loss: 1.9208847284317017\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 311, Loss: 1.8776826858520508\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 104, Loss: 1.899155616760254\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 312, Loss: 2.0056238174438477\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 313, Loss: 1.8402087688446045\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 314, Loss: 1.8519108295440674\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 105, Loss: 1.9088071584701538\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 315, Loss: 1.9688410758972168\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 316, Loss: 1.9471688270568848\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 317, Loss: 1.9539923667907715\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 106, Loss: 1.8970695734024048\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 318, Loss: 1.9987965822219849\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 319, Loss: 1.985100269317627\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 320, Loss: 1.9853373765945435\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 107, Loss: 1.9399397373199463\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 321, Loss: 1.8867688179016113\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 322, Loss: 2.0805788040161133\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 323, Loss: 1.9732718467712402\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 108, Loss: 1.8523025512695312\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 324, Loss: 1.7723767757415771\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 325, Loss: 1.9200423955917358\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 326, Loss: 1.8507918119430542\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 109, Loss: 1.8198888301849365\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 327, Loss: 1.9630188941955566\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 328, Loss: 2.0584988594055176\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 329, Loss: 1.950522541999817\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 110, Loss: 1.847597360610962\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 330, Loss: 1.9670040607452393\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 331, Loss: 1.887324571609497\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 332, Loss: 1.8851306438446045\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 111, Loss: 1.9571986198425293\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 333, Loss: 1.850738286972046\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 334, Loss: 2.093287944793701\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 335, Loss: 1.9405524730682373\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 112, Loss: 2.0655558109283447\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 336, Loss: 2.238382339477539\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 337, Loss: 2.014772653579712\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 338, Loss: 2.0118050575256348\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 113, Loss: 1.9917731285095215\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 339, Loss: 2.0619049072265625\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 340, Loss: 1.8556315898895264\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 341, Loss: 1.904587745666504\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 114, Loss: 2.1547083854675293\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 342, Loss: 1.9390838146209717\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 343, Loss: 1.8789323568344116\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 344, Loss: 1.9625190496444702\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 115, Loss: 1.9214811325073242\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 345, Loss: 2.050403356552124\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 346, Loss: 1.9740135669708252\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 347, Loss: 1.9045164585113525\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 116, Loss: 1.8221513032913208\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 348, Loss: 2.051973819732666\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 349, Loss: 1.9995743036270142\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 350, Loss: 2.1156506538391113\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 117, Loss: 2.203160285949707\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 351, Loss: 1.8787416219711304\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 352, Loss: 1.9835118055343628\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 118, Loss: 1.9013569355010986\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 353, Loss: 1.999995231628418\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 354, Loss: 1.8583440780639648\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 355, Loss: 2.1659486293792725\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 119, Loss: 2.034593343734741\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 356, Loss: 2.075713634490967\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 357, Loss: 1.8567825555801392\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 358, Loss: 1.9651739597320557\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 120, Loss: 1.8870747089385986\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 359, Loss: 1.9309170246124268\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 360, Loss: 1.8272628784179688\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 361, Loss: 1.9162988662719727\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 121, Loss: 2.061946153640747\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 362, Loss: 1.8614836931228638\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 363, Loss: 1.9041656255722046\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 364, Loss: 1.946474552154541\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 122, Loss: 2.0758631229400635\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 365, Loss: 1.931351900100708\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 366, Loss: 2.1172714233398438\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 367, Loss: 1.9622328281402588\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 123, Loss: 2.018439292907715\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 368, Loss: 1.906038522720337\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 369, Loss: 1.8857574462890625\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 370, Loss: 1.8300504684448242\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 124, Loss: 1.8035789728164673\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 371, Loss: 1.9135867357254028\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 372, Loss: 1.8809858560562134\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 373, Loss: 1.8449718952178955\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 125, Loss: 2.0028162002563477\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 374, Loss: 1.9596617221832275\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 375, Loss: 1.884465217590332\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 126, Loss: 1.8502428531646729\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 376, Loss: 1.8266792297363281\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 377, Loss: 1.839242935180664\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 378, Loss: 1.9994878768920898\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 127, Loss: 1.9302129745483398\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 379, Loss: 2.0163016319274902\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 380, Loss: 2.024547815322876\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 381, Loss: 1.8738279342651367\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 128, Loss: 1.9387263059616089\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 382, Loss: 1.9530315399169922\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 383, Loss: 1.7377567291259766\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 384, Loss: 1.9153556823730469\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 129, Loss: 2.008725643157959\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 385, Loss: 2.040534496307373\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 386, Loss: 1.919837474822998\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 387, Loss: 2.0122222900390625\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 130, Loss: 1.973740577697754\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 388, Loss: 1.9791162014007568\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 389, Loss: 1.8609094619750977\n","Worker 0 updated layers: [0]\n","Worker 0 - Step 390, Loss: 1.9600733518600464\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 131, Loss: 1.914643406867981\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 132, Loss: 1.9520212411880493\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 133, Loss: 1.9679614305496216\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 134, Loss: 1.9497153759002686\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 135, Loss: 1.9244502782821655\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 136, Loss: 2.0273284912109375\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 137, Loss: 1.9124735593795776\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 138, Loss: 1.9061110019683838\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 139, Loss: 1.9358680248260498\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 140, Loss: 2.0547666549682617\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 141, Loss: 1.7195311784744263\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 142, Loss: 1.69132399559021\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 143, Loss: 1.8565795421600342\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 144, Loss: 1.9576513767242432\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 145, Loss: 1.8483474254608154\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 146, Loss: 1.9234188795089722\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 147, Loss: 1.945304036140442\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 148, Loss: 1.9053027629852295\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 149, Loss: 1.8789249658584595\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 150, Loss: 1.8752977848052979\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 151, Loss: 1.984894037246704\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 152, Loss: 1.9807528257369995\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 153, Loss: 1.7446383237838745\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 154, Loss: 2.0169098377227783\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 155, Loss: 1.7665257453918457\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 156, Loss: 1.8474072217941284\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 157, Loss: 1.8191297054290771\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 158, Loss: 1.8017756938934326\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 159, Loss: 1.9125216007232666\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 160, Loss: 1.878955364227295\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 161, Loss: 1.7198143005371094\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 162, Loss: 1.969173789024353\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 163, Loss: 1.9464390277862549\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 164, Loss: 1.898719310760498\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 165, Loss: 1.9381744861602783\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 166, Loss: 1.9119545221328735\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 167, Loss: 1.8714187145233154\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 168, Loss: 1.8906519412994385\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 169, Loss: 1.799910545349121\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 170, Loss: 1.7301274538040161\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 171, Loss: 1.9750913381576538\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 172, Loss: 1.9466115236282349\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 173, Loss: 1.8446283340454102\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 174, Loss: 2.063516616821289\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 175, Loss: 1.965787649154663\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 176, Loss: 1.9899392127990723\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 177, Loss: 1.8754868507385254\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 178, Loss: 1.878171443939209\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 179, Loss: 1.9478927850723267\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 180, Loss: 1.9584150314331055\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 181, Loss: 1.9518439769744873\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 182, Loss: 2.003718376159668\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 183, Loss: 1.8785390853881836\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 184, Loss: 1.8367700576782227\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 185, Loss: 1.8803433179855347\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 186, Loss: 1.8311121463775635\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 187, Loss: 1.891211748123169\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 188, Loss: 1.743971347808838\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 189, Loss: 1.9152026176452637\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 190, Loss: 1.895204782485962\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 191, Loss: 1.7856172323226929\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 192, Loss: 1.763779878616333\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 193, Loss: 1.686967372894287\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 194, Loss: 1.989493489265442\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 195, Loss: 1.7151706218719482\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 196, Loss: 1.7059694528579712\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 197, Loss: 1.7772269248962402\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 198, Loss: 1.8929321765899658\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 199, Loss: 1.8394447565078735\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 200, Loss: 1.9471659660339355\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 201, Loss: 1.7712453603744507\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 202, Loss: 1.8278758525848389\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 203, Loss: 1.8015599250793457\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 204, Loss: 1.807004451751709\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 205, Loss: 1.8291746377944946\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 206, Loss: 1.8763483762741089\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 207, Loss: 1.6820366382598877\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 208, Loss: 1.9838993549346924\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 209, Loss: 1.8049428462982178\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 210, Loss: 1.8869056701660156\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 211, Loss: 1.8301525115966797\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 212, Loss: 1.6771209239959717\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 213, Loss: 1.9375536441802979\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 214, Loss: 1.8594951629638672\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 215, Loss: 1.8469946384429932\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 216, Loss: 1.7396010160446167\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 217, Loss: 1.7438669204711914\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 218, Loss: 1.7819199562072754\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 219, Loss: 1.6971564292907715\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 220, Loss: 1.790645718574524\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 221, Loss: 1.94581937789917\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 222, Loss: 1.8802438974380493\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 223, Loss: 1.7669634819030762\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 224, Loss: 1.7295184135437012\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 225, Loss: 1.6616747379302979\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 226, Loss: 1.8610508441925049\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 227, Loss: 1.8103058338165283\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 228, Loss: 1.9275718927383423\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 229, Loss: 1.7575252056121826\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 230, Loss: 1.8453290462493896\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 231, Loss: 1.8434163331985474\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 232, Loss: 1.7387712001800537\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 233, Loss: 1.7653123140335083\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 234, Loss: 1.6946563720703125\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 235, Loss: 1.8857901096343994\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 236, Loss: 1.756094217300415\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 237, Loss: 1.8765873908996582\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 238, Loss: 1.747942328453064\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 239, Loss: 1.8997998237609863\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 240, Loss: 2.0104784965515137\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 241, Loss: 1.8014812469482422\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 242, Loss: 1.6873713731765747\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 243, Loss: 1.7626699209213257\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 244, Loss: 1.8937205076217651\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 245, Loss: 2.0349936485290527\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 246, Loss: 1.8224895000457764\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 247, Loss: 1.9335012435913086\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 248, Loss: 1.7720069885253906\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 249, Loss: 1.8837171792984009\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 250, Loss: 1.6474699974060059\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 251, Loss: 1.8666951656341553\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 252, Loss: 1.8239598274230957\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 253, Loss: 1.7943845987319946\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 254, Loss: 2.0627551078796387\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 255, Loss: 1.838809847831726\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 256, Loss: 1.7500474452972412\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 257, Loss: 1.7829431295394897\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 258, Loss: 1.7853140830993652\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 259, Loss: 1.5471806526184082\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 260, Loss: 1.9032859802246094\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 261, Loss: 1.8286632299423218\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 262, Loss: 1.8027286529541016\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 263, Loss: 1.7260973453521729\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 264, Loss: 1.8050916194915771\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 265, Loss: 1.7480958700180054\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 266, Loss: 1.9057866334915161\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 267, Loss: 1.7646667957305908\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 268, Loss: 1.8698830604553223\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 269, Loss: 1.736797571182251\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 270, Loss: 1.8880748748779297\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 271, Loss: 1.808956265449524\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 272, Loss: 1.7003873586654663\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 273, Loss: 1.6488239765167236\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 274, Loss: 1.7537130117416382\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 275, Loss: 1.8018485307693481\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 276, Loss: 1.7437645196914673\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 277, Loss: 1.7993104457855225\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 278, Loss: 1.8755000829696655\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 279, Loss: 1.9033071994781494\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 280, Loss: 1.8916401863098145\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 281, Loss: 1.7181823253631592\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 282, Loss: 1.8485832214355469\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 283, Loss: 1.8530789613723755\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 284, Loss: 1.673803448677063\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 285, Loss: 2.036691904067993\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 286, Loss: 1.8391045331954956\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 287, Loss: 1.8483283519744873\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 288, Loss: 1.8706855773925781\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 289, Loss: 2.053008556365967\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 290, Loss: 1.886522889137268\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 291, Loss: 1.729565978050232\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 292, Loss: 1.8126850128173828\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 293, Loss: 1.8018946647644043\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 294, Loss: 2.076592206954956\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 295, Loss: 1.932448148727417\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 296, Loss: 1.7429373264312744\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 297, Loss: 1.9765269756317139\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 298, Loss: 1.7994581460952759\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 299, Loss: 1.8004179000854492\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 300, Loss: 1.9120571613311768\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 301, Loss: 1.875841736793518\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 302, Loss: 1.8279982805252075\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 303, Loss: 1.8064641952514648\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 304, Loss: 1.6178863048553467\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 305, Loss: 1.7957885265350342\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 306, Loss: 1.7085893154144287\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 307, Loss: 1.6859307289123535\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 308, Loss: 1.8544888496398926\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 309, Loss: 1.7287330627441406\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 310, Loss: 1.7388663291931152\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 311, Loss: 1.9497770071029663\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 312, Loss: 1.78593111038208\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 313, Loss: 1.6936180591583252\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 314, Loss: 1.8487516641616821\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 315, Loss: 1.8214585781097412\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 316, Loss: 1.672868251800537\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 317, Loss: 1.8610550165176392\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 318, Loss: 1.7716816663742065\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 319, Loss: 1.9188475608825684\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 320, Loss: 1.7601646184921265\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 321, Loss: 1.7412346601486206\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 322, Loss: 1.75851309299469\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 323, Loss: 1.8043041229248047\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 324, Loss: 1.8193926811218262\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 325, Loss: 1.9148995876312256\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 326, Loss: 1.789894700050354\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 327, Loss: 1.814655065536499\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 328, Loss: 1.81735098361969\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 329, Loss: 1.8183138370513916\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 330, Loss: 1.7467982769012451\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 331, Loss: 1.6458511352539062\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 332, Loss: 1.7263416051864624\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 333, Loss: 1.7123956680297852\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 334, Loss: 1.7913724184036255\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 335, Loss: 1.7880444526672363\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 336, Loss: 1.9604909420013428\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 337, Loss: 1.9404420852661133\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 338, Loss: 1.9238166809082031\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 339, Loss: 1.7166551351547241\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 340, Loss: 1.8791362047195435\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 341, Loss: 1.80939519405365\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 342, Loss: 1.9427952766418457\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 343, Loss: 1.7642600536346436\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 344, Loss: 1.8555316925048828\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 345, Loss: 1.7992048263549805\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 346, Loss: 1.6542644500732422\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 347, Loss: 1.6080211400985718\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 348, Loss: 1.6659823656082153\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 349, Loss: 1.7118834257125854\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 350, Loss: 1.749121904373169\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 351, Loss: 1.6635024547576904\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 352, Loss: 1.887653112411499\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 353, Loss: 1.8782886266708374\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 354, Loss: 1.6451966762542725\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 355, Loss: 1.6681816577911377\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 356, Loss: 1.865903615951538\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 357, Loss: 1.898249864578247\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 358, Loss: 1.8206853866577148\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 359, Loss: 1.6431384086608887\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 360, Loss: 1.7640405893325806\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 361, Loss: 1.7659497261047363\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 362, Loss: 1.7687559127807617\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 363, Loss: 1.7850980758666992\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 364, Loss: 1.7614612579345703\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 365, Loss: 1.6604483127593994\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 366, Loss: 1.617195963859558\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 367, Loss: 1.9108870029449463\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 368, Loss: 1.9985392093658447\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 369, Loss: 1.6307388544082642\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 370, Loss: 1.845095157623291\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 371, Loss: 1.6294771432876587\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 372, Loss: 1.8079051971435547\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 373, Loss: 1.7791659832000732\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 374, Loss: 1.6865687370300293\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 375, Loss: 1.614441990852356\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 376, Loss: 1.8409279584884644\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 377, Loss: 1.6830838918685913\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 378, Loss: 1.722588300704956\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 379, Loss: 1.6412878036499023\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 380, Loss: 1.8375052213668823\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 381, Loss: 1.7960665225982666\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 382, Loss: 1.9958868026733398\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 383, Loss: 1.7770743370056152\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 384, Loss: 1.7495667934417725\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 385, Loss: 1.7958555221557617\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 386, Loss: 1.699139952659607\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 387, Loss: 1.7820820808410645\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 388, Loss: 1.8299133777618408\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 389, Loss: 1.6736042499542236\n","Worker 1 updated layers: [2, 5, 6, 7]\n","Worker 1 - Step 390, Loss: 1.9951046705245972\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.3375 - loss: 1.8323\n","Test Loss: 1.8376532793045044\n","Test Accuracy: 0.33399999141693115\n"]}]}]}