{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 19:43:40.497264: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-13 19:43:40.497293: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               4224      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 784)               201488    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 476,720\n",
      "Trainable params: 476,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 19:43:45.370835: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-06-13 19:43:45.370885: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-06-13 19:43:45.370918: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lenovo-Lenovo-ideapad-320-15IKB): /proc/driver/nvidia/version does not exist\n",
      "2024-06-13 19:43:45.372168: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "encoding_dim = 32  # Size of our encoded representations \n",
    "\n",
    "input_img = keras.Input(shape=(x_train.shape[1],))                   # Input image\n",
    "encoded = layers.Dense(256, activation='relu')(input_img)   # \"encoded\" is encoded representation of  input\n",
    "encoded = layers.Dense(128, activation='relu')(encoded)   # \"encoded\" is encoded representation of  input\n",
    "\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(encoded)   # \"encoded\" is encoded representation of  input\n",
    "\n",
    "decoded = layers.Dense(128, activation='relu')(encoded)   # \"encoded\" is encoded representation of  input\n",
    "decoded = layers.Dense(256, activation='relu')(decoded)   # \"encoded\" is encoded representation of  input\n",
    "\n",
    "\n",
    "decoded = layers.Dense(784, activation='sigmoid')(decoded)           # \"decoded\" is lossy reconstruction of  input\n",
    "autoencoder = keras.Model(input_img, decoded)                        # This model maps an input to its reconstruction\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 8s 31ms/step - loss: 0.3576 - val_loss: 0.3119\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 7s 30ms/step - loss: 0.3024 - val_loss: 0.3032\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 7s 31ms/step - loss: 0.2948 - val_loss: 0.2941\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.2901 - val_loss: 0.2904\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 7s 29ms/step - loss: 0.2867 - val_loss: 0.2875\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 7s 31ms/step - loss: 0.2843 - val_loss: 0.2857\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 8s 35ms/step - loss: 0.2823 - val_loss: 0.2837\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 7s 30ms/step - loss: 0.2808 - val_loss: 0.2826\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 8s 32ms/step - loss: 0.2795 - val_loss: 0.2812\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 8s 32ms/step - loss: 0.2783 - val_loss: 0.2799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76b4161910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5826/704971066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# This is our encoded (32-dimensional) input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the decoder model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdecoded_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_layer' is not defined"
     ]
    }
   ],
   "source": [
    "x_testr = keras.Model(input_img, encoded)            # This model maps an input to its encoded representation\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))   # This is our encoded (32-dimensional) input\n",
    "\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))  # Create the decoder model\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('First row is orignal images')\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "print('Second row is reconstrcuted images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded representations\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = keras.Model(input_img, encoded)           # This model maps an input to its encoded representation\n",
    "encoded_imgs = encoder.predict(x_test)              # This is our encoded (32-dimensional) input\n",
    "\n",
    "print('Encoded representations')\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(1, n + 1):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(encoded_imgs[i].reshape((1, 4 * 8)).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
