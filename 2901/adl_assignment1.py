# -*- coding: utf-8 -*-
"""ADL_Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i958Ykdfs-HlbSnSiZjFB_euGi5M51NO

## Group ID
ADL Group 75


## Group Members
1. Ravindra Kumar Tholiya - 2023AA05124
2. Jahnavi Gali - 2023AA5684
3. Shivam Sahil - 2023AA05663

## Problem Statement
This assignment is about feature extraction via dimensionality reduction using variants of autoencoders.  Use the CIFAR10 dataset provided in Keras, after conversion to gray-level images! Use randomly selected 70% of the dataset as training set and remaining 30% as the test set.

Task 1: Perform standard PCA with 70% of the training dataset and identify the eigenvectors associated with top eigenvalues with 95% total energy. With these, train a logistic regression classifier to classify the images into 10 classes. Draw the ROC curve for the test dataset. Repeat the same with randomized PCA and compare.

Task 2: Train a single layer autoencoder with linear activation function and appropriately mean and variance normalized input with constraint that encoder weight matrix and decoder weight matrix are transpose w,r,t, each other. Compare the eigenvectors obtained in step 1 with those obtained using the autoencoders. Explain your observations.

Task 3: Train an appropriate deep convolutional autoencoder with same dimension of latent space. Calculate the reconstruction error fand compare that with a single hidden layer autoencoder (with sigmoid activation at the autoencoder and linear at the decoder) for the test dataset. What will be the reconstruction error if the hidden nodes are distributed equally (approximately) among 3 hidden layers in a new 3 hidden layer autoencoder with sigmoid activation at the autoencoder and linear at the decoder final layer?

Task 4. Train a deep convolutional autoencoder with MNIST dataset and using extracted features train a MLP classifier with 7 outputs (7 segment LED display) that are representative of 10 digits. For example images of "0" will be classified as

   1

1    1

   1    

1     1

   1

7 will be "classified" as

   1

0    1

   0    

0    1

   0

Generate the confusion matrix for the corresponding test dataset.

Task 5: Upload both *.ipynb with all outputs embedded and corresponding *.html files.

Step 1: Use the CIFAR10 dataset provided in Keras.
After conversion to gray-level images! Use randomly selected 70% of the dataset as training set and remaining 30% as the test set.
"""

# Perform necessary imports
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc
from tensorflow.keras.datasets import cifar10, mnist
from skimage.color import rgb2gray
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Lambda
from tensorflow.keras import backend as K, layers, models
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, classification_report
import math
import random

"""## Task 1: Perform standard PCA with 70% of the training dataset and identify the eigenvectors associated with top ei"""

# Step 1: Load CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Combine train and test data for splitting later
X = np.concatenate((x_train, x_test))
y = np.concatenate((y_train, y_test))

# Step 2: Convert images to grayscale
X_gray = np.array([rgb2gray(img) for img in X])

# Step 3: Normalize the dataset
X_gray = X_gray / 255.0

# Step 4: Split the data
X_train, X_test, y_train, y_test = train_test_split(X_gray, y, test_size=0.3, random_state=42)

# Verify data shapes
print("Training data shape:", X_train.shape)
print("Testing data shape:", X_test.shape)

# Display a few samples to verify grayscale conversion
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_train[i], cmap='gray')
    plt.axis('off')
plt.suptitle('Sample Grayscale Images from CIFAR-10 Dataset')
plt.show()

# Step 5: Flatten images for PCA
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# Standardize the flattened data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_flat)
X_test_scaled = scaler.transform(X_test_flat)

# Apply PCA to retain 95% of the variance
pca = PCA(0.95)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print("Original training data shape:", X_train_scaled.shape)
print("Reduced training data shape (PCA):", X_train_pca.shape)

# Step 6: Train Logistic Regression on PCA features
log_reg = LogisticRegression(max_iter=1000, multi_class='multinomial')
log_reg.fit(X_train_pca, y_train.ravel())

# Predict probabilities for the test set
y_pred_proba = log_reg.predict_proba(X_test_pca)

# Step 7: Compute ROC curve for each class
fpr = {}
tpr = {}
roc_auc = {}
for i in range(10):
    fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_pred_proba[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(12, 8))
for i in range(10):
    plt.plot(fpr[i], tpr[i], label=f"Class {i} (AUC = {roc_auc[i]:.2f})")
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Logistic Regression with PCA Features')
plt.legend(loc='best')
plt.show()

# Apply Randomized PCA to retain 95% of the variance
# Determine the number of components for 95% variance with default PCA
pca_temp = PCA(0.95)
pca_temp.fit(X_train_scaled)
num_components = pca_temp.n_components_
print(f"Number of components for 95% variance: {num_components}")

# Apply Randomized PCA using the determined number of components
random_pca = PCA(n_components=num_components, svd_solver='randomized')
X_train_random_pca = random_pca.fit_transform(X_train_scaled)
X_test_random_pca = random_pca.transform(X_test_scaled)

print("Reduced training data shape (Randomized PCA):", X_train_random_pca.shape)

# Train Logistic Regression on Randomized PCA features
log_reg_random = LogisticRegression(max_iter=1000, multi_class='multinomial')
log_reg_random.fit(X_train_random_pca, y_train.ravel())

# Predict probabilities for the test set (Randomized PCA)
y_pred_random_proba = log_reg_random.predict_proba(X_test_random_pca)

# Compute ROC curve for each class
fpr_random = {}
tpr_random = {}
roc_auc_random = {}
for i in range(10):
    fpr_random[i], tpr_random[i], _ = roc_curve((y_test == i).astype(int), y_pred_random_proba[:, i])
    roc_auc_random[i] = auc(fpr_random[i], tpr_random[i])

# Plot ROC curves for Randomized PCA
plt.figure(figsize=(12, 8))
for i in range(10):
    plt.plot(fpr_random[i], tpr_random[i], label=f"Class {i} (AUC = {roc_auc_random[i]:.2f})")
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Logistic Regression with Randomized PCA Features')
plt.legend(loc='best')
plt.show()

# Compare standard PCA vs Randomized PCA results
print("Standard PCA Explained Variance Ratio:", sum(pca.explained_variance_ratio_))
print("Randomized PCA Explained Variance Ratio:", sum(random_pca.explained_variance_ratio_))

y_pred_pca = log_reg.predict(X_test_pca)
pca_accuracy = accuracy_score(y_test, y_pred_pca)
print("Test Accuracy (Logistic Regression + Standard PCA):", pca_accuracy)
print("Classification Report (Standard PCA):")
print(classification_report(y_test, y_pred_pca))

# -----------------------------
# 1B. Evaluate classification performance (Randomized PCA)
# -----------------------------
y_pred_random_pca = log_reg_random.predict(X_test_random_pca)
random_pca_accuracy = accuracy_score(y_test, y_pred_random_pca)
print("\nTest Accuracy (Logistic Regression + Randomized PCA):", random_pca_accuracy)
print("Classification Report (Randomized PCA):")
print(classification_report(y_test, y_pred_random_pca))

# -----------------------------
# 2. Compare both PCA methods
# -----------------------------
print("\nComparison of PCA vs. Randomized PCA:")
print(f" - Standard PCA accuracy:       {pca_accuracy:.4f}")
print(f" - Randomized PCA accuracy:     {random_pca_accuracy:.4f}")
print(f" - Standard PCA explained var:  {sum(pca.explained_variance_ratio_):.4f}")
print(f" - Randomized PCA explained var:{sum(random_pca.explained_variance_ratio_):.4f}")
print("Observations: Randomized PCA often yields similar results but can be faster for large dimensional data, etc.")

# -----------------------------
# 3. Visualize top PCA eigenvectors
# -----------------------------
num_eigenvectors_to_show = 5
plt.figure(figsize=(10, 2))
for i in range(num_eigenvectors_to_show):
    ax = plt.subplot(1, num_eigenvectors_to_show, i + 1)
    # pca.components_[i] has shape (input_dim,), reshape to (32,32)
    # Note: X_train_scaled was flattened from (32, 32) -> 1024, so if your shape is 1024
    # you can reshape to (32, 32).
    plt.imshow(pca.components_[i].reshape(32, 32), cmap='gray')
    plt.axis('off')
    plt.title(f"PC {i+1}")
plt.suptitle("Top PCA Eigenvectors")
plt.show()

"""### Discussion:

We observe that the top principal components capture broad grayscale patterns.

Our Randomized PCA yields a similar accuracy of 95%, indicating it approximates the principal components well for this dataset.

## Task 2: Train a single layer autoencoder with linear activation function and appropriately mean and variance normalized input with constraint that encoder weight matrix and decoder weight matrix are transpose w,r,t, each other. Compare the eigenvectors obtained in step 1 with those obtained using the autoencoders. Explain your observations.
"""

# Input dimensions
input_dim = X_train_scaled.shape[1]  # Number of features after flattening
latent_dim = X_train_pca.shape[1]  # Match PCA latent dimension

# Define the encoder
input_layer = Input(shape=(input_dim,))
encoder_layer = Dense(latent_dim, activation='linear', name='encoder_layer')
encoded = encoder_layer(input_layer)

# Define the decoder using tied weights
def tied_decoder(encoded_input, encoder_layer):
    tied_weights = K.transpose(encoder_layer.kernel)  # Transpose the encoder weights
    return K.dot(encoded_input, tied_weights)

# Use Lambda layer for tied weights
decoded = tf.keras.layers.Lambda(
    lambda x: tied_decoder(x, encoder_layer=encoder_layer),
    name='decoder'
)(encoded)

# Build the autoencoder model
autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# Train the autoencoder
history = autoencoder.fit(
    X_train_scaled, X_train_scaled,  # Use the scaled training data
    epochs=50,
    batch_size=256,
    validation_data=(X_test_scaled, X_test_scaled),  # Use the scaled testing data
    verbose=1
)

# Extract the encoder model to get latent representations
encoder_model = Model(inputs=input_layer, outputs=encoded)
X_train_autoencoded = encoder_model.predict(X_train_scaled)
X_test_autoencoded = encoder_model.predict(X_test_scaled)

# Print latent space shape
print("Latent space shape:", X_train_autoencoded.shape)

# -----------------------------
# Compare autoencoder encoder weights to PCA components
# -----------------------------
# PCA components_ shape: (n_components, input_dim)
# AE encoder weights shape: (input_dim, latent_dim)
# Transpose AE encoder weights to get (latent_dim, input_dim)

ae_weights = encoder_layer.get_weights()[0]      # shape -> (input_dim, latent_dim)
ae_weights_t = ae_weights.T                      # shape -> (latent_dim, input_dim)
pca_components = pca.components_                # shape -> (n_components, input_dim)

correlations = []
for i in range(latent_dim):
    corr = np.corrcoef(ae_weights_t[i], pca_components[i])[0, 1]
    correlations.append(corr)

for i, c in enumerate(correlations):
    print(f"Component/Neuron {i} correlation (AE vs. PCA): {c:.4f}")

# Optional: visualize side by side
num_components_to_show = min(latent_dim, 5)
plt.figure(figsize=(12, 4))
for i in range(num_components_to_show):
    # PCA component i
    plt.subplot(2, num_components_to_show, i+1)
    plt.imshow(pca_components[i].reshape(32, 32), cmap='gray')
    plt.title(f"PCA comp {i+1}")
    plt.axis('off')

    # AE weights i
    plt.subplot(2, num_components_to_show, num_components_to_show + i + 1)
    plt.imshow(ae_weights_t[i].reshape(32, 32), cmap='gray')
    plt.title(f"AE weight {i+1}")
    plt.axis('off')

plt.suptitle("Visual Comparison of PCA Components vs. Linear AE Weights")
plt.tight_layout()
plt.show()

"""### Task 3:

Train an appropriate deep convolutional autoencoder (ConvAE) on the CIFAR-10 (grayscale) dataset with the same dimension of latent space as in Task 2.
Calculate and compare the reconstruction error on the test set with:
The single-layer autoencoder (linear encoder-decoder with tied weights) from Task 2.
A single hidden layer autoencoder with sigmoid activation in the encoder and linear activation in the decoder.
Distribute the same total number of hidden nodes equally (approximately) among 3 hidden layers (all with sigmoid activation at the encoder side and linear at the final decoder). Check the reconstruction error again.
"""

# Reshape your training/testing data from (N, 32, 32) to (N, 32, 32, 1)
X_train_cae = X_train[..., np.newaxis]  # shape -> (N, 32, 32, 1)
X_test_cae = X_test[..., np.newaxis]    # shape -> (N, 32, 32, 1)

# Use the same scaling (already /255.0) or apply standardization if needed
# e.g., X_train_cae = scaler_for_images.fit_transform(X_train_cae.reshape(...)).reshape(...)

##############################################################################
# 1. Define the architecture
##############################################################################
latent_dim = X_train_pca.shape[1]  # from Task 2

# ----- Encoder -----
encoder_input = layers.Input(shape=(32, 32, 1))

x = layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(encoder_input)
x = layers.MaxPooling2D(pool_size=2, padding='same')(x)  # shape -> (16,16,32)

x = layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(pool_size=2, padding='same')(x)  # shape -> (8,8,64)

x = layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(pool_size=2, padding='same')(x)  # shape -> (4,4,128)

# Flatten
x = layers.Flatten()(x)  # shape -> (4*4*128) = 2048 if 4x4 with 128 filters

# Dense layer to go down to the latent space
encoded = layers.Dense(latent_dim, activation='relu')(x)

# ----- Decoder -----
# We need to reverse the process in the decoder.
# First go from latent_dim -> shape that matches (4,4,128)
decoder_input = layers.Dense(4*4*128, activation='relu')(encoded)

x = layers.Reshape((4, 4, 128))(decoder_input)

x = layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, activation='relu',
                           padding='same', output_padding=1)(x)  # shape -> (8,8,128)

x = layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu',
                           padding='same', output_padding=1)(x)  # shape -> (16,16,64)

x = layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu',
                           padding='same', output_padding=1)(x)  # shape -> (32,32,32)

# Output = 1 channel (grayscale), linear or sigmoid (try both)
decoded = layers.Conv2DTranspose(filters=1, kernel_size=3, activation='sigmoid',
                                 padding='same')(x)

# Build the model
conv_autoencoder = models.Model(encoder_input, decoded)
conv_autoencoder.compile(optimizer='adam', loss='mse')
conv_autoencoder.summary()

##############################################################################
# 2. Train the deep Conv Autoencoder
##############################################################################
epochs = 50
batch_size = 128
history_convAE = conv_autoencoder.fit(
    X_train_cae, X_train_cae,
    epochs=epochs,
    batch_size=batch_size,
    shuffle=True,
    validation_data=(X_test_cae, X_test_cae)
)

##############################################################################
# 3. Compute reconstruction error
##############################################################################
# Use MSE on the test set
X_test_pred = conv_autoencoder.predict(X_test_cae)
reconstruction_error_convAE = np.mean((X_test_cae - X_test_pred)**2)
print("Conv Autoencoder Test Reconstruction Error:", reconstruction_error_convAE)

"""### Single-layer linear AE (tied weights) from Task 2"""

X_test_pred_linear = autoencoder.predict(X_test_scaled)  # autoencoder from Task 2
# MSE across entire test set
reconstruction_error_linear = np.mean((X_test_scaled - X_test_pred_linear)**2)
print("Single-layer linear AE Reconstruction Error:", reconstruction_error_linear)

"""### Single hidden layer AE with Sigmoid (encoder) -> Linear (decoder)"""

input_ae = layers.Input(shape=(input_dim,))
hidden_nodes = X_train_pca.shape[1]
encoded = layers.Dense(hidden_nodes, activation='sigmoid')(input_ae)
decoded = layers.Dense(input_dim, activation='linear')(encoded)

single_hidden_ae = models.Model(input_ae, decoded)
single_hidden_ae.compile(optimizer='adam', loss='mse')
single_hidden_ae.fit(
    X_train_scaled, X_train_scaled,
    epochs=50,
    batch_size=256,
    validation_data=(X_test_scaled, X_test_scaled),
    verbose=1
    )
X_test_pred_single_hidden = single_hidden_ae.predict(X_test_scaled)
reconstruction_error_single_hidden = np.mean((X_test_scaled - X_test_pred_single_hidden)**2)
print("Single Hidden AE Reconstruction Error:", reconstruction_error_single_hidden)

"""### 3 hidden layer AE with Sigmoid (encoder) and Linear (final decoder)"""

input_3layers = layers.Input(shape=(input_dim,))
x = layers.Dense(100, activation='sigmoid')(input_3layers)
x = layers.Dense(100, activation='sigmoid')(x)
x = layers.Dense(100, activation='sigmoid')(x)
decoded_3layers = layers.Dense(input_dim, activation='linear')(x)

deep_ae_3layers = models.Model(input_3layers, decoded_3layers)
deep_ae_3layers.compile(optimizer='adam', loss='mse')
deep_ae_3layers.fit(
    X_train_scaled, X_train_scaled,
    epochs=50,
    batch_size=256,
    validation_data=(X_test_scaled, X_test_scaled),
    verbose=1
)
# Reconstruction error
X_test_pred_3layers = deep_ae_3layers.predict(X_test_scaled)
reconstruction_error_3layers = np.mean((X_test_scaled - X_test_pred_3layers)**2)
print("3-hidden-layer AE Reconstruction Error:", reconstruction_error_3layers)

print("Reconstruction Errors for Different Autoencoders:")
print("---------------------------------------")
print(f"Single-layer linear AE (tied weights): {reconstruction_error_linear:.6f}")
print(f"Single-hidden-layer AE (sigmoid->lin): {reconstruction_error_single_hidden:.6f}")
print(f"3-hidden-layer AE (sigmoid->lin):      {reconstruction_error_3layers:.6f}")
print(f"Deep Conv AE:                          {reconstruction_error_convAE:.6f}")

print("""
Observations:
- The deep convolutional AE typically captures spatial structure better, potentially yielding
  lower reconstruction error (depending on training).
- The single-layer linear AE might have higher error if the data is complex.
- The multi-layer (3 hidden layers) AE can capture more non-linearities than a single hidden layer,
  so we might see a difference in reconstruction error.
""")

"""## Task 4:

Train a deep convolutional autoencoder on the MNIST dataset (instead of CIFAR-10).
Using the encoded features from that autoencoder, train an MLP classifier with 7 outputs, corresponding to the 7 segments of a typical LED display.
For each digit (0–9), you have a target 7-segment pattern. Generate the confusion matrix for the test set predictions vs. the true 7-segment labels.
"""

# ----- 1. Load MNIST -----
(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()

# Scale to [0,1], reshape to (N, 28, 28, 1)
X_train_mnist = X_train_mnist.astype('float32') / 255.
X_test_mnist  = X_test_mnist.astype('float32')  / 255.

X_train_mnist = np.expand_dims(X_train_mnist, axis=-1)  # (N, 28,28,1)
X_test_mnist  = np.expand_dims(X_test_mnist, axis=-1)

# ----- 2. Build a deep conv AE -----
latent_dim_mnist = 32

input_img_mnist = layers.Input(shape=(28, 28, 1))
x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img_mnist)
x = layers.MaxPooling2D((2,2), padding='same')(x)  # -> (14,14,32)

x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)
x = layers.MaxPooling2D((2,2), padding='same')(x)  # -> (7,7,64)

x = layers.Flatten()(x)                              # -> shape (7*7*64) = 3136
encoded_mnist = layers.Dense(latent_dim_mnist, activation='relu')(x)

# Decoder
decoder_input_mnist = layers.Dense(7*7*64, activation='relu')(encoded_mnist)
x = layers.Reshape((7,7,64))(decoder_input_mnist)

x = layers.Conv2DTranspose(64, (3,3), strides=2, activation='relu',
                           padding='same', output_padding=1)(x)  # -> (14,14,64)
x = layers.Conv2DTranspose(32, (3,3), strides=2, activation='relu',
                           padding='same', output_padding=1)(x)  # -> (28,28,32)
decoded_mnist = layers.Conv2DTranspose(1, (3,3), activation='sigmoid',
                                       padding='same')(x)

mnist_autoencoder = models.Model(input_img_mnist, decoded_mnist)
mnist_autoencoder.compile(optimizer='adam', loss='mse')
mnist_autoencoder.summary()

# Training
history_mnist = mnist_autoencoder.fit(
    X_train_mnist, X_train_mnist,
    epochs=20,
    batch_size=256,
    validation_data=(X_test_mnist, X_test_mnist)
)

# Extract encoder model for feature extraction
encoder_mnist = models.Model(input_img_mnist, encoded_mnist)

# Example of extracting features
X_train_mnist_features = encoder_mnist.predict(X_train_mnist)
X_test_mnist_features = encoder_mnist.predict(X_test_mnist)

"""### MNIST labels (y_train_mnist, y_test_mnist) to 7-segment labels:"""

digit_to_7seg = {
    0: [1,1,1,1,1,1,0],
    1: [0,1,1,0,0,0,0],
    2: [1,1,0,1,1,0,1],
    3: [1,1,1,1,0,0,1],
    4: [0,1,1,0,0,1,1],
    5: [1,0,1,1,0,1,1],
    6: [1,0,1,1,1,1,1],
    7: [1,1,1,0,0,0,0],
    8: [1,1,1,1,1,1,1],
    9: [1,1,1,1,0,1,1],
}
def digit_to_seg_label(digit):
    return digit_to_7seg[digit]

y_train_7seg = np.array([digit_to_seg_label(d) for d in y_train_mnist])
y_test_7seg  = np.array([digit_to_seg_label(d) for d in y_test_mnist])

"""## MLP on the encoded features to predict 7-segment outputs"""

input_dim_feats = latent_dim_mnist
mlp_input = layers.Input(shape=(input_dim_feats,))
x = layers.Dense(64, activation='relu')(mlp_input)
x = layers.Dense(32, activation='relu')(x)
seg_outputs = layers.Dense(7, activation='sigmoid')(x)

seg_model = models.Model(mlp_input, seg_outputs)
seg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
seg_model.summary()

seg_model.fit(
    X_train_mnist_features, y_train_7seg,
    epochs=20,
    batch_size=256,
    validation_data=(X_test_mnist_features, y_test_7seg)
)

"""## Generate a confusion Matrix"""

# 1) Predict on test features
y_pred_7seg = seg_model.predict(X_test_mnist_features)
# 2) Binarize predictions at 0.5
y_pred_7seg_bin = (y_pred_7seg > 0.5).astype(int)

# 3) Map the 7-seg vector back to a digit
def seg_to_digit(seg_vec):
    # seg_vec is array of shape (7,) e.g. [1,1,1,1,1,1,0]
    # naive approach: loop over digit_to_7seg
    for digit, seg_pattern in digit_to_7seg.items():
        if np.array_equal(seg_vec, seg_pattern):
            return digit
    return -1

y_pred_digits = np.array([seg_to_digit(vec) for vec in y_pred_7seg_bin])

# Now we have predicted digits vs. true digits (y_test_mnist).
cm = confusion_matrix(y_test_mnist, y_pred_digits, labels=[0,1,2,3,4,5,6,7,8,9])
print("Confusion Matrix:\n", cm)

# -----------------------------
# 1. Calculate accuracy from the confusion matrix
# -----------------------------
y_pred_digits = np.array([seg_to_digit(vec) for vec in y_pred_7seg_bin])
cm = confusion_matrix(y_test_mnist, y_pred_digits, labels=[0,1,2,3,4,5,6,7,8,9])
acc_7seg = accuracy_score(y_test_mnist, y_pred_digits)
print("Confusion Matrix:\n", cm)
print("Overall digit classification accuracy (via 7-seg MLP):", acc_7seg)

# -----------------------------
# 2. Display a few test samples and their predicted 7-seg
# -----------------------------

def visualize_7seg_predictions(num_samples=5): # We can change this if needed
    sample_indices = random.sample(range(len(X_test_mnist)), num_samples)
    plt.figure(figsize=(12, 2 * num_samples))
    for i, idx in enumerate(sample_indices):
        img = X_test_mnist[idx].squeeze()  # shape (28,28)
        true_digit = y_test_mnist[idx]
        pred_digit = y_pred_digits[idx]

        plt.subplot(num_samples, 2, 2*i + 1)
        plt.imshow(img, cmap='gray')
        plt.title(f"True digit: {true_digit}")
        plt.axis('off')

        plt.subplot(num_samples, 2, 2*i + 2)
        plt.bar(range(7), y_pred_7seg_bin[idx])
        plt.ylim(0, 1)
        plt.title(f"Pred seg -> digit {pred_digit}")
        plt.xticks(range(7))
    plt.tight_layout()
    plt.show()

visualize_7seg_predictions(5)

"""### Observations:

We achieve 95.3% accuracy in mapping digits to 7-segment codes.
"""

