{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8741062c-0e69-43f0-9487-6889c8513b77",
      "metadata": {
        "id": "8741062c-0e69-43f0-9487-6889c8513b77"
      },
      "source": [
        "### Group ID:\n",
        "### Group Members Name with Student ID:\n",
        "1. Student 1\n",
        "2. Student 2\n",
        "3. Student 3\n",
        "4. Student 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebde8947-7ed3-4347-9ab9-04ce825100a7",
      "metadata": {
        "id": "ebde8947-7ed3-4347-9ab9-04ce825100a7"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "The objective of the problem is to implement an Actor-Critic reinforcement learning algorithm to optimize energy consumption in a building. The agent should learn to adjust the temperature settings dynamically to minimize energy usage while maintaining comfortable indoor conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29da66c9-875c-4994-80da-376e6da938ce",
      "metadata": {
        "id": "29da66c9-875c-4994-80da-376e6da938ce"
      },
      "source": [
        "#### Dataset Details\n",
        "Dataset: https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction\n",
        "\n",
        "This dataset contains energy consumption data for a residential building, along with various environmental and operational factors.\n",
        "\n",
        "Data Dictionary:\n",
        "* Appliances:       Energy use in Wh\n",
        "* lights:           Energy use of light fixtures in the house in Wh\n",
        "* T1 - T9:          Temperatures in various rooms and outside\n",
        "* RH_1 to RH_9:     Humidity measurements in various rooms and outside\n",
        "* Visibility:       Visibility in km\n",
        "* Tdewpoint:       Dew point temperature\n",
        "* Pressure_mm_hgg:  Pressure in mm Hg\n",
        "* Windspeed:        Wind speed in m/s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294b0609-22d6-4453-b23d-2de22a5241bd",
      "metadata": {
        "id": "294b0609-22d6-4453-b23d-2de22a5241bd"
      },
      "source": [
        "#### Environment Details\n",
        "**State Space:**\n",
        "The state space consists of various features from the dataset that impact energy consumption and comfort levels.\n",
        "\n",
        "* Current Temperature (T1 to T9): Temperatures in various rooms and outside.\n",
        "* Current Humidity (RH_1 to RH_9): Humidity measurements in different locations.\n",
        "* Visibility (Visibility): Visibility in meters.\n",
        "* Dew Point (Tdewpoint): Dew point temperature.\n",
        "* Pressure (Press_mm_hg): Atmospheric pressure in mm Hg.\n",
        "* Windspeed (Windspeed): Wind speed in m/s.\n",
        "\n",
        "Total State Vector Dimension: Number of features = 9 (temperature) + 9 (humidity) + 1 (visibility) + 1 (dew point) + 1 (pressure) + 1 (windspeed) = 22 features\n",
        "\n",
        "**Target Variable:** Appliances (energy consumption in Wh).\n",
        "\n",
        "**Action Space:**\n",
        "The action space consists of discrete temperature adjustments:\n",
        "* Action 0: Decrease temperature by 1°C\n",
        "* Action 1: Maintain current temperature\n",
        "* Action 2: Increase temperature by 1°C\n",
        "\n",
        "\n",
        "- If the action is to decrease the temperature by 1°C, you'll adjust each temperature feature (T1 to T9) down by 1°C.\n",
        "- If the action is to increase the temperature by 1°C, you'll adjust each temperature feature (T1 to T9) up by 1°C.\n",
        "- Other features remain unchanged.\n",
        "\n",
        "**Policy (Actor):** A neural network that outputs a probability distribution over possible temperature adjustments.\n",
        "\n",
        "**Value function (Critic):** A neural network that estimates the expected cumulative reward (energy savings) from a given state.\n",
        "\n",
        "**Reward function:**\n",
        "The reward function should reflect the overall comfort and energy efficiency based on all temperature readings. i.e., balance between minimising temperature deviations and minimizing energy consumption.\n",
        "\n",
        "* Calculate the penalty based on the deviation of each temperature from the target temperature and then aggregate these penalties.\n",
        "* Measure the change in energy consumption before and after applying the RL action.\n",
        "* Combine the comfort penalty and energy savings to get the final reward.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Target temperature=22°C\n",
        "\n",
        "Initial Temperatures: T1=23, T2=22, T3=21, T4=23, T5=22, T6=21, T7=24, T8=22, T9=23\n",
        "\n",
        "Action Taken: Decrease temperature by 1°C for each room\n",
        "\n",
        "Resulting Temperatures: T1 = 22, T2 = 21, T3 = 20, T4 = 22, T5 = 21, T6 = 20, T7 = 23, T8 = 21, T9 = 22\n",
        "\n",
        "Energy Consumption: 50 Wh (before RL adjustment) and 48 Wh (after RL adjustment)\n",
        "* Energy Before (50 Wh): Use the energy consumption from the dataset at the current time step.\n",
        "* Energy After (48 Wh): Use the energy consumption from the dataset at the next time step (if available).\n",
        "\n",
        "Consider only temperature features for deviation calculation.\n",
        "\n",
        "Deviation = abs (Ti− Ttarget )\n",
        "\n",
        "Deviations=[ abs(22−22), abs(21−22), abs(20−22), abs(22−22),  abs(21−22), abs(20−22), abs(23−22), abs(21−22), abs(22−22) ]\n",
        "\n",
        "Deviations = [0, 1, 2, 0, 1, 2, 1, 1, 0], Sum of deviations = 8\n",
        "\n",
        "Energy Savings = Energy Before−Energy After = 50 – 48 = 2Wh\n",
        "\n",
        "Reward= −Sum of Deviations + Energy Savings = -8+6 = -2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95be925",
      "metadata": {
        "id": "a95be925"
      },
      "source": [
        "#### Expected Outcomes\n",
        "1. Pre-process the dataset to handle any missing values and create training and testing sets.\n",
        "2. Implement the Actor-Critic algorithm using TensorFlow.\n",
        "3. Train the model over 500 episodes to minimize energy consumption while maintaining an indoor temperature of 22°C.\n",
        "4. Plot the total reward obtained in each episode to evaluate the learning progress.\n",
        "5. Evaluate the performance of the model on test set to measure its performance\n",
        "6. Provide graphs showing the convergence of the Actor and Critic losses.\n",
        "7. Plot the learned policy by showing the action probabilities across different state values (e.g., temperature settings).\n",
        "8. Provide an analysis on a comparison of the energy consumption before and after applying the reinforcement learning algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb68ad3",
      "metadata": {
        "id": "4fb68ad3"
      },
      "source": [
        "#### Code Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b497301",
      "metadata": {
        "id": "5b497301"
      },
      "outputs": [],
      "source": [
        "#### Load the dataset\n",
        "data=pd.read_csv(file_path)\n",
        "\n",
        "# Check and replace missing values\n",
        "# Pre process the data set to get the features and target and scale them\n",
        "\n",
        "features = [  ]\n",
        "target=[  ]\n",
        "\n",
        "X=data[features]\n",
        "y=data[target]\n",
        "\n",
        "# Normalize them with Standard Scaler\n",
        "\n",
        "# Split the data to training and testing sets (80% for training, 20% for testing)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(test_size=0.2,random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9f6b9c",
      "metadata": {
        "id": "9e9f6b9c"
      },
      "source": [
        "### Create an EnergyConsumption Environment (1 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00acc2a9",
      "metadata": {
        "id": "00acc2a9"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27777860",
      "metadata": {
        "id": "27777860"
      },
      "source": [
        "### Print state space and action space (0.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "005044e9",
      "metadata": {
        "id": "005044e9"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf34260",
      "metadata": {
        "id": "3bf34260"
      },
      "source": [
        "### Clearly define the parameters used for training an AI agent. (1 M)\n",
        "* Number of episodes\n",
        "* Max capacity of replay memory\n",
        "* Batch size\n",
        "* Period of Q target network updates\n",
        "* Discount factor for future rewards\n",
        "* Initial value for epsilon of the e-greedy\n",
        "* Final value for epsilon of the e-greedy\n",
        "* Learning rate of ADAM optimizer, and etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J59AEDG7gy1W"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ],
      "id": "J59AEDG7gy1W"
    },
    {
      "cell_type": "markdown",
      "id": "8330a3c2",
      "metadata": {
        "id": "8330a3c2"
      },
      "source": [
        "#### Define the separate functions for DecreaseTemperature, IncreaseTemperature and MaintainCurrentTemperature actions. (1.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e974279",
      "metadata": {
        "id": "1e974279"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae48b2f0",
      "metadata": {
        "id": "ae48b2f0"
      },
      "source": [
        "#### Implement a replay buffer for storing the experiences. (0.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6c0a8b",
      "metadata": {
        "id": "5a6c0a8b"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design network DQN (0.5 M)"
      ],
      "metadata": {
        "id": "SwvZUqIuer7Q"
      },
      "id": "SwvZUqIuer7Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d262ae2d",
      "metadata": {
        "id": "d262ae2d"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Iterations (0.5 M)\n",
        "\n",
        "***Note: print all the episodes with the values of investment and buying. (if not printed then -1 will be done.)***"
      ],
      "metadata": {
        "id": "xzis8NiEe21A"
      },
      "id": "xzis8NiEe21A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc20a76c",
      "metadata": {
        "id": "fc20a76c"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design network DDQN (0.5 M)"
      ],
      "metadata": {
        "id": "Yg7CGz6Gh2JS"
      },
      "id": "Yg7CGz6Gh2JS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK4zBM7fh2JS"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ],
      "id": "TK4zBM7fh2JS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Iterations (0.5 M)\n",
        "\n",
        "***Note: print all the episodes with the values of investment and buying. (if not printed then -1 will be done.)***"
      ],
      "metadata": {
        "id": "24Gey_bAh2JT"
      },
      "id": "24Gey_bAh2JT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2-Y_lCLh2JT"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ],
      "id": "T2-Y_lCLh2JT"
    },
    {
      "cell_type": "markdown",
      "id": "93cee383",
      "metadata": {
        "id": "93cee383"
      },
      "source": [
        "### Plot the graph for agents for decreasing, increasing, and maintaining the temperature for DQN and DDQN together. (0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc218d2a",
      "metadata": {
        "id": "dc218d2a"
      },
      "outputs": [],
      "source": [
        "### write your code below this line"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclude your assignment with your analysis consisting of at least 200 words by summarizing your findings for agent’s behaviour using Actor-Critic, DQN and DDQN techniques for optimizing the energy consumption. (1 M)"
      ],
      "metadata": {
        "id": "svqK0auGiP1A"
      },
      "id": "svqK0auGiP1A"
    },
    {
      "cell_type": "code",
      "source": [
        "### write your code below this line"
      ],
      "metadata": {
        "id": "N9_1QzS9iVRA"
      },
      "id": "N9_1QzS9iVRA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}