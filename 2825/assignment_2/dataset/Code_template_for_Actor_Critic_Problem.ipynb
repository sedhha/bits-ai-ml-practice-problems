{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8741062c-0e69-43f0-9487-6889c8513b77",
      "metadata": {
        "id": "8741062c-0e69-43f0-9487-6889c8513b77"
      },
      "source": [
        "### Group ID:\n",
        "### Group Members Name with Student ID:\n",
        "1. Student 1\n",
        "2. Student 2\n",
        "3. Student 3\n",
        "4. Student 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebde8947-7ed3-4347-9ab9-04ce825100a7",
      "metadata": {
        "id": "ebde8947-7ed3-4347-9ab9-04ce825100a7"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "The objective of the problem is to implement an Actor-Critic reinforcement learning algorithm to optimize energy consumption in a building. The agent should learn to adjust the temperature settings dynamically to minimize energy usage while maintaining comfortable indoor conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29da66c9-875c-4994-80da-376e6da938ce",
      "metadata": {
        "id": "29da66c9-875c-4994-80da-376e6da938ce"
      },
      "source": [
        "#### Dataset Details\n",
        "Dataset: https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction\n",
        "\n",
        "This dataset contains energy consumption data for a residential building, along with various environmental and operational factors.\n",
        "\n",
        "Data Dictionary:\n",
        "* Appliances:       Energy use in Wh\n",
        "* lights:           Energy use of light fixtures in the house in Wh\n",
        "* T1 - T9:          Temperatures in various rooms and outside\n",
        "* RH_1 to RH_9:     Humidity measurements in various rooms and outside\n",
        "* Visibility:       Visibility in km\n",
        "* Tdewpoint:       Dew point temperature\n",
        "* Pressure_mm_hgg:  Pressure in mm Hg\n",
        "* Windspeed:        Wind speed in m/s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294b0609-22d6-4453-b23d-2de22a5241bd",
      "metadata": {
        "id": "294b0609-22d6-4453-b23d-2de22a5241bd"
      },
      "source": [
        "#### Environment Details\n",
        "**State Space:**\n",
        "The state space consists of various features from the dataset that impact energy consumption and comfort levels.\n",
        "\n",
        "* Current Temperature (T1 to T9): Temperatures in various rooms and outside.\n",
        "* Current Humidity (RH_1 to RH_9): Humidity measurements in different locations.\n",
        "* Visibility (Visibility): Visibility in meters.\n",
        "* Dew Point (Tdewpoint): Dew point temperature.\n",
        "* Pressure (Press_mm_hg): Atmospheric pressure in mm Hg.\n",
        "* Windspeed (Windspeed): Wind speed in m/s.\n",
        "\n",
        "Total State Vector Dimension: Number of features = 9 (temperature) + 9 (humidity) + 1 (visibility) + 1 (dew point) + 1 (pressure) + 1 (windspeed) = 22 features\n",
        "\n",
        "**Target Variable:** Appliances (energy consumption in Wh).\n",
        "\n",
        "**Action Space:**\n",
        "The action space consists of discrete temperature adjustments:\n",
        "* Action 0: Decrease temperature by 1°C\n",
        "* Action 1: Maintain current temperature\n",
        "* Action 2: Increase temperature by 1°C\n",
        "\n",
        "\n",
        "- If the action is to decrease the temperature by 1°C, you'll adjust each temperature feature (T1 to T9) down by 1°C.\n",
        "- If the action is to increase the temperature by 1°C, you'll adjust each temperature feature (T1 to T9) up by 1°C.\n",
        "- Other features remain unchanged.\n",
        "\n",
        "**Policy (Actor):** A neural network that outputs a probability distribution over possible temperature adjustments.\n",
        "\n",
        "**Value function (Critic):** A neural network that estimates the expected cumulative reward (energy savings) from a given state.\n",
        "\n",
        "**Reward function:**\n",
        "The reward function should reflect the overall comfort and energy efficiency based on all temperature readings. i.e., balance between minimising temperature deviations and minimizing energy consumption.\n",
        "\n",
        "* Calculate the penalty based on the deviation of each temperature from the target temperature and then aggregate these penalties.\n",
        "* Measure the change in energy consumption before and after applying the RL action.\n",
        "* Combine the comfort penalty and energy savings to get the final reward.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Target temperature=22°C\n",
        "\n",
        "Initial Temperatures: T1=23, T2=22, T3=21, T4=23, T5=22, T6=21, T7=24, T8=22, T9=23\n",
        "\n",
        "Action Taken: Decrease temperature by 1°C for each room\n",
        "\n",
        "Resulting Temperatures: T1 = 22, T2 = 21, T3 = 20, T4 = 22, T5 = 21, T6 = 20, T7 = 23, T8 = 21, T9 = 22\n",
        "\n",
        "Energy Consumption: 50 Wh (before RL adjustment) and 48 Wh (after RL adjustment)\n",
        "* Energy Before (50 Wh): Use the energy consumption from the dataset at the current time step.\n",
        "* Energy After (48 Wh): Use the energy consumption from the dataset at the next time step (if available).\n",
        "\n",
        "Consider only temperature features for deviation calculation.\n",
        "\n",
        "Deviation = abs (Ti− Ttarget )\n",
        "\n",
        "Deviations=[ abs(22−22), abs(21−22), abs(20−22), abs(22−22),  abs(21−22), abs(20−22), abs(23−22), abs(21−22), abs(22−22) ]\n",
        "\n",
        "Deviations = [0, 1, 2, 0, 1, 2, 1, 1, 0], Sum of deviations = 8\n",
        "\n",
        "Energy Savings = Energy Before−Energy After = 50 – 48 = 2Wh\n",
        "\n",
        "Reward= −Sum of Deviations + Energy Savings = -8+6 = -2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95be925",
      "metadata": {
        "id": "a95be925"
      },
      "source": [
        "#### Expected Outcomes\n",
        "1. Pre-process the dataset to handle any missing values and create training and testing sets.\n",
        "2. Implement the Actor-Critic algorithm using TensorFlow.\n",
        "3. Train the model over 500 episodes to minimize energy consumption while maintaining an indoor temperature of 22°C.\n",
        "4. Plot the total reward obtained in each episode to evaluate the learning progress.\n",
        "5. Evaluate the performance of the model on test set to measure its performance\n",
        "6. Provide graphs showing the convergence of the Actor and Critic losses.\n",
        "7. Plot the learned policy by showing the action probabilities across different state values (e.g., temperature settings).\n",
        "8. Provide an analysis on a comparison of the energy consumption before and after applying the reinforcement learning algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb68ad3",
      "metadata": {
        "id": "4fb68ad3"
      },
      "source": [
        "#### Code Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b497301",
      "metadata": {
        "id": "5b497301"
      },
      "outputs": [],
      "source": [
        "#### Load the dataset\n",
        "data=pd.read_csv(file_path)\n",
        "\n",
        "# Check and replace missing values\n",
        "# Pre process the data set to get the features and target and scale them\n",
        "\n",
        "features = [  ]\n",
        "target=[  ]\n",
        "\n",
        "X=data[features]\n",
        "y=data[target]\n",
        "\n",
        "# Normalize them with Standard Scaler\n",
        "\n",
        "# Split the data to training and testing sets (80% for training, 20% for testing)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(test_size=0.2,random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9f6b9c",
      "metadata": {
        "id": "9e9f6b9c"
      },
      "source": [
        "#### Defining Actor Critic Model using tensorflow (1 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00acc2a9",
      "metadata": {
        "id": "00acc2a9"
      },
      "outputs": [],
      "source": [
        "### Define Actor Model\n",
        "\n",
        "def build_actor_model():\n",
        "\n",
        "    # define the NN model to give probability distribution over actions\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "### Define Critic Model\n",
        "\n",
        "def build_critic_model():\n",
        "\n",
        "    # define the NN model for value function estimation\n",
        "\n",
        "    return model\n",
        "\n",
        "state_space = 22\n",
        "action = 3  # Decrease, Maintain, Increase\n",
        "\n",
        "actor_model = build_actor_model()\n",
        "critic_model = build_critic_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27777860",
      "metadata": {
        "id": "27777860"
      },
      "source": [
        "### Reward Function (0.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "005044e9",
      "metadata": {
        "id": "005044e9"
      },
      "outputs": [],
      "source": [
        "### Calculate Reward Function\n",
        "\n",
        "def calculate_reward():\n",
        "\n",
        "    # consider only temperature features for deviation calculation with target temperature as 22C\n",
        "    # calculate energy savings by taking difference between energy before and after\n",
        "    # calculate and return the reward\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf34260",
      "metadata": {
        "id": "3bf34260"
      },
      "source": [
        "#### Environment Simulation (0.5 M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86b7850",
      "metadata": {
        "id": "e86b7850"
      },
      "outputs": [],
      "source": [
        "### Environment Simulation\n",
        "\n",
        "def simulate_environment():\n",
        "\n",
        "    temp_adjustment = 1\n",
        "    # Increase of decrease each temperature by 1C\n",
        "\n",
        "    # get the energy before from current index\n",
        "    # get the energy after from next index\n",
        "\n",
        "    # get the respective reward\n",
        "\n",
        "    return next_state,reward"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8330a3c2",
      "metadata": {
        "id": "8330a3c2"
      },
      "source": [
        "#### Implementation of Training Function (2 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e974279",
      "metadata": {
        "id": "1e974279"
      },
      "outputs": [],
      "source": [
        "# Train the Actor-Critic models\n",
        "\n",
        "def train_function(episodes=500):\n",
        "\n",
        "# for each episode:\n",
        "    # get the action probabilities\n",
        "    # chose the action with highest probability\n",
        "\n",
        "    # similate the environment with the chosen action\n",
        "\n",
        "    # store results\n",
        "\n",
        "    # update the state\n",
        "\n",
        "# Compute critic target values with discount factor and rewards and the next values obtained from critic model for next state\n",
        "\n",
        "# Update Critic model, capture critic loss\n",
        "\n",
        "# Calculate advantages\n",
        "\n",
        "# Update Actor Model, capture actor loss\n",
        "\n",
        "# Print the mean reward of all states for each episode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae48b2f0",
      "metadata": {
        "id": "ae48b2f0"
      },
      "source": [
        "#### Evaluate the performance of the model on test set (0.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6c0a8b",
      "metadata": {
        "id": "5a6c0a8b"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "\n",
        "def evaluate_model():\n",
        "\n",
        "    # predict the action and simulate the environment accordingly and get the respective next state\n",
        "\n",
        "    # calculate rewards for test set\n",
        "\n",
        "\n",
        "\n",
        "# Print the total reward obtained on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SwvZUqIuer7Q",
      "metadata": {
        "id": "SwvZUqIuer7Q"
      },
      "source": [
        "### Plot the convergence of Actor and Critic losses (1 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d262ae2d",
      "metadata": {
        "id": "d262ae2d"
      },
      "outputs": [],
      "source": [
        "# Plot the convergence of Actor and Critic losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xzis8NiEe21A",
      "metadata": {
        "id": "xzis8NiEe21A"
      },
      "source": [
        "### Plot the learned policy - by showing the action probabilities across different state values (1 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc20a76c",
      "metadata": {
        "id": "fc20a76c"
      },
      "outputs": [],
      "source": [
        "# Plot the learned policy - by showing the action probabilities across different state values\n",
        "\n",
        "# From the trained actor model, for each state in training set,\n",
        "# plot the probability of each action (increasing/decreasing/maintaining) the temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93cee383",
      "metadata": {
        "id": "93cee383"
      },
      "source": [
        "#### Conclusion (0.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc218d2a",
      "metadata": {
        "id": "dc218d2a"
      },
      "outputs": [],
      "source": [
        "# Provide an analysis on a comparison of the energy consumption\n",
        "# before and after applying the reinforcement learning algorithm."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
