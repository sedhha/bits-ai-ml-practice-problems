{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8741062c-0e69-43f0-9487-6889c8513b77",
      "metadata": {
        "id": "8741062c-0e69-43f0-9487-6889c8513b77"
      },
      "source": [
        "### Group ID: 134\n",
        "### Group Members Name with Student ID:\n",
        "\n",
        "| Student Name       | Student ID    | Contribution |\n",
        "|--------------------|---------------|--------------|\n",
        "| Chakshu            | 2023aa05280   | 100%         |\n",
        "| Gali Jahnavi       | 2023aa05684   | 100%         |\n",
        "| Aashaank Pratap    | 2023aa05023   | 100%         |\n",
        "| Shivam Sahil       | 2023aa05663   | 100%         |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebde8947-7ed3-4347-9ab9-04ce825100a7",
      "metadata": {
        "id": "ebde8947-7ed3-4347-9ab9-04ce825100a7"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "The objective of the problem is to implement an Actor-Critic reinforcement learning algorithm to optimize energy consumption in a building. The agent should learn to adjust the temperature settings dynamically to minimize energy usage while maintaining comfortable indoor conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29da66c9-875c-4994-80da-376e6da938ce",
      "metadata": {
        "id": "29da66c9-875c-4994-80da-376e6da938ce"
      },
      "source": [
        "#### Dataset Details\n",
        "Dataset: https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction\n",
        "\n",
        "This dataset contains energy consumption data for a residential building, along with various environmental and operational factors.\n",
        "\n",
        "Data Dictionary:\n",
        "* Appliances:       Energy use in Wh\n",
        "* lights:           Energy use of light fixtures in the house in Wh\n",
        "* T1 - T9:          Temperatures in various rooms and outside\n",
        "* RH_1 to RH_9:     Humidity measurements in various rooms and outside\n",
        "* Visibility:       Visibility in km\n",
        "* Tdewpoint:       Dew point temperature\n",
        "* Pressure_mm_hgg:  Pressure in mm Hg\n",
        "* Windspeed:        Wind speed in m/s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294b0609-22d6-4453-b23d-2de22a5241bd",
      "metadata": {
        "id": "294b0609-22d6-4453-b23d-2de22a5241bd"
      },
      "source": [
        "#### Environment Details\n",
        "**State Space:**\n",
        "The state space consists of various features from the dataset that impact energy consumption and comfort levels.\n",
        "\n",
        "* Current Temperature (T1 to T9): Temperatures in various rooms and outside.\n",
        "* Current Humidity (RH_1 to RH_9): Humidity measurements in different locations.\n",
        "* Visibility (Visibility): Visibility in meters.\n",
        "* Dew Point (Tdewpoint): Dew point temperature.\n",
        "* Pressure (Press_mm_hg): Atmospheric pressure in mm Hg.\n",
        "* Windspeed (Windspeed): Wind speed in m/s.\n",
        "\n",
        "Total State Vector Dimension: Number of features = 9 (temperature) + 9 (humidity) + 1 (visibility) + 1 (dew point) + 1 (pressure) + 1 (windspeed) = 22 features\n",
        "\n",
        "**Target Variable:** Appliances (energy consumption in Wh).\n",
        "\n",
        "**Action Space:**\n",
        "The action space consists of discrete temperature adjustments:\n",
        "* Action 0: Decrease temperature by 1°C\n",
        "* Action 1: Maintain current temperature\n",
        "* Action 2: Increase temperature by 1°C\n",
        "\n",
        "\n",
        "- If the action is to decrease the temperature by 1°C, you'll adjust each temperature feature (T1 to T9) down by 1°C.\n",
        "- If the action is to increase the temperature by 1°C, you'll adjust each temperature feature (T1 to T9) up by 1°C.\n",
        "- Other features remain unchanged.\n",
        "\n",
        "**Policy (Actor):** A neural network that outputs a probability distribution over possible temperature adjustments.\n",
        "\n",
        "**Value function (Critic):** A neural network that estimates the expected cumulative reward (energy savings) from a given state.\n",
        "\n",
        "**Reward function:**\n",
        "The reward function should reflect the overall comfort and energy efficiency based on all temperature readings. i.e., balance between minimising temperature deviations and minimizing energy consumption.\n",
        "\n",
        "* Calculate the penalty based on the deviation of each temperature from the target temperature and then aggregate these penalties.\n",
        "* Measure the change in energy consumption before and after applying the RL action.\n",
        "* Combine the comfort penalty and energy savings to get the final reward.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Target temperature=22°C\n",
        "\n",
        "Initial Temperatures: T1=23, T2=22, T3=21, T4=23, T5=22, T6=21, T7=24, T8=22, T9=23\n",
        "\n",
        "Action Taken: Decrease temperature by 1°C for each room\n",
        "\n",
        "Resulting Temperatures: T1 = 22, T2 = 21, T3 = 20, T4 = 22, T5 = 21, T6 = 20, T7 = 23, T8 = 21, T9 = 22\n",
        "\n",
        "Energy Consumption: 50 Wh (before RL adjustment) and 48 Wh (after RL adjustment)\n",
        "* Energy Before (50 Wh): Use the energy consumption from the dataset at the current time step.\n",
        "* Energy After (48 Wh): Use the energy consumption from the dataset at the next time step (if available).\n",
        "\n",
        "Consider only temperature features for deviation calculation.\n",
        "\n",
        "Deviation = abs (Ti− Ttarget )\n",
        "\n",
        "Deviations=[ abs(22−22), abs(21−22), abs(20−22), abs(22−22),  abs(21−22), abs(20−22), abs(23−22), abs(21−22), abs(22−22) ]\n",
        "\n",
        "Deviations = [0, 1, 2, 0, 1, 2, 1, 1, 0], Sum of deviations = 8\n",
        "\n",
        "Energy Savings = Energy Before−Energy After = 50 – 48 = 2Wh\n",
        "\n",
        "Reward= −Sum of Deviations + Energy Savings = -8+6 = -2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95be925",
      "metadata": {
        "id": "a95be925"
      },
      "source": [
        "#### Expected Outcomes\n",
        "1. Pre-process the dataset to handle any missing values and create training and testing sets.\n",
        "2. Implement the Actor-Critic algorithm using TensorFlow.\n",
        "3. Train the model over 500 episodes to minimize energy consumption while maintaining an indoor temperature of 22°C.\n",
        "4. Plot the total reward obtained in each episode to evaluate the learning progress.\n",
        "5. Evaluate the performance of the model on test set to measure its performance\n",
        "6. Provide graphs showing the convergence of the Actor and Critic losses.\n",
        "7. Plot the learned policy by showing the action probabilities across different state values (e.g., temperature settings).\n",
        "8. Provide an analysis on a comparison of the energy consumption before and after applying the reinforcement learning algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb68ad3",
      "metadata": {
        "id": "4fb68ad3"
      },
      "source": [
        "#### Code Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "5b497301",
      "metadata": {
        "id": "5b497301"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Appliances</th>\n",
              "      <th>lights</th>\n",
              "      <th>T1</th>\n",
              "      <th>RH_1</th>\n",
              "      <th>T2</th>\n",
              "      <th>RH_2</th>\n",
              "      <th>T3</th>\n",
              "      <th>RH_3</th>\n",
              "      <th>T4</th>\n",
              "      <th>...</th>\n",
              "      <th>T9</th>\n",
              "      <th>RH_9</th>\n",
              "      <th>T_out</th>\n",
              "      <th>Press_mm_hg</th>\n",
              "      <th>RH_out</th>\n",
              "      <th>Windspeed</th>\n",
              "      <th>Visibility</th>\n",
              "      <th>Tdewpoint</th>\n",
              "      <th>rv1</th>\n",
              "      <th>rv2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-11 17:00:00</td>\n",
              "      <td>60</td>\n",
              "      <td>30</td>\n",
              "      <td>19.89</td>\n",
              "      <td>47.596667</td>\n",
              "      <td>19.2</td>\n",
              "      <td>44.790000</td>\n",
              "      <td>19.79</td>\n",
              "      <td>44.730000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>17.033333</td>\n",
              "      <td>45.53</td>\n",
              "      <td>6.600000</td>\n",
              "      <td>733.5</td>\n",
              "      <td>92.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>5.3</td>\n",
              "      <td>13.275433</td>\n",
              "      <td>13.275433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-11 17:10:00</td>\n",
              "      <td>60</td>\n",
              "      <td>30</td>\n",
              "      <td>19.89</td>\n",
              "      <td>46.693333</td>\n",
              "      <td>19.2</td>\n",
              "      <td>44.722500</td>\n",
              "      <td>19.79</td>\n",
              "      <td>44.790000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>17.066667</td>\n",
              "      <td>45.56</td>\n",
              "      <td>6.483333</td>\n",
              "      <td>733.6</td>\n",
              "      <td>92.0</td>\n",
              "      <td>6.666667</td>\n",
              "      <td>59.166667</td>\n",
              "      <td>5.2</td>\n",
              "      <td>18.606195</td>\n",
              "      <td>18.606195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-11 17:20:00</td>\n",
              "      <td>50</td>\n",
              "      <td>30</td>\n",
              "      <td>19.89</td>\n",
              "      <td>46.300000</td>\n",
              "      <td>19.2</td>\n",
              "      <td>44.626667</td>\n",
              "      <td>19.79</td>\n",
              "      <td>44.933333</td>\n",
              "      <td>18.926667</td>\n",
              "      <td>...</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>45.50</td>\n",
              "      <td>6.366667</td>\n",
              "      <td>733.7</td>\n",
              "      <td>92.0</td>\n",
              "      <td>6.333333</td>\n",
              "      <td>55.333333</td>\n",
              "      <td>5.1</td>\n",
              "      <td>28.642668</td>\n",
              "      <td>28.642668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-11 17:30:00</td>\n",
              "      <td>50</td>\n",
              "      <td>40</td>\n",
              "      <td>19.89</td>\n",
              "      <td>46.066667</td>\n",
              "      <td>19.2</td>\n",
              "      <td>44.590000</td>\n",
              "      <td>19.79</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>18.890000</td>\n",
              "      <td>...</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>45.40</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>733.8</td>\n",
              "      <td>92.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>51.500000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>45.410389</td>\n",
              "      <td>45.410389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 17:40:00</td>\n",
              "      <td>60</td>\n",
              "      <td>40</td>\n",
              "      <td>19.89</td>\n",
              "      <td>46.333333</td>\n",
              "      <td>19.2</td>\n",
              "      <td>44.530000</td>\n",
              "      <td>19.79</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>18.890000</td>\n",
              "      <td>...</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>45.40</td>\n",
              "      <td>6.133333</td>\n",
              "      <td>733.9</td>\n",
              "      <td>92.0</td>\n",
              "      <td>5.666667</td>\n",
              "      <td>47.666667</td>\n",
              "      <td>4.9</td>\n",
              "      <td>10.084097</td>\n",
              "      <td>10.084097</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date  Appliances  lights     T1       RH_1    T2       RH_2  \\\n",
              "0  2016-01-11 17:00:00          60      30  19.89  47.596667  19.2  44.790000   \n",
              "1  2016-01-11 17:10:00          60      30  19.89  46.693333  19.2  44.722500   \n",
              "2  2016-01-11 17:20:00          50      30  19.89  46.300000  19.2  44.626667   \n",
              "3  2016-01-11 17:30:00          50      40  19.89  46.066667  19.2  44.590000   \n",
              "4  2016-01-11 17:40:00          60      40  19.89  46.333333  19.2  44.530000   \n",
              "\n",
              "      T3       RH_3         T4  ...         T9   RH_9     T_out  Press_mm_hg  \\\n",
              "0  19.79  44.730000  19.000000  ...  17.033333  45.53  6.600000        733.5   \n",
              "1  19.79  44.790000  19.000000  ...  17.066667  45.56  6.483333        733.6   \n",
              "2  19.79  44.933333  18.926667  ...  17.000000  45.50  6.366667        733.7   \n",
              "3  19.79  45.000000  18.890000  ...  17.000000  45.40  6.250000        733.8   \n",
              "4  19.79  45.000000  18.890000  ...  17.000000  45.40  6.133333        733.9   \n",
              "\n",
              "   RH_out  Windspeed  Visibility  Tdewpoint        rv1        rv2  \n",
              "0    92.0   7.000000   63.000000        5.3  13.275433  13.275433  \n",
              "1    92.0   6.666667   59.166667        5.2  18.606195  18.606195  \n",
              "2    92.0   6.333333   55.333333        5.1  28.642668  28.642668  \n",
              "3    92.0   6.000000   51.500000        5.0  45.410389  45.410389  \n",
              "4    92.0   5.666667   47.666667        4.9  10.084097  10.084097  \n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Necessary Libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#### Load the dataset\n",
        "data=pd.read_csv(r'dataset/energydata_complete.csv')\n",
        "# Display the first few rows of the dataset to inspect it\n",
        "data.head(), data.info(), data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5b7deffb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(date           0\n",
              " Appliances     0\n",
              " lights         0\n",
              " T1             0\n",
              " RH_1           0\n",
              " T2             0\n",
              " RH_2           0\n",
              " T3             0\n",
              " RH_3           0\n",
              " T4             0\n",
              " RH_4           0\n",
              " T5             0\n",
              " RH_5           0\n",
              " T6             0\n",
              " RH_6           0\n",
              " T7             0\n",
              " RH_7           0\n",
              " T8             0\n",
              " RH_8           0\n",
              " T9             0\n",
              " RH_9           0\n",
              " T_out          0\n",
              " Press_mm_hg    0\n",
              " RH_out         0\n",
              " Windspeed      0\n",
              " Visibility     0\n",
              " Tdewpoint      0\n",
              " rv1            0\n",
              " rv2            0\n",
              " dtype: int64,\n",
              " date           datetime64[ns]\n",
              " Appliances              int64\n",
              " lights                  int64\n",
              " T1                    float64\n",
              " RH_1                  float64\n",
              " T2                    float64\n",
              " RH_2                  float64\n",
              " T3                    float64\n",
              " RH_3                  float64\n",
              " T4                    float64\n",
              " RH_4                  float64\n",
              " T5                    float64\n",
              " RH_5                  float64\n",
              " T6                    float64\n",
              " RH_6                  float64\n",
              " T7                    float64\n",
              " RH_7                  float64\n",
              " T8                    float64\n",
              " RH_8                  float64\n",
              " T9                    float64\n",
              " RH_9                  float64\n",
              " T_out                 float64\n",
              " Press_mm_hg           float64\n",
              " RH_out                float64\n",
              " Windspeed             float64\n",
              " Visibility            float64\n",
              " Tdewpoint             float64\n",
              " rv1                   float64\n",
              " rv2                   float64\n",
              " dtype: object)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check and replace missing values\n",
        "data = data.fillna(data.mean())\n",
        "\n",
        "# Pre-process the dataset to get the features and target and scale them\n",
        "features = ['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', \n",
        "            'RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_5', 'RH_6', 'RH_7', 'RH_8', 'RH_9', \n",
        "            'Visibility', 'Tdewpoint', 'Press_mm_hg', 'Windspeed']\n",
        "target = ['Appliances']\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Normalize them with Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data to training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)\n",
        "\n",
        "print(\"Data preprocessing completed.\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e8ff9397",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre process the dataset to get the features and target and scale them\n",
        "# Selecting the features to be normalized\n",
        "features_to_normalize = data.columns.drop(['date', 'Appliances', 'lights'])  # Excluding date, Appliances, and lights\n",
        "\n",
        "# Applying Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "data[features_to_normalize] = scaler.fit_transform(data[features_to_normalize])\n",
        "\n",
        "# Define features and target variable\n",
        "features = data.drop(['Appliances', 'date'], axis=1)  # Exclude 'date' if it's not used as a feature\n",
        "target = data['Appliances']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9f6b9c",
      "metadata": {
        "id": "9e9f6b9c"
      },
      "source": [
        "#### Defining Actor Critic Model using tensorflow (1 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "00acc2a9",
      "metadata": {
        "id": "00acc2a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State space (number of features): 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shivamsahil/Downloads/bits/assignments/venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "### Define Actor Model\n",
        "\n",
        "data = data.drop(['date', 'Appliances'], axis=1)  # Dropping non-feature columns\n",
        "\n",
        "# Check the number of features now\n",
        "state_space = data.shape[1]  # This should be the actual number of features\n",
        "print(f\"State space (number of features): {state_space}\")\n",
        "\n",
        "### Redefine Actor and Critic Models to match the actual state space\n",
        "def build_actor_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(state_space,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(3, activation='softmax')  # Assuming 3 actions: Decrease, Maintain, Increase\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_critic_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(state_space,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)  # Output is the value estimation\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Rebuild models with the corrected state space\n",
        "actor_model = build_actor_model()\n",
        "critic_model = build_critic_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27777860",
      "metadata": {
        "id": "27777860"
      },
      "source": [
        "### Reward Function (0.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "005044e9",
      "metadata": {
        "id": "005044e9"
      },
      "outputs": [],
      "source": [
        "### Calculate Reward Function\n",
        "\n",
        "def calculate_reward(current_state, next_state):\n",
        "    target_temperature = 22\n",
        "    # Extract temperature features (assuming they are labeled from 'T1' to 'T9')\n",
        "    temp_features = ['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']\n",
        "    current_temps = current_state[temp_features]\n",
        "    next_temps = next_state[temp_features]\n",
        "    \n",
        "    # Calculate deviation penalty: sum of absolute differences from target temperature\n",
        "    deviation_penalty = sum(abs(current_temps - target_temperature))\n",
        "    \n",
        "    # Calculate energy savings: difference in energy use before and after the action\n",
        "    energy_savings = current_state['Appliances'] - next_state['Appliances']\n",
        "    \n",
        "    # Combine the comfort penalty and energy savings to get the final reward\n",
        "    reward = energy_savings - deviation_penalty\n",
        "\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf34260",
      "metadata": {
        "id": "3bf34260"
      },
      "source": [
        "#### Environment Simulation (0.5 M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e86b7850",
      "metadata": {
        "id": "e86b7850"
      },
      "outputs": [],
      "source": [
        "### Environment Simulation\n",
        "\n",
        "def simulate_environment(current_state, action, data, index):\n",
        "    # Adjust temperature based on action\n",
        "    temp_adjustment = action - 1  # action: 0 (decrease by 1°C), 1 (maintain), 2 (increase by 1°C)\n",
        "    temp_features = ['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']\n",
        "    \n",
        "    # Update temperatures\n",
        "    next_state = current_state.copy()\n",
        "    next_state[temp_features] += temp_adjustment\n",
        "    \n",
        "    # Ensure we don't go out of bounds in the dataset\n",
        "    if index + 1 < len(data):\n",
        "        next_index = index + 1\n",
        "    else:\n",
        "        next_index = index  # Stay at the last index if we're at the end of the data set\n",
        "    \n",
        "    # Get energy consumption before and after the action\n",
        "    energy_before = current_state['Appliances']\n",
        "    energy_after = data.iloc[next_index]['Appliances']\n",
        "    \n",
        "    # Update the state with new energy usage for continuity in simulation\n",
        "    next_state['Appliances'] = energy_after\n",
        "    \n",
        "    # Calculate the reward\n",
        "    reward = calculate_reward(current_state, next_state)\n",
        "    \n",
        "    # Return the new state and the reward\n",
        "    return next_state, reward"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8330a3c2",
      "metadata": {
        "id": "8330a3c2"
      },
      "source": [
        "#### Implementation of Training Function (2 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "1e974279",
      "metadata": {
        "id": "1e974279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'Appliances'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Appliances'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m         mean_reward \u001b[38;5;241m=\u001b[39m total_reward \u001b[38;5;241m/\u001b[39m index\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Mean Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[49], line 42\u001b[0m, in \u001b[0;36mtrain_function\u001b[0;34m(features, episodes)\u001b[0m\n\u001b[1;32m     39\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(action_probabilities)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Simulate the environment with the chosen action\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m next_state, reward \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Compute target and advantage for updating critic and actor\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[46], line 19\u001b[0m, in \u001b[0;36msimulate_environment\u001b[0;34m(current_state, action, data, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m     next_index \u001b[38;5;241m=\u001b[39m index  \u001b[38;5;66;03m# Stay at the last index if we're at the end of the data set\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Get energy consumption before and after the action\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m energy_before \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAppliances\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m energy_after \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[next_index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAppliances\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Update the state with new energy usage for continuity in simulation\u001b[39;00m\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.12/site-packages/pandas/core/series.py:1111\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.12/site-packages/pandas/core/series.py:1227\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
            "File \u001b[0;32m~/Downloads/bits/assignments/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Appliances'"
          ]
        }
      ],
      "source": [
        "# Train the Actor-Critic models\n",
        "\n",
        "def update_models(current_state, action, advantage, target, actor_model, critic_model, optimizer_actor, optimizer_critic):\n",
        "    # Convert current state to a suitable tensor for prediction\n",
        "    state_tensor = tf.convert_to_tensor(current_state.values.reshape(1, -1), dtype=tf.float32)\n",
        "\n",
        "    # Update Critic Model\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(critic_model.trainable_variables)\n",
        "        value = critic_model(state_tensor)\n",
        "        # Calculate critic loss as mean squared error between target values and predicted values\n",
        "        loss_critic = tf.keras.losses.MSE(target, value)\n",
        "    grads = tape.gradient(loss_critic, critic_model.trainable_variables)\n",
        "    optimizer_critic.apply_gradients(zip(grads, critic_model.trainable_variables))\n",
        "    \n",
        "    # Update Actor Model\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(actor_model.trainable_variables)\n",
        "        action_probs = actor_model(state_tensor)\n",
        "        action_log_probs = tf.math.log(action_probs[0, action])\n",
        "        loss_actor = -action_log_probs * advantage  # Negative for gradient ascent\n",
        "    grads = tape.gradient(loss_actor, actor_model.trainable_variables)\n",
        "    optimizer_actor.apply_gradients(zip(grads, actor_model.trainable_variables))\n",
        "\n",
        "\n",
        "def train_function(features, episodes=500):\n",
        "    discount_factor = 0.99\n",
        "    optimizer_actor = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    optimizer_critic = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        total_reward = 0\n",
        "        current_state = features.iloc[0]  # Reset to initial state at start of each episode\n",
        "        index = 0\n",
        "        \n",
        "        while index < len(features) - 1:\n",
        "            # Predict action probabilities from actor model\n",
        "            action_probabilities = actor_model.predict(current_state.values.reshape(1, -1))\n",
        "            action = np.argmax(action_probabilities)\n",
        "            \n",
        "            # Simulate the environment with the chosen action\n",
        "            next_state, reward = simulate_environment(current_state, action, features, index)\n",
        "            total_reward += reward\n",
        "            \n",
        "            # Compute target and advantage for updating critic and actor\n",
        "            value_current = critic_model.predict(current_state.values.reshape(1, -1))\n",
        "            value_next = critic_model.predict(next_state.values.reshape(1, -1))\n",
        "            target = reward + discount_factor * value_next\n",
        "            advantage = target - value_current\n",
        "            \n",
        "            # Update models\n",
        "            update_models(current_state, action, advantage, target, actor_model, critic_model, optimizer_actor, optimizer_critic)\n",
        "            \n",
        "            # Update state and index\n",
        "            current_state = next_state\n",
        "            index += 1\n",
        "        \n",
        "        # Print mean reward for episode\n",
        "        mean_reward = total_reward / index\n",
        "        print(f'Episode {episode + 1}: Mean Reward = {mean_reward}')\n",
        "\n",
        "train_function(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae48b2f0",
      "metadata": {
        "id": "ae48b2f0"
      },
      "source": [
        "#### Evaluate the performance of the model on test set (0.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6c0a8b",
      "metadata": {
        "id": "5a6c0a8b"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "\n",
        "def evaluate_model():\n",
        "\n",
        "    # predict the action and simulate the environment accordingly and get the respective next state\n",
        "\n",
        "    # calculate rewards for test set\n",
        "\n",
        "\n",
        "\n",
        "# Print the total reward obtained on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SwvZUqIuer7Q",
      "metadata": {
        "id": "SwvZUqIuer7Q"
      },
      "source": [
        "### Plot the convergence of Actor and Critic losses (1 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d262ae2d",
      "metadata": {
        "id": "d262ae2d"
      },
      "outputs": [],
      "source": [
        "# Plot the convergence of Actor and Critic losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xzis8NiEe21A",
      "metadata": {
        "id": "xzis8NiEe21A"
      },
      "source": [
        "### Plot the learned policy - by showing the action probabilities across different state values (1 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc20a76c",
      "metadata": {
        "id": "fc20a76c"
      },
      "outputs": [],
      "source": [
        "# Plot the learned policy - by showing the action probabilities across different state values\n",
        "\n",
        "# From the trained actor model, for each state in training set,\n",
        "# plot the probability of each action (increasing/decreasing/maintaining) the temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93cee383",
      "metadata": {
        "id": "93cee383"
      },
      "source": [
        "#### Conclusion (0.5 M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc218d2a",
      "metadata": {
        "id": "dc218d2a"
      },
      "outputs": [],
      "source": [
        "# Provide an analysis on a comparison of the energy consumption\n",
        "# before and after applying the reinforcement learning algorithm."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
