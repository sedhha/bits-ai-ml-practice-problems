{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "# Define the LeNet model\n",
    "def create_model():\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=(32, 32, 1)),  # Input layer\n",
    "        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Conv layer\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n",
    "        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Conv layer\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n",
    "        layers.Flatten(),                                        # Flatten\n",
    "        layers.Dense(120, activation='relu'),                   # Dense layer\n",
    "        layers.Dense(84, activation='relu'),                    # Dense layer\n",
    "        layers.Dense(10, activation='softmax')                  # Output layer\n",
    "    ])\n",
    "\n",
    "# Dataset preparation\n",
    "def create_dataset():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    x_train = tf.image.resize(x_train, (32, 32))\n",
    "    x_test = tf.image.resize(x_test, (32, 32))\n",
    "    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n",
    "    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Worker class\n",
    "class Worker:\n",
    "    def __init__(self, worker_id, model, dataset, parameter_server):\n",
    "        self.worker_id = worker_id\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.parameter_server = parameter_server\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    def compute_and_send_gradients(self, features, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(features, training=True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.parameter_server.receive_gradients(self.worker_id, gradients)\n",
    "        return loss\n",
    "\n",
    "    def run(self):\n",
    "        for step, (features, labels) in enumerate(self.dataset):\n",
    "            features = tf.convert_to_tensor(features)\n",
    "            labels = tf.convert_to_tensor(labels)\n",
    "            loss = self.compute_and_send_gradients(features, labels)\n",
    "            print(f\"Worker {self.worker_id} - Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Parameter server\n",
    "class ParameterServer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradient_accumulator = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def receive_gradients(self, worker_id, gradients):\n",
    "        with self.lock:\n",
    "            for i, grad in enumerate(gradients):\n",
    "                self.gradient_accumulator[i] += grad\n",
    "            print(f\"Parameter Server - Received gradients from Worker {worker_id}\")\n",
    "\n",
    "    def apply_gradients(self):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        with self.lock:\n",
    "            optimizer.apply_gradients(zip(self.gradient_accumulator, self.model.trainable_variables))\n",
    "            self.gradient_accumulator = [tf.zeros_like(var) for var in self.model.trainable_variables]\n",
    "            print(\"Parameter Server - Gradients applied and parameters updated\")\n",
    "\n",
    "# Parallel training setup\n",
    "train_dataset, test_dataset = create_dataset()\n",
    "global_model = create_model()\n",
    "parameter_server = ParameterServer(global_model)\n",
    "\n",
    "# Split dataset for workers\n",
    "worker_datasets = [train_dataset.shard(num_shards=3, index=i) for i in range(3)]\n",
    "workers = [Worker(worker_id=i, model=create_model(), dataset=worker_datasets[i], parameter_server=parameter_server) for i in range(3)]\n",
    "\n",
    "# Run workers in parallel\n",
    "threads = []\n",
    "for worker in workers:\n",
    "    thread = threading.Thread(target=worker.run)\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all workers to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "# Apply accumulated gradients on the parameter server\n",
    "parameter_server.apply_gradients()\n",
    "\n",
    "# Evaluate the global model\n",
    "global_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "test_loss, test_acc = global_model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Use MirroredStrategy for distributed training simulation\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Define the LeNet model\n",
    "with strategy.scope():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(32, 32, 1)),  # Input layer\n",
    "        layers.Conv2D(6, kernel_size=(5, 5), activation='relu'),  # Conv layer\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n",
    "        layers.Conv2D(16, kernel_size=(5, 5), activation='relu'), # Conv layer\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),               # Pooling layer\n",
    "        layers.Flatten(),                                        # Flatten\n",
    "        layers.Dense(120, activation='relu'),                   # Dense layer\n",
    "        layers.Dense(84, activation='relu'),                    # Dense layer\n",
    "        layers.Dense(10, activation='softmax')                  # Output layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Dataset preparation\n",
    "def create_dataset():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    x_train = tf.image.resize(x_train, (32, 32))\n",
    "    x_test = tf.image.resize(x_test, (32, 32))\n",
    "    x_train = tf.image.rgb_to_grayscale(x_train) / 255.0\n",
    "    x_test = tf.image.rgb_to_grayscale(x_test) / 255.0\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(64)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset, test_dataset = create_dataset()\n",
    "\n",
    "# Distribute the dataset\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n",
    "\n",
    "# Callback to observe gradients and weights\n",
    "class GradientCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(f\"\\n--- Batch {batch} ---\")\n",
    "        for i, weight in enumerate(self.model.trainable_weights):\n",
    "            print(f\"Layer {i} weight shape: {weight.shape}\")\n",
    "            print(f\"Sample weight values: {weight.numpy().flatten()[:5]}\")  # Display first 5 weights\n",
    "        print(\"------------------\")\n",
    "\n",
    "# Function to print data distribution across replicas\n",
    "@tf.function\n",
    "def print_batch_distribution(features, labels):\n",
    "    replica_context = tf.distribute.get_replica_context()\n",
    "    tf.print(f\"Replica {replica_context.replica_id_in_sync_group}: \"\n",
    "             f\"Feature shape: {tf.shape(features)}, Label shape: {tf.shape(labels)}\")\n",
    "\n",
    "# Observe how dataset is distributed\n",
    "iterator = iter(train_dist_dataset)\n",
    "features, labels = next(iterator)\n",
    "strategy.run(print_batch_distribution, args=(features, labels))\n",
    "\n",
    "# Train the model and observe updates\n",
    "model.fit(train_dist_dataset, epochs=1, callbacks=[GradientCallback()])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_dist_dataset)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
