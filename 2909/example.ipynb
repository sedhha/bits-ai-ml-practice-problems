{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85f2b1b",
   "metadata": {},
   "source": [
    "# Real-Time Prediction with Apache Kafka and Neural Networks\n",
    "\n",
    "In this notebook, we build a real-time data processing system using Apache Kafka and integrate a neural network to predict next day stock prices. We simulate the data, train a simple NN model using TensorFlow/Keras, and implement a producer-consumer architecture where the producer streams data and the consumer processes it in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if necessary)\n",
    "# !pip install kafka-python tensorflow\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70b263",
   "metadata": {},
   "source": [
    "## Global Configuration\n",
    "\n",
    "Here we define our Kafka broker URL, topics, and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration\n",
    "broker_url = 'localhost:9092'\n",
    "input_topic = 'input-data'\n",
    "pred_topic = 'predictions'\n",
    "\n",
    "# Other configurations\n",
    "producer_interval = 1  # seconds between messages\n",
    "\n",
    "print('Global configuration set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3b946",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "We simulate a dataset for stock prices. For simplicity, today's price is used as input and the next day's price (with some noise) is the target. Then we build and train a small feedforward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4466e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training data\n",
    "np.random.seed(42)\n",
    "prices = np.linspace(100, 200, num=1000)  # simulated prices\n",
    "noise = np.random.normal(0, 2, prices.shape)\n",
    "next_day_prices = prices + noise  # next day prices with noise\n",
    "\n",
    "X = prices.reshape(-1, 1)\n",
    "y = next_day_prices.reshape(-1, 1)\n",
    "\n",
    "print('Training data created.')\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "print('Training the model...')\n",
    "model.fit(X, y, epochs=50, batch_size=32, verbose=0)\n",
    "print('Model training complete.')\n",
    "\n",
    "# Save the trained model to disk so that the consumer can load it\n",
    "model.save('stock_model.h5')\n",
    "print('Model saved as stock_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a55a51",
   "metadata": {},
   "source": [
    "## Producer Implementation\n",
    "\n",
    "The producer simulates real-time streaming by publishing messages with a unique ID, stock price, and a timestamp to the `input-data` Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_producer():\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=broker_url,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        # Simulate a stock price\n",
    "        price = random.uniform(100, 200)\n",
    "        data = {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'features': {'price': price},\n",
    "            'timestamp': datetime.datetime.utcnow().isoformat()\n",
    "        }\n",
    "        producer.send(input_topic, value=data)\n",
    "        print('Produced:', data)\n",
    "        time.sleep(producer_interval)\n",
    "\n",
    "# To test the producer by itself, uncomment the following line:\n",
    "# start_producer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3ac3d2",
   "metadata": {},
   "source": [
    "## Consumer Implementation with Neural Network Prediction\n",
    "\n",
    "The consumer subscribes to the `input-data` topic, processes incoming messages, loads the pre-trained neural network, and uses it to predict the next day's price. The result (including the original price, prediction, and timestamp) is then published to the `predictions` topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_consumer():\n",
    "    # Load the pre-trained model\n",
    "    model = keras.models.load_model('stock_model.h5')\n",
    "    \n",
    "    consumer = KafkaConsumer(\n",
    "        input_topic,\n",
    "        bootstrap_servers=broker_url,\n",
    "        auto_offset_reset='earliest',\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    "    )\n",
    "    \n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=broker_url,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    for msg in consumer:\n",
    "        data = msg.value\n",
    "        price = data['features']['price']\n",
    "        \n",
    "        # Preprocess the data (for our simple example, no scaling is applied)\n",
    "        input_array = np.array([[price]])\n",
    "        \n",
    "        # Predict the next day price using the model\n",
    "        pred = model.predict(input_array)\n",
    "        prediction = pred[0, 0]\n",
    "        \n",
    "        result = {\n",
    "            'id': data['id'],\n",
    "            'original_price': price,\n",
    "            'predicted_next_day_price': float(prediction),\n",
    "            'prediction_timestamp': datetime.datetime.utcnow().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Publish the prediction to the predictions topic\n",
    "        producer.send(pred_topic, value=result)\n",
    "        print('Consumed and Predicted:', result)\n",
    "\n",
    "# To test the consumer by itself, uncomment the following line:\n",
    "# start_consumer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c82c12d",
   "metadata": {},
   "source": [
    "## Running the Producer and Consumer\n",
    "\n",
    "For demonstration purposes, we run both the producer and consumer in separate threads. In production, these would typically be separate services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f61e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start producer and consumer in separate daemon threads\n",
    "producer_thread = threading.Thread(target=start_producer, daemon=True)\n",
    "consumer_thread = threading.Thread(target=start_consumer, daemon=True)\n",
    "\n",
    "producer_thread.start()\n",
    "consumer_thread.start()\n",
    "\n",
    "print('Producer and Consumer threads started. Press Ctrl+C to stop.')\n",
    "\n",
    "# Keep the notebook running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print('Stopping threads...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ffbd9",
   "metadata": {},
   "source": [
    "## Real-Time Monitoring\n",
    "\n",
    "Logs printed in the notebook show the number of messages produced and consumed, along with prediction details. In a production system, you might integrate a dashboard to monitor metrics like message throughput, latency, and prediction accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
